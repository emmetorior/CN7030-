{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1voaoCRx-Q57JjM1dgoUIz8tAreSpqKkK",
      "authorship_tag": "ABX9TyMlWXVc89egyMEv6MQhDTcj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmetorior/CN7030-/blob/main/ASSN_CN7050_FINAL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "!pip install --upgrade pyspark pyarrow pandas\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydLyxVtJ6Kvu",
        "outputId": "c2530553-406d-4627-8acf-14c3ad724088"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (19.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "import pyarrow as pa\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array_contains\n",
        "from pyspark.sql.functions import col, explode, split, array, lit\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "import ast\n"
      ],
      "metadata": {
        "id": "BXoMU4N61zyF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Files Exist and Are Readable.\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nct0uoTBAfqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "parquet_validation_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/validation-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_validation_file))\n",
        "\n",
        "parquet_test_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/test-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_test_file))\n",
        "\n",
        "parquet_train_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/train-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_train_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8njwXTOmAR0C",
        "outputId": "5ccc8b5d-384f-419d-8eb5-1db8561a3203"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load files into Spark. We have a train, test and validation file. Each one is loaded in from the parquet downloaded from huggingface.co"
      ],
      "metadata": {
        "id": "rcjt7dWfOWWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SET UP A SPARK SESSION AND SET OPTIONS"
      ],
      "metadata": {
        "id": "eaV1SHPcMhKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.Builder().master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
        "spark.conf.set(\"spark.sql.parquet.binaryAsString\", \"true\")"
      ],
      "metadata": {
        "id": "w14Dd0wJMc7U"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#spark.conf.set(\"spark.driver.memory\", \"8g\")  # Increase to 8 GB (or higher if needed)\n",
        "#spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
        "\n",
        "#disable for now spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
        "\n",
        "dfs_train = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_train_file)\n",
        "dfs_test = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_test_file)\n",
        "dfs_validation = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_validation_file)"
      ],
      "metadata": {
        "id": "wTvMuJLvQLDG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the file contents"
      ],
      "metadata": {
        "id": "k8VcUfUHOgry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfs_train.show(5)\n",
        "dfs_test.show(5)\n",
        "dfs_validation.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u26PJdQibtkR",
        "outputId": "52856d4e-22bf-4c6b-ebce-53659685e87e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|        industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|301972057|      600| Trustpilot|Price Comparison|My experience is ...|[[Staff support: ...|['staff-support.a...|\n",
            "|301982453|      514|Google Play|         Banking|I love it so hand...|[[Company brand: ...|['company-brand.g...|\n",
            "|301980653|      369|Google Play|    Ride Hailing|  Sometimes it takes|[[Company brand: ...|['company-brand.g...|\n",
            "|301979991|      727|Apple Store|         Fashion|This is the worst...|[[Logistics rides...|['logistics-rides...|\n",
            "|301984330|      549|Google Play|  Travel Booking|So easy & loads o...|[[Company brand: ...|['company-brand.g...|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|    industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "|301982094|      514|Google Play|     Banking|Very useful and e...|[[Purchase bookin...|['purchase-bookin...|\n",
            "|301981085|      369|Google Play|Ride Hailing|easy to use.gud r...|[[Staff support: ...|['staff-support.a...|\n",
            "|301986508|      685|Google Play|     Trading|            money üòÅ|[[Company brand: ...|['company-brand.g...|\n",
            "|301981875|      514|Google Play|     Banking|      Great facility|[[Company brand: ...|['company-brand.g...|\n",
            "|301977341|      411|Apple Store|   Groceries|Love doing my ORG...|[[Purchase bookin...|['purchase-bookin...|\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+---------+-----------+--------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|      industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+--------------+--------------------+--------------------+--------------------+\n",
            "|610309432|     5827|Google Play|    Consulting|How do I retrieve...|[[Account managem...|['account-managem...|\n",
            "|301974039|      616| Trustpilot|       Fashion|Super fast delive...|[[Logistics rides...|['logistics-rides...|\n",
            "|301983653|      549|Google Play|Travel Booking|Great tool for ch...|[[Online experien...|['online-experien...|\n",
            "|301985771|      616|Google Play|       Fashion|Fast shipping but...|[[Logistics rides...|['logistics-rides...|\n",
            "|301980111|      727|Apple Store|       Fashion|Great app. Easy t...|[[Staff support: ...|['staff-support.a...|\n",
            "+---------+---------+-----------+--------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File was not loading. Implemented a Workaround by converting the pandas version\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "of the parquet file converted to CSV and read into spark to create a spark dataframe. Maybe delete this segment later as problem was resolved.  "
      ],
      "metadata": {
        "id": "v5cc0tMjaYRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*"
      ],
      "metadata": {
        "id": "8w_7RGLkjact"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training file row count:\", dfs_train.count())\n",
        "dfs_train.printSchema()\n",
        "print(\"Testing file row count:\", dfs_test.count())\n",
        "dfs_test.printSchema()\n",
        "print(\"Validation file row count:\", dfs_validation.count())\n",
        "dfs_validation.printSchema()"
      ],
      "metadata": {
        "id": "f_nMCgTX5Ivi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b499c014-c14d-4430-8c16-623112f05d12"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file row count: 7930\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n",
            "Testing file row count: 1587\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n",
            "Validation file row count: 1057\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#findspark.init()\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql.functions import col, sum\n"
      ],
      "metadata": {
        "id": "nUmFm9EZ51ga"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_counts_train = dfs_train.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_train.columns])\n",
        "null_counts_train.show()\n",
        "null_counts_test = dfs_test.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_test.columns])\n",
        "null_counts_test.show()\n",
        "null_counts_validation = dfs_validation.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_validation.columns])\n",
        "null_counts_validation.show()"
      ],
      "metadata": {
        "id": "MFVBX1FfdmFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3199dac9-97f3-45fe-d40e-4a5f04d99803"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "| id|org_index|data_source|industry|text|labels|label_codes|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "|  0|        0|          0|       0|   0|     0|          0|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "| id|org_index|data_source|industry|text|labels|label_codes|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "|  0|        0|          0|       0|   0|     0|          0|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "| id|org_index|data_source|industry|text|labels|label_codes|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "|  0|        0|          0|       0|   0|     0|          0|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ACD-Only Classifier Pipeline\n",
        "from pyspark.sql.functions import col, array_contains, lit, udf\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast"
      ],
      "metadata": {
        "id": "80PqdfPiNZo2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning and Extracting functions\n",
        "def clean_categories(categories):\n",
        "    return categories.replace('-', '_').replace('.', '_')\n",
        "\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "        return [clean_categories(cat) for cat in categories]\n",
        "    return []"
      ],
      "metadata": {
        "id": "nTg0cCjYNd_P"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Extract Categories"
      ],
      "metadata": {
        "id": "LOrPiwbaOY9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dfs_train_clean = dfs_train_clean.limit(5000)\n",
        "#\n",
        "# Extract categories for each DataFrame\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "dfs_train_clean = dfs_train_clean.limit(1000)\n",
        "\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "dfs_test_clean = dfs_test_clean.limit(1000)\n",
        "\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "dfs_validation_clean = dfs_validation_clean.limit(1000)"
      ],
      "metadata": {
        "id": "vEDS52iDOj18"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Train Logistic Regression Model\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train_data = assembler.transform(dfs_train_clean).select(\"features\", \"id\")\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"id\")\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# Test on test set\n",
        "test_data = assembler.transform(dfs_test_clean).select(\"features\", \"id\")\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1 = evaluator.evaluate(predictions)\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Validate on validation set\n",
        "validation_data = assembler.transform(dfs_validation_clean).select(\"features\", \"id\")\n",
        "validation_predictions = model.transform(validation_data)\n",
        "\n",
        "f1_val = evaluator.evaluate(validation_predictions)\n",
        "precision_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"Validation F1 Score: {f1_val}\")\n",
        "print(f\"Validation Precision: {precision_val}\")\n",
        "print(f\"Validation Recall: {recall_val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "YEP3xhGmKyX-",
        "outputId": "4cec1b9c-6599-4d48-8e31-165b59f12408"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o1768.fit.\n: java.lang.OutOfMemoryError: Java heap space\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-29ddd5299dbb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Test on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1768.fit.\n: java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add me back later, if needed. Initialize Spark session. Display the schema to understand the data structure\n",
        "#dfp_train.printSchema()  Let's extract the categories from label_codes. First, we need to convert the string representation of label codes to actual lists. Then We'll use a UDF (User Defined Function) for this\n",
        "def clean_categories(categories):\n",
        "  return categories.replace('-', '_').replace('.', '_')\n",
        "\n",
        "#Extract just the category part from each label code\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "\n",
        "    # Convert string representation to list\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)#\n",
        "\n",
        "        # Extract category part (remove the sentiment indicator at the end)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "\n",
        "        categories = clean_categories(categories)\n",
        "        return categories\n",
        "    return []"
      ],
      "metadata": {
        "id": "Tq7X4GbkTyjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zd7M82pWT78Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Convert 'label_codes' from string to list\n",
        "dfs_train['label_codes'] = dfs_train['label_codes'].apply(\n",
        "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "# Extract categories and clean them\n",
        "def extract_categories(label_codes):\n",
        "    return [\n",
        "        code.rsplit('.', 1)[0].replace('-', '_').replace('.', '_')\n",
        "        for code in label_codes\n",
        "    ]\n",
        "\n",
        "# Apply the function to create a 'categories' column\n",
        "dfp_train['categories'] = dfp_train['label_codes'].apply(extract_categories)"
      ],
      "metadata": {
        "id": "Ky-gnLs4JDV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#dfs_train = spark.createDataFrame(dfp_train)     #pandas df has no ... withColumn so we need to convert it - Oops it's not a pandas df, it's a pyarrow Table that needs to be first converted to a pandas df\n",
        "#dfp_train = dfp_train.to_pandas()\n",
        "#dfs_train = spark.createDataFrame(dfp_train)\n",
        "##It seems that the to_pandas method is not available for Spark DataFrames -  use the collect method to convert the Spark DataFrame to a Pandas DataFrame:\n",
        "\n",
        "# Convert Spark DataFrame to Pandas\n",
        "dfp_train = spark.read.parquet(parquet_train_file).toPandas()\n",
        "print ( dfp_train.columns)\n",
        "# For each category, create a binary indicator column (1 if present, 0 if not)\n",
        "print(dfp_train.head(10))\n",
        "\n"
      ],
      "metadata": {
        "id": "gH6mayVWnipM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Add me back later, if needed. Initialize Spark session. Display the schema to understand the data structure\n",
        "##dfp_train.printSchema()  Let's extract the categories from label_codes. First, we need to convert the string representation of label codes to actual lists. Then We'll use a UDF (User Defined Function) for this\n",
        "#def clean_categories(categories):\n",
        "#  return categories.replace('-', '_').replace('.', '_')\n",
        "#\n",
        "##Extract just the category part from each label code\n",
        "#@udf(returnType=ArrayType(StringType()))\n",
        "#def extract_categories(label_codes):\n",
        "#\n",
        "#    # Convert string representation to list\n",
        "#    if isinstance(label_codes, str):\n",
        "#        codes = ast.literal_eval(label_codes)#\n",
        "#\n",
        "#        # Extract category part (remove the sentiment indicator at the end)\n",
        "#        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "#\n",
        "#        categories = clean_categories(categories)\n",
        "#        return categories\n",
        "#    return []"
      ],
      "metadata": {
        "id": "V6JHSyWfJi6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_with_categories = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "# Create a list of all unique categories from the dataset\n",
        "df_with_categories.printSchema()\n",
        "#all_categories = df_with_categories.select(explode(\"categories\")).distinct().collect()\n",
        "#all_categories = [row[0] for row in all_categories]\n"
      ],
      "metadata": {
        "id": "oSDhKNxTpY3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) with Clean Column Names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: Extract Sentiment for Each Category\n",
        "\n",
        "# Extract 'label_codes' from the original Spark DataFrame\n",
        "#label_codes_df = df_with_categories.select('id', 'label_codes').toPandas()\n",
        "label_codes_df = df_with_categories.select('id', 'label_codes', 'categories').toPandas()\n",
        "print(label_codes_df)\n",
        "\n",
        "# Merge with train_pandas using the 'id' column\n",
        "train_pandas = train_pandas.merge(label_codes_df, on='id', how='left')\n",
        "\n",
        "def extract_aspect_and_sentiment(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [(code.rsplit('.', 1)[0], int(code.rsplit('.', 1)[-1])) for code in codes]\n",
        "    return []\n",
        "\n",
        "train_pandas['aspect_sentiment_pairs'] = train_pandas['label_codes'].apply(extract_aspect_and_sentiment)\n",
        "\n",
        "# For each category, create a sentiment column (-1, 0, or 1)\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    train_pandas[f\"sentiment_{clean_aspect}\"] = train_pandas['aspect_sentiment_pairs'].apply(\n",
        "        lambda x: next((sent for asp, sent in x if asp == aspect), 0)\n",
        "    )\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "#train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "#test_pandas = test_data.limit(200).toPandas()\n",
        "train_pandas = df_with_categories.limit(1000).toPandas()\n",
        "test_pandas = df_with_categories.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery_life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cye52wMXoiFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRINT ALL CATEGORIES FOUND"
      ],
      "metadata": {
        "id": "gfeFLTrL-i_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in all_categories:\n",
        "  print(cat + '\\n')"
      ],
      "metadata": {
        "id": "fd0-2x4Lu2ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each category, create a binary indicator column (1 if present, 0 if not)\n",
        "for cat in all_categories:\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{cat.replace('-', '_').replace('.', '_')}\",\n",
        "        array_contains(col(\"categories\"), cat).cast(\"integer\")\n",
        "    )\n"
      ],
      "metadata": {
        "id": "suGL6w1BwrI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in all_categories:\n",
        "  print(cat + '\\n')"
      ],
      "metadata": {
        "id": "E0u6elBY-uWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_pandas.columns)"
      ],
      "metadata": {
        "id": "vM7ZkSlToFeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "FIX ME TODAY\n",
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) with Clean Column Names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: Extract Sentiment for Each Category\n",
        "\n",
        "def extract_aspect_and_sentiment(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [(code.rsplit('.', 1)[0], int(code.rsplit('.', 1)[-1])) for code in codes]\n",
        "    return []\n",
        "\n",
        "train_pandas['aspect_sentiment_pairs'] = train_pandas['label_codes'].apply(extract_aspect_and_sentiment)\n",
        "\n",
        "# For each category, create a sentiment column (-1, 0, or 1)\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    train_pandas[f\"sentiment_{clean_aspect}\"] = train_pandas['aspect_sentiment_pairs'].apply(\n",
        "        lambda x: next((sent for asp, sent in x if asp == aspect), 0)\n",
        "    )\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery_life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BYsspjOVnkvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the user comments from the text column and remove stop words."
      ],
      "metadata": {
        "id": "QbOTD5kNXdfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "# Select relevant columns for the ACD task\n",
        "feature_cols = [f\"has_{category.replace('-', '_').replace('.', '_')}\" for category in all_categories]\n",
        "acd_df = df_with_categories.select(\"id\", \"text\", *feature_cols)\n",
        "\n",
        "# Convert text to features using TF-IDF\n",
        "# First, tokenize the text\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(acd_df)\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered_data = remover.transform(wordsData)\n",
        "\n",
        "# Convert words to term frequency features\n",
        "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"tf\", minDF=2.0)\n",
        "cv_model = cv.fit(filtered_data)\n",
        "tf_data = cv_model.transform(filtered_data)\n",
        "\n",
        "# Convert term frequency features to TF-IDF\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
        "idf_model = idf.fit(tf_data)\n",
        "tfidf_data = idf_model.transform(tf_data)\n",
        "\n",
        "# Final dataset ready for modeling\n",
        "final_acd_df = tfidf_data.select(\"id\", \"features\", *feature_cols)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = final_acd_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Cache the datasets for faster processing\n",
        "train_data.cache()\n",
        "test_data.cache()\n"
      ],
      "metadata": {
        "id": "gw5lYX0_LauT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final_acd_df.show(5)\n"
      ],
      "metadata": {
        "id": "TIddta9BZVDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Since you're working with text data, you'll need to extract relevant features from each document. Some common techniques include:\n",
        "#Tokenization (splitting text into individual words or tokens)\n",
        "#Stopword removal (removing common words like \"the\", \"and\", etc.)\n",
        "#Stemming or Lemmatization (reducing words to their base form)\n",
        "#Part-of-speech tagging (identifying grammatical categories of each word)\n",
        "# Create a SparkSession with your test/train/validate data sets\n",
        "#spark = SparkSession.builder.appName(\"Aspect-Based Sentiment Analysis\").getOrCreate()\n",
        "\n",
        "# Load the training and validate datasets into DataFrames\n",
        "#train_df = spark.read.csv(\"path/to/train/dataset\", header=True, inferSchema=True)\n",
        "#val_df = spark.read.csv(\"path/to/val/dataset\", header=True, inferSchema=True)\n",
        "\n",
        "# Define a function to extract features from each column in the dataset\n",
        "def feature_extraction(df):\n",
        "    # Tokenization and stopword removal\n",
        "    df = df.select(\n",
        "        explode(col(\"text\").cast(\"string\")).alias(\"tokens\"),\n",
        "        explode(col(\"sentiment\").cast(\"string\")).alias(\"sentiment\")\n",
        "    )\n",
        "\n",
        "    # Stemming or Lemmatization\n",
        "    df = df.withColumn(\n",
        "        \"stemmed_tokens\",\n",
        "        df.tokens.map(lambda x: x.lower() if x.isnumeric() else x)  # Remove numbers and convert to lowercase\n",
        "    )\n",
        "\n",
        "    # Part-of-speech tagging (optional)\n",
        "    #df = df.withColumn(\"pos_tags\", explode(col(\"word\").cast(\"string\")).map(lambda x: {\"POS\": \"NNP\"} if x.isnumeric() else {\"POS\": \"NOUN\"})  # Example\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "cCPtQoKDXbiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XeoRym6BzNyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#all_words = filtered_data.rdd.flatMap(lambda x: x.filtered_words).collect()\n",
        "all_words = filtered_data.select(\"filtered_words\").rdd.flatMap(lambda x: x.filtered_words).collect()\n",
        "word_counts = Counter(all_words)\n",
        "top_words = dict(word_counts.most_common(20))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(top_words.keys(), top_words.values())\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Top 20 Word Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#2. TF-IDF Visualization:\n"
      ],
      "metadata": {
        "id": "-idjLDxZGVL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Extract only the binary category columns and sentiment\n",
        "category_columns = [col for col in train_pandas.columns if col.startswith('has_')]\n",
        "df_for_heatmap = train_pandas[category_columns + ['sentiment']]\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df_for_heatmap.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Aspect-Sentiment Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AAgDXRpaLYpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Chart: Heatmap or bar chart.\n",
        "#Purpose: Shows the TF-IDF scores for selected words across documents.\n",
        "#Implementation: After TF-IDF, convert the sparse vectors to dense arrays and plot the values.\n",
        "#Python\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have a few documents and words you want to visualize\n",
        "selected_docs = tfidf_data.take(5) #take the first 5 documents.\n",
        "selected_words = cv_model.vocabulary[:10] # take the first 10 vocabulary words.\n",
        "\n",
        "tfidf_matrix = np.zeros((len(selected_docs), len(selected_words)))\n",
        "\n",
        "for i, row in enumerate(selected_docs):\n",
        "    dense_features = row.features.toArray()\n",
        "    for j, word in enumerate(selected_words):\n",
        "        if word in cv_model.vocabulary:\n",
        "            word_index = cv_model.vocabulary.index(word)\n",
        "            tfidf_matrix[i,j] = dense_features[word_index]\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(tfidf_matrix, annot=True, xticklabels=selected_words)\n",
        "plt.xlabel(\"Selected Words\")\n",
        "plt.ylabel(\"Documents\")\n",
        "plt.title(\"TF-IDF Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "#3. Model Evaluation Metrics:\n",
        "#\n",
        "#Charts: Confusion matrix, ROC curve, precision-recall curve.\n",
        "#Purpose: Shows the performance of the trained classification model.\n",
        "#Implementation: After training and predicting on the test set, use metrics like accuracy, precision, recall, F1-score, and create plots to visualize them.\n",
        "#Confusion Matrix: shows the number of correct and incorrect predictions for each class.\n",
        "#ROC Curve: shows the trade-off between true positive rate and false positive rate.\n",
        "#Precision-Recall Curve: shows the trade-off between precision and recall.\n",
        "#4. Document Length Distribution:#\n",
        "#\n",
        "#Chart: Histogram.\n",
        "#Purpose: Shows the distribution of the number of words in each document.\n",
        "#Implementation: Count the number of words in each document after tokenization.\n",
        "#Python#\n",
        "#\n",
        "doc_lengths = wordsData.rdd.map(lambda x: len(x.words)).collect()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(doc_lengths, bins=50)\n",
        "plt.xlabel(\"Document Length (Number of Words)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Document Length Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X5ntO_-ZITfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.show(10)"
      ],
      "metadata": {
        "id": "-YdoY8B4lQQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Function to train and evaluate an SVM model for one category\n",
        "def train_svm_for_category(category, train_data, test_data):\n",
        "    # Prepare label column name\n",
        "    label_col = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "\n",
        "    # Create SVM model\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=label_col, maxIter=10)\n",
        "\n",
        "    # Train the model\n",
        "    model = svm.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # For binary classification metrics (precision, recall)\n",
        "    binary_evaluator = BinaryClassificationEvaluator(\n",
        "        labelCol=label_col, rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "    # For F1 score (use MulticlassClassificationEvaluator)\n",
        "    multi_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "    # Calculate AUC (Area Under ROC)\n",
        "    auc = binary_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate F1\n",
        "    f1 = multi_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate precision and recall manually\n",
        "    # First, get true positives, false positives, true negatives, false negatives\n",
        "    tp = predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 1)).count()\n",
        "    fp = predictions.filter((col(label_col) == 0) & (col(\"prediction\") == 1)).count()\n",
        "    fn = predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 0)).count()\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"category\": category,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"auc\": auc\n",
        "    }"
      ],
      "metadata": {
        "id": "024iF0ZjFCon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models were running too slow. \\Caching the DataFrame during SVM training to avoid redundant data processing. Reducing the number of SVM iterations and limiting the dataset size for deep learning. Simplifying the LSTM and CNN architectures by reducing the number of units and filters. Lowering the number of epochs and silencing verbose outputs for faster training.\n",
        "\n",
        "Added visualizations to pipeline:\n",
        "\n",
        "LSTM Loss Over Epochs to show how the model improves during training.\n",
        "CNN Accuracy Over Epochs to track performance across epochs.\n",
        "\n"
      ],
      "metadata": {
        "id": "A9isoSjT1zSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) with Clean Column Names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: ABSA (SVM Classifier for each Aspect) with Caching for Speed\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{clean_aspect}') == 1).cache()\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=5)  # Reduced iterations for speed\n",
        "    model = svm.fit(tfidf_data)\n",
        "\n",
        "    aspect_df.unpersist()\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery_life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZhwcmnLmuzZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE END"
      ],
      "metadata": {
        "id": "c9Z3_Jc1u1RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD)\n",
        "for aspect in all_categories:\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: ABSA (SVM Classifier for each Aspect) with Caching for Speed\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{aspect}') == 1).cache()\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=5)  # Reduced iterations for speed\n",
        "    model = svm.fit(tfidf_data)\n",
        "\n",
        "    aspect_df.unpersist()\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery-life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h-4eMTi81uzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional charts, such as confusion matrices or precision-recall curves.\n"
      ],
      "metadata": {
        "id": "OIDyy3yc2TtN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ltf8M5Oc2S-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models were running too slow.\n",
        "\\Caching the DataFrame during SVM training to avoid redundant data processing.\n",
        "Reducing the number of SVM iterations and limiting the dataset size for deep learning.\n",
        "Simplifying the LSTM and CNN architectures by reducing the number of units and filters.\n",
        "Lowering the number of epochs and silencing verbose outputs for faster training."
      ],
      "metadata": {
        "id": "QqRJCneG1a21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Spark DataFrame to Pandas for deep learning\n",
        "# Collect the data (be careful with large datasets)\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "# We need to convert the sparse vector features to numpy arrays\n",
        "from pyspark.ml.linalg import SparseVector\n",
        "\n",
        "def sparse_to_array(sparse_vector):\n",
        "    return sparse_vector.toArray() if isinstance(sparse_vector, SparseVector) else sparse_vector\n",
        "\n",
        "# Convert features to numpy arrays\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Prepare X (features) and y (labels) for training\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "\n",
        "# Get all label columns\n",
        "label_cols = [col for col in train_pandas.columns if col.startswith('has_')]\n",
        "y_train = train_pandas[label_cols].values\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "# Now build a deep learning model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define model architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "results = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {results[0]}\")\n",
        "print(f\"Test Accuracy: {results[1]}\")\n",
        "print(f\"Test Precision: {results[2]}\")\n",
        "print(f\"Test Recall: {results[3]}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate F1 score manually\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "precision = precision_score(y_test, y_pred_binary, average='micro')\n",
        "recall = recall_score(y_test, y_pred_binary, average='micro')\n",
        "f1 = f1_score(y_test, y_pred_binary, average='micro')\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "DNmIWqJYF47I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END WORKING *AREA*\n"
      ],
      "metadata": {
        "id": "onEAObLs1M6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "# For simplicity, let's implement a binary classification model for each category\n",
        "# In practice, you might want to use a multi-label approach\n",
        "\n",
        "# Function to train and evaluate an SVM model for one category\n",
        "def train_svm_for_category(category, train_data, test_data):\n",
        "    # Prepare label column name\n",
        "    label_col = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "\n",
        "    # Create SVM model\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=label_col, maxIter=10)\n",
        "\n",
        "    # Train the model\n",
        "    model = svm.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # Evaluate model\n",
        "   # evaluator = MulticlassClassificationEvaluator(\n",
        "   #     labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
        "   # f1 = evaluator.evaluate(predictions)\n",
        "    #\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col,\n",
        "                                              predictionCol=\"prediction\",\n",
        "                                              metricName=\"weightedPrecision\")\n",
        "precision = evaluator.evaluate(predictions)\n",
        "print(\"Weighted Precision:\", precision)\n",
        "    evaluator.setMetricName(\"precision\")\n",
        "    precision = evaluator.evaluate(predictions)\n",
        "\n",
        "    evaluator.setMetricName(\"recall\")\n",
        "    recall = evaluator.evaluate(predictions)\n",
        "\n",
        "    return {\"category\": category, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Train models for all categories\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "    result = train_svm_for_category(category, train_data, test_data)\n",
        "    svm_results.append(result)\n",
        "\n",
        "# Print results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "svm_results_df.show()"
      ],
      "metadata": {
        "id": "5I0GaU5Umj0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training SVM models for all categories...\")\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "    try:\n",
        "        result = train_svm_for_category(category, train_data, test_data)\n",
        "        svm_results.append(result)\n",
        "        print(f\"Completed category: {category}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing category {category}: {str(e)}\")\n",
        "\n",
        "# Create a dataframe with results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "print(\"SVM Results:\")\n",
        "svm_results_df.show()\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_precision = np.mean([r[\"precision\"] for r in svm_results])\n",
        "avg_recall = np.mean([r[\"recall\"] for r in svm_results])\n",
        "avg_f1 = np.mean([r[\"f1\"] for r in svm_results])\n",
        "\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average F1: {avg_f1:.4f}\")\n",
        "\n",
        "# Convert to pandas DataFrame for deep learning model\n",
        "# We'll process a subset of records to handle memory constraints\n",
        "# For a real model, you might want to use a data loader or mini-batches\n",
        "print(\"Preparing data for deep learning model...\")\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit to 1000 samples for demonstration\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Function to convert sparse vector to array\n",
        "def sparse_to_array(v):\n",
        "    if hasattr(v, 'toArray'):\n",
        "        return v.toArray()\n",
        "    return v\n",
        "\n",
        "# Apply conversion function\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Stack features into numpy arrays\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "\n",
        "# Extract labels\n",
        "y_train = train_pandas[feature_cols].values\n",
        "y_test = test_pandas[feature_cols].values\n",
        "\n",
        "print(\"Data preparation for deep learning complete!\")"
      ],
      "metadata": {
        "id": "pjwA9QxfmIkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models for all categories\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "   # result = train_svm_for_category(category, train_data, test_data)\n",
        "    result = train_svm_for_category(category, train_data, test_data)\n",
        "    svm_results.append(result)\n",
        "\n",
        "# Print results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "svm_results_df.show()"
      ],
      "metadata": {
        "id": "qQZ0jSpGKD9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speeded up version"
      ],
      "metadata": {
        "id": "lLr-hMhUwTP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) - Clean column names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "## Step 1: Aspect Category Detection (ACD)\n",
        "#for aspect in all_categories:\n",
        "#    df_with_categories = df_with_categories.withColumn(\n",
        "#        f\"has_{aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "#    )\n",
        "#\n",
        "#\n",
        "## Step 2: ABSA (SVM Classifier for each Aspect) with Caching for Speed\n",
        "#def train_absa_svm(aspect, df_with_categories):\n",
        "#    aspect_df = df_with_categories.filter(col(f'has_{aspect}') == 1).cache()\n",
        "#\n",
        "\n",
        "# Step 2: Adjust the SVM Function to Use the Clean Column Name\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{clean_aspect}') == 1).cache()\n",
        "\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=5)  # Reduced iterations for speed\n",
        "    model = svm.fit(tfidf_data)\n",
        "\n",
        "    aspect_df.unpersist()\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery-life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))"
      ],
      "metadata": {
        "id": "aQuL0YIJwPjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very slow version"
      ],
      "metadata": {
        "id": "fuP_tJHBwQaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models)\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD)\n",
        "# Binary indicator columns for each aspect\n",
        "for aspect in all_categories:\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: ABSA (SVM Classifier for each Aspect)\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{aspect}') == 1)\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=10)\n",
        "    model = svm.fit(tfidf_data)\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Step 5: CNN Model for ABSA\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Step 6: Compare SVM, LSTM, and CNN Models\n",
        "svm_model = train_absa_svm(\"battery-life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "kOVK6q_eKHHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Old version of file loading into pandas\n"
      ],
      "metadata": {
        "id": "-Z6G2T8CNNuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Try with pyarrow first to validate the file\n",
        "try:\n",
        "    dfp_train = pq.read_table(parquet_train_file)\n",
        "    dfp_test = pq.read_table(parquet_train_file)\n",
        "    dfp_validation = pq.read_table(parquet_validation_file)\n",
        "    print(\"File can be read with pyarrow\")\n",
        "except Exception as e:\n",
        "    print(f\"Pyarrow error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8zGv69MPmIV",
        "outputId": "6a49f102-fe3f-451a-b68a-ee048b3e8842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File can be read with pyarrow\n"
          ]
        }
      ]
    }
  ]
}