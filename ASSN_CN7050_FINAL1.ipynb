{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1voaoCRx-Q57JjM1dgoUIz8tAreSpqKkK",
      "authorship_tag": "ABX9TyNj5IYeXhbIeyYszCXBYP2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmetorior/CN7030-/blob/main/ASSN_CN7050_FINAL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "!pip install --upgrade pyspark pyarrow pandas\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydLyxVtJ6Kvu",
        "outputId": "1e4c1dc2-298b-449a-a004-29927b4d235b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, pandas\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3 pyarrow-19.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "import pyarrow as pa\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array_contains\n",
        "from pyspark.sql.functions import col, explode, split, array, lit\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "import ast\n"
      ],
      "metadata": {
        "id": "BXoMU4N61zyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Files Exist and Are Readable.\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nct0uoTBAfqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "parquet_validation_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/validation-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_validation_file))\n",
        "\n",
        "parquet_test_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/test-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_test_file))\n",
        "\n",
        "parquet_train_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/train-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_train_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8njwXTOmAR0C",
        "outputId": "665a1eae-5f78-46e7-8b0b-7a8357ac0e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.Builder().master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
        "spark.conf.set(\"spark.sql.parquet.binaryAsString\", \"true\")\n",
        "#disable for now spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
        "\n",
        "dfs_train = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_train_file)\n",
        "dfs_test = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_test_file)\n",
        "dfs_validation = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_validation_file)"
      ],
      "metadata": {
        "id": "wTvMuJLvQLDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs_train.show(5)\n",
        "dfs_test.show(5)\n",
        "dfs_validation.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u26PJdQibtkR",
        "outputId": "a2fea392-837e-4b2a-c455-5f56a95a210a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|        industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|301972057|      600| Trustpilot|Price Comparison|My experience is ...|[[Staff support: ...|['staff-support.a...|\n",
            "|301982453|      514|Google Play|         Banking|I love it so hand...|[[Company brand: ...|['company-brand.g...|\n",
            "|301980653|      369|Google Play|    Ride Hailing|  Sometimes it takes|[[Company brand: ...|['company-brand.g...|\n",
            "|301979991|      727|Apple Store|         Fashion|This is the worst...|[[Logistics rides...|['logistics-rides...|\n",
            "|301984330|      549|Google Play|  Travel Booking|So easy & loads o...|[[Company brand: ...|['company-brand.g...|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|    industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "|301982094|      514|Google Play|     Banking|Very useful and e...|[[Purchase bookin...|['purchase-bookin...|\n",
            "|301981085|      369|Google Play|Ride Hailing|easy to use.gud r...|[[Staff support: ...|['staff-support.a...|\n",
            "|301986508|      685|Google Play|     Trading|            money üòÅ|[[Company brand: ...|['company-brand.g...|\n",
            "|301981875|      514|Google Play|     Banking|      Great facility|[[Company brand: ...|['company-brand.g...|\n",
            "|301977341|      411|Apple Store|   Groceries|Love doing my ORG...|[[Purchase bookin...|['purchase-bookin...|\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+---------+-----------+--------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|      industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+--------------+--------------------+--------------------+--------------------+\n",
            "|610309432|     5827|Google Play|    Consulting|How do I retrieve...|[[Account managem...|['account-managem...|\n",
            "|301974039|      616| Trustpilot|       Fashion|Super fast delive...|[[Logistics rides...|['logistics-rides...|\n",
            "|301983653|      549|Google Play|Travel Booking|Great tool for ch...|[[Online experien...|['online-experien...|\n",
            "|301985771|      616|Google Play|       Fashion|Fast shipping but...|[[Logistics rides...|['logistics-rides...|\n",
            "|301980111|      727|Apple Store|       Fashion|Great app. Easy t...|[[Staff support: ...|['staff-support.a...|\n",
            "+---------+---------+-----------+--------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File was not loading. Implemented a Workaround by converting the pandas version\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "of the parquet file converted to CSV and read into spark to create a spark dataframe. Maybe delete this segment later as problem was resolved.  "
      ],
      "metadata": {
        "id": "v5cc0tMjaYRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Try with pyarrow first to validate the file\n",
        "try:\n",
        "    dfp_train = pq.read_table(parquet_train_file)\n",
        "    dfp_test = pq.read_table(parquet_train_file)\n",
        "    dfp_validation = pq.read_table(parquet_validation_file)\n",
        "    print(\"File can be read with pyarrow\")\n",
        "except Exception as e:\n",
        "    print(f\"Pyarrow error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8zGv69MPmIV",
        "outputId": "aa936242-84a5-45c9-b791-161fdcda9d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File can be read with pyarrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*"
      ],
      "metadata": {
        "id": "8w_7RGLkjact"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training file row count:\", dfs_train.count())\n",
        "dfs_train.printSchema()\n",
        "print(\"Testing file row count:\", dfs_test.count())\n",
        "dfs_test.printSchema()\n",
        "print(\"Validation file row count:\", dfs_validation.count())\n",
        "dfs_validation.printSchema()"
      ],
      "metadata": {
        "id": "f_nMCgTX5Ivi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af6c04d-6053-4f48-d8b3-e7209462361e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file row count: 7930\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n",
            "Testing file row count: 1587\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n",
            "Validation file row count: 1057\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#findspark.init()\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n"
      ],
      "metadata": {
        "id": "nUmFm9EZ51ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd.read_parquet(parquet_file, engine='auto')\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Count nulls in each column - not working\n",
        "#null_counts = test_df.select([col(c).isNull().sum().alias(c) for c in test_df.columns])\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "null_counts = dfs_train.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_train.columns])\n",
        "\n",
        "null_counts.show()\n",
        "# drop nulls\n",
        "dfs_train_clean = dfs_train.dropna()\n",
        "dfs_train_clean.show()\n"
      ],
      "metadata": {
        "id": "MFVBX1FfdmFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66a1e31-23cb-4513-cad7-8eb7211eb368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "| id|org_index|data_source|industry|text|labels|label_codes|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "|  0|        0|          0|       0|   0|     0|          0|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|        industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|301972057|      600| Trustpilot|Price Comparison|My experience is ...|[[Staff support: ...|['staff-support.a...|\n",
            "|301982453|      514|Google Play|         Banking|I love it so hand...|[[Company brand: ...|['company-brand.g...|\n",
            "|301980653|      369|Google Play|    Ride Hailing|  Sometimes it takes|[[Company brand: ...|['company-brand.g...|\n",
            "|301979991|      727|Apple Store|         Fashion|This is the worst...|[[Logistics rides...|['logistics-rides...|\n",
            "|301984330|      549|Google Play|  Travel Booking|So easy & loads o...|[[Company brand: ...|['company-brand.g...|\n",
            "|301979193|      616|Apple Store|         Fashion|Not all reviews a...|[[Company brand: ...|['company-brand.r...|\n",
            "|301972213|      600| Trustpilot|Price Comparison|very easy to use ...|[[Purchase bookin...|['purchase-bookin...|\n",
            "|301984266|      549|Google Play|  Travel Booking|Very useful, espe...|[[Company brand: ...|['company-brand.g...|\n",
            "|301988812|      727|Google Play|         Fashion|Great service, ea...|[[Staff support: ...|['staff-support.a...|\n",
            "|301983615|      411|Google Play|       Groceries| great app well done|[[Online experien...|['online-experien...|\n",
            "|301978802|      616|Apple Store|         Fashion|          Very clear|[[Purchase bookin...|['purchase-bookin...|\n",
            "|301987981|      727|Google Play|         Fashion|Always great! Whe...|[[Online experien...|['online-experien...|\n",
            "|301980748|      369|Google Play|    Ride Hailing|My cellphone was ...|[[Company brand: ...|['company-brand.g...|\n",
            "|301978518|      549|Apple Store|  Travel Booking|Have used ORG549 ...|[[Value: Discount...|['value.discounts...|\n",
            "|301972776|      600| Trustpilot|Price Comparison|Brilliant, saved ...|[[Company brand: ...|['company-brand.g...|\n",
            "|301985269|      727|Google Play|         Fashion|         Easy to use|[[Purchase bookin...|['purchase-bookin...|\n",
            "|301986668|      685|Google Play|         Trading|Very easy to act....|[[Purchase bookin...|['purchase-bookin...|\n",
            "|301982712|      411|Google Play|       Groceries|Can't register, d...|[[Account managem...|['account-managem...|\n",
            "|301977112|      411|Apple Store|       Groceries|I always use the ...|[[Company brand: ...|['company-brand.g...|\n",
            "|301982692|      411|Google Play|       Groceries|Much improved ove...|[[Online experien...|['online-experien...|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Initialize Spark session\n",
        "#spark = SparkSession.builder.appName(\"ABSA_Project\").getOrCreate()\n",
        "\n",
        "# Assuming the dataframe is already loaded as 'df'\n",
        "# If not, you can load it like this:\n",
        "# df = spark.read.csv(\"export_45.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Display the schema to understand the data structure\n",
        "#dfp_train.printSchema()\n",
        "# Let's extract the categories from label_codes\n",
        "# First, we need to convert the string representation of label codes to actual lists\n",
        "# We'll use a UDF (User Defined Function) for this\n",
        "\n",
        "# Define a UDF to extract just the category part from each label code\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    # Convert string representation to list\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        # Extract category part (remove the sentiment indicator at the end)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "        return categories\n",
        "    return []\n",
        "\n",
        "#dfs_train = spark.createDataFrame(dfp_train)     #pandas df has no ... withColumn so we need to convert it - Oops it's not a pandas df, it's a pyarrow Table that needs to be first converted to a pandas df\n",
        "#dfp_train = dfp_train.to_pandas()\n",
        "#dfs_train = spark.createDataFrame(dfp_train)\n",
        "##It seems that the to_pandas method is not available for Spark DataFrames -  use the collect method to convert the Spark DataFrame to a Pandas DataFrame:\n",
        "\n",
        "dfp_train = spark.read.parquet(parquet_train_file)\n",
        "dfp_train = dfp_train.collect()\n",
        "dfs_train = spark.createDataFrame(dfp_train)\n"
      ],
      "metadata": {
        "id": "gH6mayVWnipM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_categories = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "# Create a list of all unique categories from the dataset\n",
        "df_with_categories.printSchema()\n",
        "df_with_categories.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSDhKNxTpY3R",
        "outputId": "ba1fd310-2a49-419f-b1ce-8a75ce310c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            " |-- categories: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|        industry|                text|              labels|         label_codes|          categories|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|301972057|      600| Trustpilot|Price Comparison|My experience is ...|[[Staff support: ...|['staff-support.a...|[staff-support.at...|\n",
            "|301982453|      514|Google Play|         Banking|I love it so hand...|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|\n",
            "|301980653|      369|Google Play|    Ride Hailing|  Sometimes it takes|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|\n",
            "|301979991|      727|Apple Store|         Fashion|This is the worst...|[[Logistics rides...|['logistics-rides...|[logistics-rides....|\n",
            "|301984330|      549|Google Play|  Travel Booking|So easy & loads o...|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_categories = df_with_categories.select(explode(\"categories\")).distinct().collect()\n",
        "all_categories = [row[0] for row in all_categories]"
      ],
      "metadata": {
        "id": "fn-fUt26qOo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in all_categories:\n",
        "  print(cat + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd0-2x4Lu2ys",
        "outputId": "34343556-1883-4dbc-d6e9-f1f1af8990f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value.price-value-for-money\n",
            "\n",
            "company-brand.general-satisfaction\n",
            "\n",
            "staff-support.attitude-of-staff\n",
            "\n",
            "staff-support.email\n",
            "\n",
            "company-brand.reviews\n",
            "\n",
            "online-experience.app-website\n",
            "\n",
            "purchase-booking-experience.ease-of-use\n",
            "\n",
            "company-brand.competitor\n",
            "\n",
            "value.discounts-promotions\n",
            "\n",
            "logistics-rides.speed\n",
            "\n",
            "account-management.account-access\n",
            "\n",
            "staff-support.phone\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each category, create a binary indicator column (1 if present, 0 if not)\n",
        "for cat in all_categories:\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{cat.replace('-', '_').replace('.', '_')}\",\n",
        "        array_contains(col(\"categories\"), cat).cast(\"integer\")\n",
        "    )\n"
      ],
      "metadata": {
        "id": "suGL6w1BwrI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_categories.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe116bc3-4dfb-4213-89b9-d00cb9c27952",
        "id": "JTvwUNh-wrnO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+-------------------------------+--------------------------------------+-----------------------------------+-----------------------+-------------------------+---------------------------------+-------------------------------------------+----------------------------+------------------------------+-------------------------+-------------------------------------+-----------------------+\n",
            "|       id|org_index|data_source|        industry|                text|              labels|         label_codes|          categories|has_value_price_value_for_money|has_company_brand_general_satisfaction|has_staff_support_attitude_of_staff|has_staff_support_email|has_company_brand_reviews|has_online_experience_app_website|has_purchase_booking_experience_ease_of_use|has_company_brand_competitor|has_value_discounts_promotions|has_logistics_rides_speed|has_account_management_account_access|has_staff_support_phone|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+-------------------------------+--------------------------------------+-----------------------------------+-----------------------+-------------------------+---------------------------------+-------------------------------------------+----------------------------+------------------------------+-------------------------+-------------------------------------+-----------------------+\n",
            "|301972057|      600| Trustpilot|Price Comparison|My experience is ...|[[Staff support: ...|['staff-support.a...|[staff-support.at...|                              0|                                     1|                                  1|                      0|                        1|                                0|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301982453|      514|Google Play|         Banking|I love it so hand...|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|                              0|                                     1|                                  0|                      0|                        0|                                0|                                          0|                           1|                             0|                        0|                                    0|                      0|\n",
            "|301980653|      369|Google Play|    Ride Hailing|  Sometimes it takes|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|                              0|                                     1|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301979991|      727|Apple Store|         Fashion|This is the worst...|[[Logistics rides...|['logistics-rides...|[logistics-rides....|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           1|                             0|                        1|                                    0|                      0|\n",
            "|301984330|      549|Google Play|  Travel Booking|So easy & loads o...|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|                              0|                                     1|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301979193|      616|Apple Store|         Fashion|Not all reviews a...|[[Company brand: ...|['company-brand.r...|[company-brand.re...|                              0|                                     0|                                  0|                      0|                        1|                                1|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301972213|      600| Trustpilot|Price Comparison|very easy to use ...|[[Purchase bookin...|['purchase-bookin...|[purchase-booking...|                              1|                                     0|                                  0|                      0|                        0|                                1|                                          1|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301984266|      549|Google Play|  Travel Booking|Very useful, espe...|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|                              0|                                     1|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301988812|      727|Google Play|         Fashion|Great service, ea...|[[Staff support: ...|['staff-support.a...|[staff-support.at...|                              0|                                     1|                                  1|                      0|                        0|                                0|                                          1|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301983615|      411|Google Play|       Groceries| great app well done|[[Online experien...|['online-experien...|[online-experienc...|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301978802|      616|Apple Store|         Fashion|          Very clear|[[Purchase bookin...|['purchase-bookin...|[purchase-booking...|                              0|                                     0|                                  0|                      0|                        0|                                0|                                          1|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301987981|      727|Google Play|         Fashion|Always great! Whe...|[[Online experien...|['online-experien...|[online-experienc...|                              0|                                     1|                                  0|                      0|                        0|                                1|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301980748|      369|Google Play|    Ride Hailing|My cellphone was ...|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|                              0|                                     1|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301978518|      549|Apple Store|  Travel Booking|Have used ORG549 ...|[[Value: Discount...|['value.discounts...|[value.discounts-...|                              0|                                     0|                                  0|                      0|                        1|                                1|                                          0|                           0|                             1|                        0|                                    0|                      0|\n",
            "|301972776|      600| Trustpilot|Price Comparison|Brilliant, saved ...|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|                              1|                                     1|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301985269|      727|Google Play|         Fashion|         Easy to use|[[Purchase bookin...|['purchase-bookin...|[purchase-booking...|                              0|                                     0|                                  0|                      0|                        0|                                0|                                          1|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301986668|      685|Google Play|         Trading|Very easy to act....|[[Purchase bookin...|['purchase-bookin...|[purchase-booking...|                              0|                                     0|                                  0|                      0|                        0|                                0|                                          1|                           0|                             0|                        0|                                    1|                      0|\n",
            "|301982712|      411|Google Play|       Groceries|Can't register, d...|[[Account managem...|['account-managem...|[account-manageme...|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           0|                             0|                        0|                                    1|                      0|\n",
            "|301977112|      411|Apple Store|       Groceries|I always use the ...|[[Company brand: ...|['company-brand.g...|[company-brand.ge...|                              0|                                     1|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|301982692|      411|Google Play|       Groceries|Much improved ove...|[[Online experien...|['online-experien...|[online-experienc...|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           1|                             0|                        0|                                    0|                      0|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+-------------------------------+--------------------------------------+-----------------------------------+-----------------------+-------------------------+---------------------------------+-------------------------------------------+----------------------------+------------------------------+-------------------------+-------------------------------------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_categories['categories'].values[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS3XhKgjrKYq",
        "outputId": "b7e14ff0-5699-4b83-a8dd-ac15c58a74ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'categories[values][0]'>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Select relevant columns for the ACD task\n",
        "feature_cols = [f\"has_{category.replace('-', '_').replace('.', '_')}\" for category in all_categories]\n",
        "acd_df = df_with_categories.select(\"id\", \"text\", *feature_cols)\n",
        "\n",
        "# Convert text to features using TF-IDF\n",
        "# First, tokenize the text\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(acd_df)\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered_data = remover.transform(wordsData)\n",
        "\n",
        "# Convert words to term frequency features\n",
        "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"tf\", minDF=2.0)\n",
        "cv_model = cv.fit(filtered_data)\n",
        "tf_data = cv_model.transform(filtered_data)\n",
        "\n",
        "# Convert term frequency features to TF-IDF\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
        "idf_model = idf.fit(tf_data)\n",
        "tfidf_data = idf_model.transform(tf_data)\n",
        "\n",
        "# Final dataset ready for modeling\n",
        "final_acd_df = tfidf_data.select(\"id\", \"features\", *feature_cols)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = final_acd_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Cache the datasets for faster processing\n",
        "train_data.cache()\n",
        "test_data.cache()\n",
        "\n",
        "# Now you can proceed with training your models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw5lYX0_LauT",
        "outputId": "383b5a70-7788-41c4-c162-516ff9ba0254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, features: vector, has_value_price_value_for_money: int, has_company_brand_general_satisfaction: int, has_staff_support_attitude_of_staff: int, has_staff_support_email: int, has_company_brand_reviews: int, has_online_experience_app_website: int, has_purchase_booking_experience_ease_of_use: int, has_company_brand_competitor: int, has_value_discounts_promotions: int, has_logistics_rides_speed: int, has_account_management_account_access: int, has_staff_support_phone: int]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since you're working with text data, you'll need to extract relevant features from each document. Some common techniques include:\n",
        "#Tokenization (splitting text into individual words or tokens)\n",
        "#Stopword removal (removing common words like \"the\", \"and\", etc.)\n",
        "#Stemming or Lemmatization (reducing words to their base form)\n",
        "#Part-of-speech tagging (identifying grammatical categories of each word)\n",
        "# Create a SparkSession with your test/train/validate data sets\n",
        "#spark = SparkSession.builder.appName(\"Aspect-Based Sentiment Analysis\").getOrCreate()\n",
        "\n",
        "# Load the training and validate datasets into DataFrames\n",
        "#train_df = spark.read.csv(\"path/to/train/dataset\", header=True, inferSchema=True)\n",
        "#val_df = spark.read.csv(\"path/to/val/dataset\", header=True, inferSchema=True)\n",
        "\n",
        "# Define a function to extract features from each column in the dataset\n",
        "def feature_extraction(df):\n",
        "    # Tokenization and stopword removal\n",
        "    df = df.select(\n",
        "        explode(col(\"text\").cast(\"string\")).alias(\"tokens\"),\n",
        "        explode(col(\"sentiment\").cast(\"string\")).alias(\"sentiment\")\n",
        "    )\n",
        "\n",
        "    # Stemming or Lemmatization\n",
        "    df = df.withColumn(\n",
        "        \"stemmed_tokens\",\n",
        "        df.tokens.map(lambda x: x.lower() if x.isnumeric() else x)  # Remove numbers and convert to lowercase\n",
        "    )\n",
        "\n",
        "    # Part-of-speech tagging (optional)\n",
        "    #df = df.withColumn(\"pos_tags\", explode(col(\"word\").cast(\"string\")).map(lambda x: {\"POS\": \"NNP\"} if x.isnumeric() else {\"POS\": \"NOUN\"})  # Example\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "cCPtQoKDXbiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(acd_df)\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered_data = remover.transform(wordsData)\n",
        "\n",
        "# Convert words to term frequency features\n",
        "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"tf\", minDF=2.0)\n",
        "cv_model = cv.fit(filtered_data)\n",
        "tf_data = cv_model.transform(filtered_data)\n",
        "\n",
        "# Convert term frequency features to TF-IDF\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
        "idf_model = idf.fit(tf_data)\n",
        "tfidf_data = idf_model.transform(tf_data)\n",
        "\n",
        "# Final dataset ready for modeling\n",
        "final_acd_df = tfidf_data.select(\"id\", \"features\", *feature_cols)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = final_acd_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Cache the datasets for faster processing\n",
        "train_data.cache()\n",
        "test_data.cache()"
      ],
      "metadata": {
        "id": "XeoRym6BzNyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6195852e-c6a1-4d8b-d1e5-44da85ddeb17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, features: vector, has_value_price_value_for_money: int, has_company_brand_general_satisfaction: int, has_staff_support_attitude_of_staff: int, has_staff_support_email: int, has_company_brand_reviews: int, has_online_experience_app_website: int, has_purchase_booking_experience_ease_of_use: int, has_company_brand_competitor: int, has_value_discounts_promotions: int, has_logistics_rides_speed: int, has_account_management_account_access: int, has_staff_support_phone: int]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YdoY8B4lQQ6",
        "outputId": "bbf0223c-a1b4-4ae0-eddf-dd43468c889d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+-------------------------------+--------------------------------------+-----------------------------------+-----------------------+-------------------------+---------------------------------+-------------------------------------------+----------------------------+------------------------------+-------------------------+-------------------------------------+-----------------------+\n",
            "|       id|            features|has_value_price_value_for_money|has_company_brand_general_satisfaction|has_staff_support_attitude_of_staff|has_staff_support_email|has_company_brand_reviews|has_online_experience_app_website|has_purchase_booking_experience_ease_of_use|has_company_brand_competitor|has_value_discounts_promotions|has_logistics_rides_speed|has_account_management_account_access|has_staff_support_phone|\n",
            "+---------+--------------------+-------------------------------+--------------------------------------+-----------------------------------+-----------------------+-------------------------+---------------------------------+-------------------------------------------+----------------------------+------------------------------+-------------------------+-------------------------------------+-----------------------+\n",
            "|171981739|(5782,[52,230,268...|                              0|                                     0|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    1|                      0|\n",
            "|172007084|(5782,[230,517],[...|                              0|                                     0|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    1|                      0|\n",
            "|172336604|(5782,[24,312,701...|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|172338532|(5782,[147,518,18...|                              0|                                     0|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    1|                      0|\n",
            "|172338766|(5782,[0,488,518,...|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|177015284|(5782,[0,24,107,7...|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|182304147|(5782,[34,580,709...|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|183658960|(5782,[746],[5.93...|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "|184656664|(5782,[179,492,11...|                              0|                                     0|                                  0|                      0|                        0|                                0|                                          0|                           0|                             0|                        0|                                    1|                      0|\n",
            "|190663704|(5782,[73,310,357...|                              0|                                     0|                                  0|                      0|                        0|                                1|                                          0|                           0|                             0|                        0|                                    0|                      0|\n",
            "+---------+--------------------+-------------------------------+--------------------------------------+-----------------------------------+-----------------------+-------------------------+---------------------------------+-------------------------------------------+----------------------------+------------------------------+-------------------------+-------------------------------------+-----------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Function to train and evaluate an SVM model for one category\n",
        "def train_svm_for_category(category, train_data, test_data):\n",
        "    # Prepare label column name\n",
        "    label_col = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "\n",
        "    # Create SVM model\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=label_col, maxIter=10)\n",
        "\n",
        "    # Train the model\n",
        "    model = svm.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # For binary classification metrics (precision, recall)\n",
        "    binary_evaluator = BinaryClassificationEvaluator(\n",
        "        labelCol=label_col, rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "    # For F1 score (use MulticlassClassificationEvaluator)\n",
        "    multi_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "    # Calculate AUC (Area Under ROC)\n",
        "    auc = binary_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate F1\n",
        "    f1 = multi_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate precision and recall manually\n",
        "    # First, get true positives, false positives, true negatives, false negatives\n",
        "    tp = predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 1)).count()\n",
        "    fp = predictions.filter((col(label_col) == 0) & (col(\"prediction\") == 1)).count()\n",
        "    fn = predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 0)).count()\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"category\": category,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"auc\": auc\n",
        "    }"
      ],
      "metadata": {
        "id": "024iF0ZjFCon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Spark DataFrame to Pandas for deep learning\n",
        "# Collect the data (be careful with large datasets)\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "# We need to convert the sparse vector features to numpy arrays\n",
        "from pyspark.ml.linalg import SparseVector\n",
        "\n",
        "def sparse_to_array(sparse_vector):\n",
        "    return sparse_vector.toArray() if isinstance(sparse_vector, SparseVector) else sparse_vector\n",
        "\n",
        "# Convert features to numpy arrays\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Prepare X (features) and y (labels) for training\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "\n",
        "# Get all label columns\n",
        "label_cols = [col for col in train_pandas.columns if col.startswith('has_')]\n",
        "y_train = train_pandas[label_cols].values\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "# Now build a deep learning model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define model architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "results = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {results[0]}\")\n",
        "print(f\"Test Accuracy: {results[1]}\")\n",
        "print(f\"Test Precision: {results[2]}\")\n",
        "print(f\"Test Recall: {results[3]}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate F1 score manually\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "precision = precision_score(y_test, y_pred_binary, average='micro')\n",
        "recall = recall_score(y_test, y_pred_binary, average='micro')\n",
        "f1 = f1_score(y_test, y_pred_binary, average='micro')\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "DNmIWqJYF47I",
        "outputId": "40c74337-5980-4d80-efa5-606d5447f44a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.3194 - loss: 0.5167 - precision: 0.3219 - recall: 0.3179 - val_accuracy: 0.4793 - val_loss: 0.2767 - val_precision: 0.6927 - val_recall: 0.3246\n",
            "Epoch 2/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.5241 - loss: 0.2711 - precision: 0.7343 - recall: 0.4372 - val_accuracy: 0.5863 - val_loss: 0.2168 - val_precision: 0.7719 - val_recall: 0.5384\n",
            "Epoch 3/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6367 - loss: 0.1811 - precision: 0.8423 - recall: 0.6670 - val_accuracy: 0.6089 - val_loss: 0.1962 - val_precision: 0.7821 - val_recall: 0.6212\n",
            "Epoch 4/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6494 - loss: 0.1396 - precision: 0.8751 - recall: 0.7651 - val_accuracy: 0.6214 - val_loss: 0.1939 - val_precision: 0.7943 - val_recall: 0.6337\n",
            "Epoch 5/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.6639 - loss: 0.1130 - precision: 0.9028 - recall: 0.8231 - val_accuracy: 0.6315 - val_loss: 0.1960 - val_precision: 0.7958 - val_recall: 0.6533\n",
            "Epoch 6/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.6928 - loss: 0.0947 - precision: 0.9164 - recall: 0.8595 - val_accuracy: 0.6308 - val_loss: 0.2045 - val_precision: 0.7848 - val_recall: 0.6588\n",
            "Epoch 7/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6796 - loss: 0.0834 - precision: 0.9221 - recall: 0.8759 - val_accuracy: 0.6151 - val_loss: 0.2158 - val_precision: 0.7827 - val_recall: 0.6759\n",
            "Epoch 8/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6859 - loss: 0.0791 - precision: 0.9283 - recall: 0.8823 - val_accuracy: 0.6198 - val_loss: 0.2214 - val_precision: 0.7843 - val_recall: 0.6658\n",
            "Epoch 9/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6875 - loss: 0.0700 - precision: 0.9348 - recall: 0.8953 - val_accuracy: 0.6159 - val_loss: 0.2323 - val_precision: 0.7713 - val_recall: 0.6869\n",
            "Epoch 10/10\n",
            "\u001b[1m161/161\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.6867 - loss: 0.0653 - precision: 0.9399 - recall: 0.9087 - val_accuracy: 0.6097 - val_loss: 0.2397 - val_precision: 0.7733 - val_recall: 0.6744\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5472 - loss: 0.2956 - precision: 0.7514 - recall: 0.6550\n",
            "Test Loss: 0.2767924666404724\n",
            "Test Accuracy: 0.5537352561950684\n",
            "Test Precision: 0.7569807767868042\n",
            "Test Recall: 0.6520105004310608\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "Precision: 0.7569808027923212\n",
            "Recall: 0.652010522360015\n",
            "F1 Score: 0.7005855037351101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "# For simplicity, let's implement a binary classification model for each category\n",
        "# In practice, you might want to use a multi-label approach\n",
        "\n",
        "# Function to train and evaluate an SVM model for one category\n",
        "def train_svm_for_category(category, train_data, test_data):\n",
        "    # Prepare label column name\n",
        "    label_col = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "\n",
        "    # Create SVM model\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=label_col, maxIter=10)\n",
        "\n",
        "    # Train the model\n",
        "    model = svm.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # Evaluate model\n",
        "   # evaluator = MulticlassClassificationEvaluator(\n",
        "   #     labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
        "   # f1 = evaluator.evaluate(predictions)\n",
        "    #\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col,\n",
        "                                              predictionCol=\"prediction\",\n",
        "                                              metricName=\"weightedPrecision\")\n",
        "precision = evaluator.evaluate(predictions)\n",
        "print(\"Weighted Precision:\", precision)\n",
        "    evaluator.setMetricName(\"precision\")\n",
        "    precision = evaluator.evaluate(predictions)\n",
        "\n",
        "    evaluator.setMetricName(\"recall\")\n",
        "    recall = evaluator.evaluate(predictions)\n",
        "\n",
        "    return {\"category\": category, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Train models for all categories\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "    result = train_svm_for_category(category, train_data, test_data)\n",
        "    svm_results.append(result)\n",
        "\n",
        "# Print results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "svm_results_df.show()"
      ],
      "metadata": {
        "id": "5I0GaU5Umj0E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "50c20384-9433-485e-8730-ae790b315d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-23-3eaf62c79b90>, line 34)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-3eaf62c79b90>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    evaluator.setMetricName(\"precision\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training SVM models for all categories...\")\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "    try:\n",
        "        result = train_svm_for_category(category, train_data, test_data)\n",
        "        svm_results.append(result)\n",
        "        print(f\"Completed category: {category}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing category {category}: {str(e)}\")\n",
        "\n",
        "# Create a dataframe with results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "print(\"SVM Results:\")\n",
        "svm_results_df.show()\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_precision = np.mean([r[\"precision\"] for r in svm_results])\n",
        "avg_recall = np.mean([r[\"recall\"] for r in svm_results])\n",
        "avg_f1 = np.mean([r[\"f1\"] for r in svm_results])\n",
        "\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average F1: {avg_f1:.4f}\")\n",
        "\n",
        "# Convert to pandas DataFrame for deep learning model\n",
        "# We'll process a subset of records to handle memory constraints\n",
        "# For a real model, you might want to use a data loader or mini-batches\n",
        "print(\"Preparing data for deep learning model...\")\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit to 1000 samples for demonstration\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Function to convert sparse vector to array\n",
        "def sparse_to_array(v):\n",
        "    if hasattr(v, 'toArray'):\n",
        "        return v.toArray()\n",
        "    return v\n",
        "\n",
        "# Apply conversion function\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Stack features into numpy arrays\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "\n",
        "# Extract labels\n",
        "y_train = train_pandas[feature_cols].values\n",
        "y_test = test_pandas[feature_cols].values\n",
        "\n",
        "print(\"Data preparation for deep learning complete!\")"
      ],
      "metadata": {
        "id": "pjwA9QxfmIkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models for all categories\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "   # result = train_svm_for_category(category, train_data, test_data)\n",
        "    result = train_svm_for_category(category, train_data, test_data)\n",
        "    svm_results.append(result)\n",
        "\n",
        "# Print results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "svm_results_df.show()"
      ],
      "metadata": {
        "id": "qQZ0jSpGKD9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kOVK6q_eKHHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical columns\n",
        "categorical_cols = [c for (c, dtype) in dfs_train_clean.dtypes if dtype == 'string']\n",
        "\n",
        "# Fill missing categorical values\n",
        "for col_name in categorical_cols:\n",
        "    dfs_train_clean = dfs_train_clean.fillna({col_name: \"Unknown\"})\n",
        "\n",
        "# Verify nulls are removed\n",
        "dfs_train_clean.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_train_clean.columns]).show()"
      ],
      "metadata": {
        "id": "n8KTIwGAslCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, mean\n",
        "\n",
        "# Fill numerical columns with mean\n",
        "numeric_cols = [c for (c, dtype) in dfs_train_clean.dtypes if dtype in ('int', 'double', 'float')]\n",
        "for col_name in numeric_cols:\n",
        "    mean_value = test_df.select(mean(col(col_name))).collect()[0][0]\n",
        "    test_df = test_df.fillna({col_name: mean_value})\n",
        "    train_df = train_df.fillna({col_name: mean_value})\n",
        "    validation_df = validation_df.fillna({col_name: mean_value})\n",
        "\n",
        "# Fill categorical columns with \"Unknown\"\n",
        "categorical_cols = [c for (c, dtype) in dfs_train_clean.dtypes if dtype == 'string']\n",
        "for col_name in categorical_cols:\n",
        "    test_df = test_df.fillna({col_name: \"Unknown\"})\n",
        "    train_df = train_df.fillna({col_name: \"Unknown\"})\n",
        "    validation_df = validation_df.fillna({col_name: \"Unknown\"})\n",
        "\n",
        "# Confirm there are no nulls left\n",
        "test_df.show(5)\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + \"_index\").fit(train_df) for col_name in categorical_cols]\n",
        "for indexer in indexers:\n",
        "    train_df = indexer.transform(train_df)\n",
        "    test_df = indexer.transform(test_df)\n",
        "    validation_df = indexer.transform(validation_df)\n",
        "\n",
        "# Drop original categorical columns (optional)\n",
        "train_df = train_df.drop(*categorical_cols)\n",
        "test_df = test_df.drop(*categorical_cols)\n",
        "validation_df = validation_df.drop(*categorical_cols)\n",
        "\n",
        "train_df.show(5)\n"
      ],
      "metadata": {
        "id": "vMzuV-KI1yOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "xram1OgsMmQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Set the thrift string size limit to 1024 bytes\n",
        "thrift_string_size_limit = 1024\n",
        "\n",
        "# Set the thrift container size limit to 1048576 bytes (1MB)\n",
        "thrift_container_size_limit = 1048576\n",
        "\n",
        "# Read the parquet file\n",
        "table = pq.read_table(parquet_train_file, thrift_string_size_limit=thrift_string_size_limit, thrift_container_size_limit=thrift_container_size_limit)"
      ],
      "metadata": {
        "id": "ZQRLtLlD-TqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dfp_train = pd.read_parquet(parquet_train_file, engine=\"pyarrow\")\n",
        "#dfp_train.to_csv(\"converted_train.csv\", index=False)\n",
        "\n",
        "#dfs_train_converted = spark.read.csv(\"converted_train.csv\", header=True, inferSchema=True)\n",
        "#dfs_train_converted.show(5)"
      ],
      "metadata": {
        "id": "NxUveRSnZyVb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}