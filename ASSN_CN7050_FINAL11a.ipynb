{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmetorior/CN7030-/blob/main/ASSN_CN7050_FINAL11a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVZVOi51Nem4"
      },
      "source": [
        "# **CN7050 Intelligent Systems – Assessment &amp; NoteBook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jphepQRESt9r"
      },
      "source": [
        "**Emmet O'Riordan 2760977 - Francesco Beninato 2760980** Group 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC3ndwmBNekj"
      },
      "source": [
        "CN7050 - Design and implementation - 24th March 2025<br>\n",
        "University of East London - Lecturer: Umair ul Hassan<br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **<center>ASPECT-BASED SENTIMENT ANALYSIS (ABSA) & ASPECT CATEGORY DETECTION(ACD)</center>**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhXf8gw7Neia"
      },
      "source": [
        "**<center>INTRODUCTION</center>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT6IX8scNef0"
      },
      "source": [
        "1. **Overview**<br>\n",
        "This assessment is based on the huggingface FABSA model which contains customer data in the form of user reviews, comments. This data can be used to analyse customer responses and sentiments using aspect-based customer analysis (ABSA) and to automatically detect the categories and run machine learning models to  predict the customer sentiment. In our project, we use three types of machine learning: <ul><li>1. traditional machine learning  <li>2. deep learning <li>3. large language models</ul>\n",
        "\n",
        "<br>The FABSA benchmark dataset, labeled with aspect categories and sentiment scores, is used for training.\n",
        "<br> We follow standard procedures here, including the following steps:\n",
        "<ul>\n",
        "<li>Data preprocessing, cleaning\n",
        "<li>Tokenization and feature vectorization\n",
        "<li>Model Training and evaluation for accuracy and F1-score\n",
        "<ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXsxhRvrNedC"
      },
      "source": [
        "2. **The Dataset**<br>\n",
        "The dataset used in this project is the FABSA benchmark dataset available at Hugging Face and contains customer review information. These reviews contain aspect category labels along with sentiment scores (-1 for negative, 0 for neutral, and 1 for positive). https://huggingface.co/datasets/jordiclive/FABSA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO7_6L4Rr2vY"
      },
      "source": [
        "<html>\n",
        "<head>\n",
        "    <title>JordicLive FABSA Dataset Structure</title>\n",
        "    <style>\n",
        "        table {\n",
        "            width: 100%;\n",
        "            border-collapse: collapse;\n",
        "        }\n",
        "        th, td {\n",
        "            border: 1px solid black;\n",
        "            padding: 10px;\n",
        "            text-align: left;\n",
        "        }\n",
        "        th {\n",
        "            background-color: #f2f2f2;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h2>Dataset Structure</h2>\n",
        "    <table>\n",
        "        <tr>\n",
        "            <th>Field</th>\n",
        "            <th>Description</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>id</strong></td>\n",
        "            <td>Unique identifier for each review.</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>org_index</strong></td>\n",
        "            <td>Organization index.</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>data_source</strong></td>\n",
        "            <td>Origin of the review (Google Play, Apple Store, etc.).</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>industry</strong></td>\n",
        "            <td>Industry category of the product/service.</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>text</strong></td>\n",
        "            <td>The review text.</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>labels</strong></td>\n",
        "            <td>Aspect categories and their respective sentiment.</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>label_codes</strong></td>\n",
        "            <td>Encoded aspect and sentiment information.</td>\n",
        "        </tr>\n",
        "    </table>\n",
        "</body>\n",
        "</html>\n",
        "    \n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRdJXEQrPTe3"
      },
      "source": [
        "3. **Options**\n",
        "<br>The original specification required three types of machine learning models to be used, from the three methodologies:\n",
        "<br>1. Traditional Machine Learning\n",
        "<br>2. Deep Learning\n",
        "<br>3. Fine-tuned Large Language Models (LLMS)\n",
        "<br>In terms of these classification problems, we wanted to identify methods that were appropriate. <br>\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Comparison of Machine Learning Approaches for ABSA</title>\n",
        "    <style>\n",
        "        table {\n",
        "            width: 100%;\n",
        "            border-collapse: collapse;\n",
        "        }\n",
        "        th, td {\n",
        "            border: 1px solid black;\n",
        "            padding: 10px;\n",
        "            text-align: left;\n",
        "        }\n",
        "        th {\n",
        "            background-color: #f2f2f2;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h2>Comparison of Machine Learning Approaches for ABSA</h2>\n",
        "    <table>\n",
        "        <tr>\n",
        "            <th>Approach</th>\n",
        "            <th>Subtype 1</th>\n",
        "            <th>Subtype 2</th>\n",
        "            <th>Subtype 3</th>\n",
        "            <th>Subtype 4</th>\n",
        "            <th>Best Choice & Why?</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>A. Traditional ML</strong></td>\n",
        "            <td>Logistic Regression (LR)</td>\n",
        "            <td>Support Vector Machine (SVM)</td>\n",
        "            <td>Naive Bayes (NB)</td>\n",
        "            <td>Random Forest (RF)</td>\n",
        "            <td><strong>SVM</strong>: Best for text classification with sparse vectors, good generalization</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>B. Deep Learning</strong></td>\n",
        "            <td>Convolutional Neural Network (CNN)</td>\n",
        "            <td>Recurrent Neural Network (RNN)</td>\n",
        "            <td>Long Short-Term Memory (LSTM)</td>\n",
        "            <td>Transformer-based models (BERT)</td>\n",
        "            <td><strong>LSTM</strong>: Handles sequential dependencies well, better for sentiment understanding</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><strong>C. Fine-Tuned LLMs</strong></td>\n",
        "            <td>BERT</td>\n",
        "            <td>RoBERTa</td>\n",
        "            <td>T5</td>\n",
        "            <td>GPT-based models</td>\n",
        "            <td><strong>RoBERTa</strong>: Pretrained for sentence classification, robust for ABSA</td>\n",
        "        </tr>\n",
        "    </table>\n",
        "</body>\n",
        "</html>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI5_M9EqtobQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjXKHsE_PAkx"
      },
      "source": [
        "**Multi-Class Classification Problem**\n",
        "\n",
        "In our dataset the user has left a comment and or a feedback rating which is positive, negative or neutral. This is represented by the -1,0,1 values visible in the dataset.\n",
        "\n",
        "<style>\n",
        "  table {\n",
        "    border-collapse: collapse;\n",
        "    width: 100%;\n",
        "  }\n",
        "\n",
        "  th, td {\n",
        "    border: 1px solid #ddd;\n",
        "    padding: 8px;\n",
        "    text-align: left;\n",
        "  }\n",
        "\n",
        "  th {\n",
        "    background-color: #f2f2f2;\n",
        "  }\n",
        "</style>\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>User Rating</th>\n",
        "    <th>Sentiment</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>POSITIVE</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>NEUTRAL</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>-1</td>\n",
        "    <td>NEGATIVE</td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgY3XquaOPnP"
      },
      "source": [
        "This completes our summary and here follows the implementation of the code in Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na6_mfCpNd6b"
      },
      "source": [
        "\n",
        "# **BEGIN CODE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DwxUbJlNGtL"
      },
      "source": [
        "# **Installations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydLyxVtJ6Kvu",
        "outputId": "5006a586-826f-4b4e-fb43-1c194e370eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, pandas\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3 pyarrow-19.0.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (7.16.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (5.10.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (2.18.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert) (5.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert) (1.4.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert) (4.3.7)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert) (4.23.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert) (4.12.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.23.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n",
            "\u001b[31mERROR: Invalid requirement: '%load_ext': Expected package name at the start of dependency specifier\n",
            "    %load_ext\n",
            "    ^\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install findspark\n",
        "!pip install --upgrade pyspark pyarrow pandas\n",
        "!pip install --upgrade torch torchvision torchaudio\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install nbconvert\n",
        "!pip install colab-xterm %load_ext colabxterm.\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t6KnA174v0fU"
      },
      "outputs": [],
      "source": [
        "#Break Colab - Memory issues - hack to cause reboot and increase the memory\n",
        "#d=[]\n",
        "#while(1):\n",
        "#  d.append('1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "Uhc-J4_5jULK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BXoMU4N61zyF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "import pyarrow as pa\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array_contains\n",
        "from pyspark.sql.functions import col, explode, split, array, lit\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "import ast\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r44Ofrp_4Z6S"
      },
      "source": [
        "# **Check Filepaths Exist and that files Are Readable.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8njwXTOmAR0C",
        "outputId": "28e77c7d-54d8-4404-a31e-828ec4d80880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "parquet_validation_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/validation-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_validation_file))\n",
        "\n",
        "parquet_test_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/test-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_test_file))\n",
        "\n",
        "parquet_train_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/train-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_train_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcjt7dWfOWWL"
      },
      "source": [
        "Load files into Spark. We have a train, test and validation file. Each one is loaded in from the parquet downloaded from huggingface.co"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaV1SHPcMhKj"
      },
      "source": [
        "# **CREATE SPARK SESSION, SET OPTIONS, READ DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w14Dd0wJMc7U"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.Builder().master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
        "spark.conf.set(\"spark.sql.parquet.binaryAsString\", \"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wTvMuJLvQLDG"
      },
      "outputs": [],
      "source": [
        "dfs_train = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_train_file)\n",
        "dfs_test = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_test_file)\n",
        "dfs_validation = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_validation_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8VcUfUHOgry"
      },
      "source": [
        "**Checking the file contents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u26PJdQibtkR",
        "outputId": "791904dd-8698-4601-8a6c-7f60bdb13582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|        industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|301972057|      600| Trustpilot|Price Comparison|My experience is ...|[[Staff support: ...|['staff-support.a...|\n",
            "|301982453|      514|Google Play|         Banking|I love it so hand...|[[Company brand: ...|['company-brand.g...|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|    industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "|301982094|      514|Google Play|     Banking|Very useful and e...|[[Purchase bookin...|['purchase-bookin...|\n",
            "|301981085|      369|Google Play|Ride Hailing|easy to use.gud r...|[[Staff support: ...|['staff-support.a...|\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+---------+---------+-----------+----------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|  industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+----------+--------------------+--------------------+--------------------+\n",
            "|610309432|     5827|Google Play|Consulting|How do I retrieve...|[[Account managem...|['account-managem...|\n",
            "|301974039|      616| Trustpilot|   Fashion|Super fast delive...|[[Logistics rides...|['logistics-rides...|\n",
            "+---------+---------+-----------+----------+--------------------+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfs_train.show(2)\n",
        "dfs_test.show(2)\n",
        "dfs_validation.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w_7RGLkjact"
      },
      "source": [
        "**Check Schema**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_nMCgTX5Ivi",
        "outputId": "4c057691-2823-4ae2-f3aa-b9d010581ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file row count: 7930\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n",
            "Testing file row count: 1587\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n",
            "Validation file row count: 1057\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training file row count:\", dfs_train.count())\n",
        "dfs_train.printSchema()\n",
        "print(\"Testing file row count:\", dfs_test.count())\n",
        "dfs_test.printSchema()\n",
        "print(\"Validation file row count:\", dfs_validation.count())\n",
        "dfs_validation.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nUmFm9EZ51ga"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "80PqdfPiNZo2"
      },
      "outputs": [],
      "source": [
        "#findspark.init()\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql.functions import col, sum\n",
        "from pyspark.sql.functions import col, array_contains, lit, explode, udf\n",
        "from pyspark.ml.classification import LogisticRegression, LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, HashingTF, IDF, Tokenizer\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for Nulls"
      ],
      "metadata": {
        "id": "upzv0yfsk0ZX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFVBX1FfdmFS",
        "outputId": "78df35d5-c939-425d-b796-bcd93c3c799b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "| id|org_index|data_source|industry|text|labels|label_codes|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "|  0|        0|          0|       0|   0|     0|          0|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "null_counts_train = dfs_train.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_train.columns])\n",
        "null_counts_train.show()\n",
        "null_counts_test = dfs_test.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_test.columns])\n",
        "null_counts_validation = dfs_validation.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_validation.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract Categories and Sentiment Values**"
      ],
      "metadata": {
        "id": "Uqk7DbGec2f-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JeQpjcL3Qv50"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType, StructType, StructField\n",
        "import ast\n",
        "\n",
        "# Extract sentiment values\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_sentiment_values(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [code.rsplit('.', 1)[1] for code in codes]\n",
        "    return []\n",
        "\n",
        "# Extract categories and sentiments as structs\n",
        "@udf(returnType=ArrayType(StructType([\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"sentiment\", StringType(), True)\n",
        "])))\n",
        "\n",
        "def extract_categories_and_sentiments(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [{\"category\": code.rsplit('.', 1)[0], \"sentiment\": code.rsplit('.', 1)[1]} for code in codes]\n",
        "    return []\n",
        "# [(\"category1\", \"-1\"), (\"category2\", \"0\"), ...]\n",
        "# Modify to return two lists instead.\n",
        "\n",
        "# Check if a category has a specific sentiment (using structs)\n",
        "@udf(returnType=\"integer\")\n",
        "def has_sentiment(category_sentiments, target_category, target_sentiment):\n",
        "    if category_sentiments is None:\n",
        "        return 0\n",
        "    for item in category_sentiments:\n",
        "        if item[\"category\"] == target_category and item[\"sentiment\"] == target_sentiment:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "@udf(returnType=ArrayType(StringType()))   # For ACD\n",
        "def extract_categories_only(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [code.rsplit('.', 1)[0] for code in codes]  # Extract only the category part\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX4ziJdJQDSi"
      },
      "source": [
        "# **EXTRACT LABEL DATA TO SEPARATE CATEGORY COLS WITH SENTIMENT**\n",
        "**ALSO DISPLAY CATEGORIES**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract categories and their sentiment values\n",
        "#dfs_train_with_sentiment = dfs_train.withColumn(\"category_sentiments\", extract_categories_and_sentiments(col(\"label_codes\"))).limit(1000)\n",
        "#dfs_test_with_sentiment = dfs_test.withColumn(\"category_sentiments\", extract_categories_and_sentiments(col(\"label_codes\"))).limit(1000)\n",
        "#dfs_validation_with_sentiment = dfs_validation.withColumn(\"category_sentiments\", extract_categories_and_sentiments(col(\"label_codes\"))).limit(1000)#\n",
        "\n",
        "# Step 2: Get all unique categories\n",
        "#(all_categories, sentiments) = dfs_train_with_sentiment.select(explode(\"category_sentiments\")).distinct().rdd.flatMap(lambda x: x[0]).collect()\n",
        "#dfs_train_with_sentiment.show(5)\n",
        "#for category in all_categories:\n",
        "#    print(category)\n",
        "\n",
        "# Step 1: Extract categories and their sentiment values\n",
        "dfs_train_with_sentiment = dfs_train.withColumn(\"category_sentiments\", extract_categories_and_sentiments(col(\"label_codes\"))).limit(2000)\n",
        "dfs_test_with_sentiment = dfs_test.withColumn(\"category_sentiments\", extract_categories_and_sentiments(col(\"label_codes\"))).limit(2000)\n",
        "dfs_validation_with_sentiment = dfs_validation.withColumn(\"category_sentiments\", extract_categories_and_sentiments(col(\"label_codes\"))).limit(2000)\n",
        "\n",
        "# Step 2: Get all unique categories\n",
        "# Original line causing the error:\n",
        "# (all_categories, sentiments) = dfs_train_with_sentiment.select(explode(\"category_sentiments\")).distinct().rdd.flatMap(lambda x: x[0]).collect()\n",
        "\n",
        "# Updated logic to extract categories and sentiments separately:\n",
        "all_categories = dfs_train_with_sentiment.select(explode(\"category_sentiments\")).select(\"col.category\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "sentiments = dfs_train_with_sentiment.select(explode(\"category_sentiments\")).select(\"col.sentiment\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "dfs_train_with_sentiment.show(2) # checking the column has updated.\n",
        "\n",
        "# extract_categories and display\n",
        "print(\"*** LIST OF CATEGORIES EXTRACTED *** \")\n",
        "for category in all_categories:\n",
        "    print(category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1jqV--_XfdC",
        "outputId": "afbda74e-301e-4f6a-c1cf-185f9950e80f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|        industry|                text|              labels|         label_codes| category_sentiments|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|301972057|      600| Trustpilot|Price Comparison|My experience is ...|[[Staff support: ...|['staff-support.a...|[{staff-support.a...|\n",
            "|301982453|      514|Google Play|         Banking|I love it so hand...|[[Company brand: ...|['company-brand.g...|[{company-brand.g...|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "*** LIST OF CATEGORIES EXTRACTED *** \n",
            "staff-support.attitude-of-staff\n",
            "company-brand.reviews\n",
            "company-brand.general-satisfaction\n",
            "company-brand.competitor\n",
            "logistics-rides.speed\n",
            "online-experience.app-website\n",
            "purchase-booking-experience.ease-of-use\n",
            "value.price-value-for-money\n",
            "value.discounts-promotions\n",
            "account-management.account-access\n",
            "staff-support.phone\n",
            "staff-support.email\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ADD COLUMNS FOR SENTIMENT LABELS (POSITIVE, NEGATIVE, NEUTRAL)**"
      ],
      "metadata": {
        "id": "2U23KdfAcpUc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Tuspoo7kH1",
        "outputId": "171ec352-fd85-4cd8-87d5-11f9b77225c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, org_index: bigint, data_source: string, industry: string, text: string, labels: array<array<string>>, label_codes: string, category_sentiments: array<struct<category:string,sentiment:string>>, staff_support_attitude_of_staff_negative: int, staff_support_attitude_of_staff_neutral: int, staff_support_attitude_of_staff_positive: int, company_brand_reviews_negative: int, company_brand_reviews_neutral: int, company_brand_reviews_positive: int, company_brand_general_satisfaction_negative: int, company_brand_general_satisfaction_neutral: int, company_brand_general_satisfaction_positive: int, company_brand_competitor_negative: int, company_brand_competitor_neutral: int, company_brand_competitor_positive: int, logistics_rides_speed_negative: int, logistics_rides_speed_neutral: int, logistics_rides_speed_positive: int, online_experience_app_website_negative: int, online_experience_app_website_neutral: int, online_experience_app_website_positive: int, purchase_booking_experience_ease_of_use_negative: int, purchase_booking_experience_ease_of_use_neutral: int, purchase_booking_experience_ease_of_use_positive: int, value_price_value_for_money_negative: int, value_price_value_for_money_neutral: int, value_price_value_for_money_positive: int, value_discounts_promotions_negative: int, value_discounts_promotions_neutral: int, value_discounts_promotions_positive: int, account_management_account_access_negative: int, account_management_account_access_neutral: int, account_management_account_access_positive: int, staff_support_phone_negative: int, staff_support_phone_neutral: int, staff_support_phone_positive: int, staff_support_email_negative: int, staff_support_email_neutral: int, staff_support_email_positive: int]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import json\n",
        "# For each category extracted, creates new columns for each category sentiment, negative, neutral, and positive sentiment\n",
        "for category in all_categories:\n",
        "#    print(json.dumps(category, sort_keys=True, indent=4)\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    # In the training data, Create columns for negative (-1), neutral (0), and positive (1) sentiment\n",
        "    dfs_train_with_sentiment = dfs_train_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_negative\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"-1\"))\n",
        "    )\n",
        "    dfs_train_with_sentiment = dfs_train_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_neutral\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"0\"))\n",
        "    )\n",
        "    dfs_train_with_sentiment = dfs_train_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_positive\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"1\"))\n",
        "    )\n",
        "\n",
        "    # Test data\n",
        "    dfs_test_with_sentiment = dfs_test_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_negative\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"-1\"))\n",
        "    )\n",
        "    dfs_test_with_sentiment = dfs_test_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_neutral\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"0\"))\n",
        "    )\n",
        "    dfs_test_with_sentiment = dfs_test_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_positive\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"1\"))\n",
        "    )\n",
        "\n",
        "    #Validation\n",
        "    dfs_validation_with_sentiment = dfs_validation_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_negative\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"-1\"))\n",
        "    )\n",
        "    dfs_validation_with_sentiment = dfs_validation_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_neutral\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"0\"))\n",
        "    )\n",
        "    dfs_validation_with_sentiment = dfs_validation_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_positive\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"1\"))\n",
        "    )\n",
        "\n",
        "# Cache the relevant dataframes\n",
        "dfs_train_with_sentiment.cache()\n",
        "dfs_test_with_sentiment.cache()\n",
        "dfs_validation_with_sentiment.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9D36XN2TDk7"
      },
      "source": [
        "# **APPLY TEXT VECTORIZATION**\n",
        "Using the hashingTF vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ch4aQSeQrVgi"
      },
      "outputs": [],
      "source": [
        "# Step 4: Text vectorization with TF-IDF (same as before)\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\", minDocFreq=2)\n",
        "\n",
        "# Apply transformations\n",
        "train_tokenized = tokenizer.transform(dfs_train_with_sentiment)\n",
        "test_tokenized = tokenizer.transform(dfs_test_with_sentiment)\n",
        "\n",
        "train_hashed = hashingTF.transform(train_tokenized)\n",
        "test_hashed = hashingTF.transform(test_tokenized)\n",
        "idf_model = idf.fit(train_hashed)\n",
        "\n",
        "train_tfidf = idf_model.transform(train_hashed)\n",
        "test_tfidf = idf_model.transform(test_hashed)\n",
        "\n",
        "# Step 5: Train sentiment classifiers for each category\n",
        "from pyspark.ml.classification import LinearSVC, LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "sentiment_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dfs_train_with_sentiment.show(5)\n",
        "for category in all_categories:\n",
        "    print(category)\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    print (f\"category {category}\")\n",
        "    dfs_train_with_sentiment.groupBy(f\"{clean_cat}_negative\").count().show()\n",
        "    dfs_train_with_sentiment.groupBy(f\"{clean_cat}_neutral\").count().show()\n",
        "    dfs_train_with_sentiment.groupBy(f\"{clean_cat}_positive\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-et2t3RAkY2",
        "outputId": "fcfb5521-9e2f-4441-b381-2f88c4d5e815"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+----------------------------------------+---------------------------------------+----------------------------------------+------------------------------+-----------------------------+------------------------------+-------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------------+--------------------------------+---------------------------------+------------------------------+-----------------------------+------------------------------+--------------------------------------+-------------------------------------+--------------------------------------+------------------------------------------------+-----------------------------------------------+------------------------------------------------+------------------------------------+-----------------------------------+------------------------------------+-----------------------------------+----------------------------------+-----------------------------------+------------------------------------------+-----------------------------------------+------------------------------------------+----------------------------+---------------------------+----------------------------+----------------------------+---------------------------+----------------------------+\n",
            "|       id|org_index|data_source|        industry|                text|              labels|         label_codes| category_sentiments|staff_support_attitude_of_staff_negative|staff_support_attitude_of_staff_neutral|staff_support_attitude_of_staff_positive|company_brand_reviews_negative|company_brand_reviews_neutral|company_brand_reviews_positive|company_brand_general_satisfaction_negative|company_brand_general_satisfaction_neutral|company_brand_general_satisfaction_positive|company_brand_competitor_negative|company_brand_competitor_neutral|company_brand_competitor_positive|logistics_rides_speed_negative|logistics_rides_speed_neutral|logistics_rides_speed_positive|online_experience_app_website_negative|online_experience_app_website_neutral|online_experience_app_website_positive|purchase_booking_experience_ease_of_use_negative|purchase_booking_experience_ease_of_use_neutral|purchase_booking_experience_ease_of_use_positive|value_price_value_for_money_negative|value_price_value_for_money_neutral|value_price_value_for_money_positive|value_discounts_promotions_negative|value_discounts_promotions_neutral|value_discounts_promotions_positive|account_management_account_access_negative|account_management_account_access_neutral|account_management_account_access_positive|staff_support_phone_negative|staff_support_phone_neutral|staff_support_phone_positive|staff_support_email_negative|staff_support_email_neutral|staff_support_email_positive|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+----------------------------------------+---------------------------------------+----------------------------------------+------------------------------+-----------------------------+------------------------------+-------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------------+--------------------------------+---------------------------------+------------------------------+-----------------------------+------------------------------+--------------------------------------+-------------------------------------+--------------------------------------+------------------------------------------------+-----------------------------------------------+------------------------------------------------+------------------------------------+-----------------------------------+------------------------------------+-----------------------------------+----------------------------------+-----------------------------------+------------------------------------------+-----------------------------------------+------------------------------------------+----------------------------+---------------------------+----------------------------+----------------------------+---------------------------+----------------------------+\n",
            "|301972057|      600| Trustpilot|Price Comparison|My experience is ...|[[Staff support: ...|['staff-support.a...|[{staff-support.a...|                                       1|                                      0|                                       0|                             1|                            0|                             0|                                          1|                                         0|                                          0|                                0|                               0|                                0|                             0|                            0|                             0|                                     0|                                    0|                                     0|                                               0|                                              0|                                               0|                                   0|                                  0|                                   0|                                  0|                                 0|                                  0|                                         0|                                        0|                                         0|                           0|                          0|                           0|                           0|                          0|                           0|\n",
            "|301982453|      514|Google Play|         Banking|I love it so hand...|[[Company brand: ...|['company-brand.g...|[{company-brand.g...|                                       0|                                      0|                                       0|                             0|                            0|                             0|                                          0|                                         0|                                          1|                                0|                               0|                                1|                             0|                            0|                             0|                                     0|                                    0|                                     0|                                               0|                                              0|                                               0|                                   0|                                  0|                                   0|                                  0|                                 0|                                  0|                                         0|                                        0|                                         0|                           0|                          0|                           0|                           0|                          0|                           0|\n",
            "|301980653|      369|Google Play|    Ride Hailing|  Sometimes it takes|[[Company brand: ...|['company-brand.g...|[{company-brand.g...|                                       0|                                      0|                                       0|                             0|                            0|                             0|                                          1|                                         0|                                          0|                                0|                               0|                                0|                             0|                            0|                             0|                                     0|                                    0|                                     0|                                               0|                                              0|                                               0|                                   0|                                  0|                                   0|                                  0|                                 0|                                  0|                                         0|                                        0|                                         0|                           0|                          0|                           0|                           0|                          0|                           0|\n",
            "|301979991|      727|Apple Store|         Fashion|This is the worst...|[[Logistics rides...|['logistics-rides...|[{logistics-rides...|                                       0|                                      0|                                       0|                             0|                            0|                             0|                                          0|                                         0|                                          0|                                1|                               0|                                0|                             1|                            0|                             0|                                     1|                                    0|                                     0|                                               0|                                              0|                                               0|                                   0|                                  0|                                   0|                                  0|                                 0|                                  0|                                         0|                                        0|                                         0|                           0|                          0|                           0|                           0|                          0|                           0|\n",
            "|301984330|      549|Google Play|  Travel Booking|So easy & loads o...|[[Company brand: ...|['company-brand.g...|[{company-brand.g...|                                       0|                                      0|                                       0|                             0|                            0|                             0|                                          0|                                         0|                                          1|                                0|                               0|                                0|                             0|                            0|                             0|                                     0|                                    0|                                     0|                                               0|                                              0|                                               0|                                   0|                                  0|                                   0|                                  0|                                 0|                                  0|                                         0|                                        0|                                         0|                           0|                          0|                           0|                           0|                          0|                           0|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+--------------------+----------------------------------------+---------------------------------------+----------------------------------------+------------------------------+-----------------------------+------------------------------+-------------------------------------------+------------------------------------------+-------------------------------------------+---------------------------------+--------------------------------+---------------------------------+------------------------------+-----------------------------+------------------------------+--------------------------------------+-------------------------------------+--------------------------------------+------------------------------------------------+-----------------------------------------------+------------------------------------------------+------------------------------------+-----------------------------------+------------------------------------+-----------------------------------+----------------------------------+-----------------------------------+------------------------------------------+-----------------------------------------+------------------------------------------+----------------------------+---------------------------+----------------------------+----------------------------+---------------------------+----------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "staff-support.attitude-of-staff\n",
            "company-brand.reviews\n",
            "company-brand.general-satisfaction\n",
            "company-brand.competitor\n",
            "logistics-rides.speed\n",
            "online-experience.app-website\n",
            "purchase-booking-experience.ease-of-use\n",
            "value.price-value-for-money\n",
            "value.discounts-promotions\n",
            "account-management.account-access\n",
            "staff-support.phone\n",
            "staff-support.email\n",
            "category staff-support.attitude-of-staff\n",
            "+----------------------------------------+-----+\n",
            "|staff_support_attitude_of_staff_negative|count|\n",
            "+----------------------------------------+-----+\n",
            "|                                       1|   58|\n",
            "|                                       0|  942|\n",
            "+----------------------------------------+-----+\n",
            "\n",
            "+---------------------------------------+-----+\n",
            "|staff_support_attitude_of_staff_neutral|count|\n",
            "+---------------------------------------+-----+\n",
            "|                                      0| 1000|\n",
            "+---------------------------------------+-----+\n",
            "\n",
            "+----------------------------------------+-----+\n",
            "|staff_support_attitude_of_staff_positive|count|\n",
            "+----------------------------------------+-----+\n",
            "|                                       1|   80|\n",
            "|                                       0|  920|\n",
            "+----------------------------------------+-----+\n",
            "\n",
            "category company-brand.reviews\n",
            "+------------------------------+-----+\n",
            "|company_brand_reviews_negative|count|\n",
            "+------------------------------+-----+\n",
            "|                             1|   16|\n",
            "|                             0|  984|\n",
            "+------------------------------+-----+\n",
            "\n",
            "+-----------------------------+-----+\n",
            "|company_brand_reviews_neutral|count|\n",
            "+-----------------------------+-----+\n",
            "|                            0| 1000|\n",
            "+-----------------------------+-----+\n",
            "\n",
            "+------------------------------+-----+\n",
            "|company_brand_reviews_positive|count|\n",
            "+------------------------------+-----+\n",
            "|                             1|   12|\n",
            "|                             0|  988|\n",
            "+------------------------------+-----+\n",
            "\n",
            "category company-brand.general-satisfaction\n",
            "+-------------------------------------------+-----+\n",
            "|company_brand_general_satisfaction_negative|count|\n",
            "+-------------------------------------------+-----+\n",
            "|                                          1|   78|\n",
            "|                                          0|  922|\n",
            "+-------------------------------------------+-----+\n",
            "\n",
            "+------------------------------------------+-----+\n",
            "|company_brand_general_satisfaction_neutral|count|\n",
            "+------------------------------------------+-----+\n",
            "|                                         0| 1000|\n",
            "+------------------------------------------+-----+\n",
            "\n",
            "+-------------------------------------------+-----+\n",
            "|company_brand_general_satisfaction_positive|count|\n",
            "+-------------------------------------------+-----+\n",
            "|                                          1|  280|\n",
            "|                                          0|  720|\n",
            "+-------------------------------------------+-----+\n",
            "\n",
            "category company-brand.competitor\n",
            "+---------------------------------+-----+\n",
            "|company_brand_competitor_negative|count|\n",
            "+---------------------------------+-----+\n",
            "|                                1|   19|\n",
            "|                                0|  981|\n",
            "+---------------------------------+-----+\n",
            "\n",
            "+--------------------------------+-----+\n",
            "|company_brand_competitor_neutral|count|\n",
            "+--------------------------------+-----+\n",
            "|                               1|    2|\n",
            "|                               0|  998|\n",
            "+--------------------------------+-----+\n",
            "\n",
            "+---------------------------------+-----+\n",
            "|company_brand_competitor_positive|count|\n",
            "+---------------------------------+-----+\n",
            "|                                1|   57|\n",
            "|                                0|  943|\n",
            "+---------------------------------+-----+\n",
            "\n",
            "category logistics-rides.speed\n",
            "+------------------------------+-----+\n",
            "|logistics_rides_speed_negative|count|\n",
            "+------------------------------+-----+\n",
            "|                             1|   45|\n",
            "|                             0|  955|\n",
            "+------------------------------+-----+\n",
            "\n",
            "+-----------------------------+-----+\n",
            "|logistics_rides_speed_neutral|count|\n",
            "+-----------------------------+-----+\n",
            "|                            0| 1000|\n",
            "+-----------------------------+-----+\n",
            "\n",
            "+------------------------------+-----+\n",
            "|logistics_rides_speed_positive|count|\n",
            "+------------------------------+-----+\n",
            "|                             1|   92|\n",
            "|                             0|  908|\n",
            "+------------------------------+-----+\n",
            "\n",
            "category online-experience.app-website\n",
            "+--------------------------------------+-----+\n",
            "|online_experience_app_website_negative|count|\n",
            "+--------------------------------------+-----+\n",
            "|                                     1|  168|\n",
            "|                                     0|  832|\n",
            "+--------------------------------------+-----+\n",
            "\n",
            "+-------------------------------------+-----+\n",
            "|online_experience_app_website_neutral|count|\n",
            "+-------------------------------------+-----+\n",
            "|                                    1|   31|\n",
            "|                                    0|  969|\n",
            "+-------------------------------------+-----+\n",
            "\n",
            "+--------------------------------------+-----+\n",
            "|online_experience_app_website_positive|count|\n",
            "+--------------------------------------+-----+\n",
            "|                                     1|  278|\n",
            "|                                     0|  722|\n",
            "+--------------------------------------+-----+\n",
            "\n",
            "category purchase-booking-experience.ease-of-use\n",
            "+------------------------------------------------+-----+\n",
            "|purchase_booking_experience_ease_of_use_negative|count|\n",
            "+------------------------------------------------+-----+\n",
            "|                                               1|   67|\n",
            "|                                               0|  933|\n",
            "+------------------------------------------------+-----+\n",
            "\n",
            "+-----------------------------------------------+-----+\n",
            "|purchase_booking_experience_ease_of_use_neutral|count|\n",
            "+-----------------------------------------------+-----+\n",
            "|                                              1|    1|\n",
            "|                                              0|  999|\n",
            "+-----------------------------------------------+-----+\n",
            "\n",
            "+------------------------------------------------+-----+\n",
            "|purchase_booking_experience_ease_of_use_positive|count|\n",
            "+------------------------------------------------+-----+\n",
            "|                                               1|  240|\n",
            "|                                               0|  760|\n",
            "+------------------------------------------------+-----+\n",
            "\n",
            "category value.price-value-for-money\n",
            "+------------------------------------+-----+\n",
            "|value_price_value_for_money_negative|count|\n",
            "+------------------------------------+-----+\n",
            "|                                   1|   23|\n",
            "|                                   0|  977|\n",
            "+------------------------------------+-----+\n",
            "\n",
            "+-----------------------------------+-----+\n",
            "|value_price_value_for_money_neutral|count|\n",
            "+-----------------------------------+-----+\n",
            "|                                  0| 1000|\n",
            "+-----------------------------------+-----+\n",
            "\n",
            "+------------------------------------+-----+\n",
            "|value_price_value_for_money_positive|count|\n",
            "+------------------------------------+-----+\n",
            "|                                   1|   95|\n",
            "|                                   0|  905|\n",
            "+------------------------------------+-----+\n",
            "\n",
            "category value.discounts-promotions\n",
            "+-----------------------------------+-----+\n",
            "|value_discounts_promotions_negative|count|\n",
            "+-----------------------------------+-----+\n",
            "|                                  1|   13|\n",
            "|                                  0|  987|\n",
            "+-----------------------------------+-----+\n",
            "\n",
            "+----------------------------------+-----+\n",
            "|value_discounts_promotions_neutral|count|\n",
            "+----------------------------------+-----+\n",
            "|                                 1|    4|\n",
            "|                                 0|  996|\n",
            "+----------------------------------+-----+\n",
            "\n",
            "+-----------------------------------+-----+\n",
            "|value_discounts_promotions_positive|count|\n",
            "+-----------------------------------+-----+\n",
            "|                                  1|   24|\n",
            "|                                  0|  976|\n",
            "+-----------------------------------+-----+\n",
            "\n",
            "category account-management.account-access\n",
            "+------------------------------------------+-----+\n",
            "|account_management_account_access_negative|count|\n",
            "+------------------------------------------+-----+\n",
            "|                                         1|   28|\n",
            "|                                         0|  972|\n",
            "+------------------------------------------+-----+\n",
            "\n",
            "+-----------------------------------------+-----+\n",
            "|account_management_account_access_neutral|count|\n",
            "+-----------------------------------------+-----+\n",
            "|                                        1|   18|\n",
            "|                                        0|  982|\n",
            "+-----------------------------------------+-----+\n",
            "\n",
            "+------------------------------------------+-----+\n",
            "|account_management_account_access_positive|count|\n",
            "+------------------------------------------+-----+\n",
            "|                                         1|    6|\n",
            "|                                         0|  994|\n",
            "+------------------------------------------+-----+\n",
            "\n",
            "category staff-support.phone\n",
            "+----------------------------+-----+\n",
            "|staff_support_phone_negative|count|\n",
            "+----------------------------+-----+\n",
            "|                           1|    9|\n",
            "|                           0|  991|\n",
            "+----------------------------+-----+\n",
            "\n",
            "+---------------------------+-----+\n",
            "|staff_support_phone_neutral|count|\n",
            "+---------------------------+-----+\n",
            "|                          1|    1|\n",
            "|                          0|  999|\n",
            "+---------------------------+-----+\n",
            "\n",
            "+----------------------------+-----+\n",
            "|staff_support_phone_positive|count|\n",
            "+----------------------------+-----+\n",
            "|                           1|   11|\n",
            "|                           0|  989|\n",
            "+----------------------------+-----+\n",
            "\n",
            "category staff-support.email\n",
            "+----------------------------+-----+\n",
            "|staff_support_email_negative|count|\n",
            "+----------------------------+-----+\n",
            "|                           1|    7|\n",
            "|                           0|  993|\n",
            "+----------------------------+-----+\n",
            "\n",
            "+---------------------------+-----+\n",
            "|staff_support_email_neutral|count|\n",
            "+---------------------------+-----+\n",
            "|                          0| 1000|\n",
            "+---------------------------+-----+\n",
            "\n",
            "+----------------------------+-----+\n",
            "|staff_support_email_positive|count|\n",
            "+----------------------------+-----+\n",
            "|                           1|    4|\n",
            "|                           0|  996|\n",
            "+----------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Vs RoBERTa**"
      ],
      "metadata": {
        "id": "80llCturn_IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Add RoBERTa imports\n",
        "import transformers\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "\n",
        "# Convert to pandas for easier processing\n",
        "train_pandas = dfs_train_with_sentiment.toPandas()\n",
        "test_pandas = dfs_test_with_sentiment.toPandas()\n",
        "validation_pandas = dfs_validation_with_sentiment.toPandas()\n",
        "\n",
        "# RoBERTa Tokenizer and Model\n",
        "roberta_model_name = 'roberta-base'\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model_name)\n",
        "roberta_model = TFRobertaModel.from_pretrained(roberta_model_name)\n",
        "\n",
        "# Tokenize text for RoBERTa\n",
        "def roberta_tokenize(texts, max_length=128):\n",
        "    return roberta_tokenizer(\n",
        "        list(texts),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "# Existing Keras Tokenizer for CNN\n",
        "keras_tokenizer = KerasTokenizer(num_words=10000)\n",
        "keras_tokenizer.fit_on_texts(train_pandas['text'])\n",
        "\n",
        "# Convert text to sequences for CNN\n",
        "train_sequences = keras_tokenizer.texts_to_sequences(train_pandas['text'])\n",
        "test_sequences = keras_tokenizer.texts_to_sequences(test_pandas['text'])\n",
        "validation_sequences = keras_tokenizer.texts_to_sequences(validation_pandas['text'])\n",
        "\n",
        "# Pad sequences for CNN\n",
        "max_seq_length = 100\n",
        "train_data_cnn = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data_cnn = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "validation_data_cnn = pad_sequences(validation_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# Tokenize for RoBERTa\n",
        "train_data_roberta = roberta_tokenize(train_pandas['text'])\n",
        "test_data_roberta = roberta_tokenize(test_pandas['text'])\n",
        "validation_data_roberta = roberta_tokenize(validation_pandas['text'])\n",
        "\n",
        "# Deep learning model results\n",
        "dl_sentiment_results = {}\n",
        "\n",
        "# For each category, train deep learning models\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create y labels\n",
        "    # 0: negative, 1: neutral, 2: positive\n",
        "    def create_sentiment_labels(df, category_col):\n",
        "        neg_col = f\"{category_col}_negative\"\n",
        "        neu_col = f\"{category_col}_neutral\"\n",
        "        pos_col = f\"{category_col}_positive\"\n",
        "\n",
        "        y = np.zeros(len(df))\n",
        "        y[df[neg_col] == 1] = 0\n",
        "        y[df[neu_col] == 1] = 1\n",
        "        y[df[pos_col] == 1] = 2\n",
        "        # Default to neutral if no sentiment is available\n",
        "        y[(df[neg_col] == 0) & (df[neu_col] == 0) & (df[pos_col] == 0)] = 1\n",
        "\n",
        "        return y\n",
        "\n",
        "    y_train = create_sentiment_labels(train_pandas, clean_cat)\n",
        "    y_test = create_sentiment_labels(test_pandas, clean_cat)\n",
        "    y_val = create_sentiment_labels(validation_pandas, clean_cat)\n",
        "\n",
        "    # Convert to one-hot encoding\n",
        "    y_train_one_hot = tf.keras.utils.to_categorical(y_train, 3)\n",
        "    y_test_one_hot = tf.keras.utils.to_categorical(y_test, 3)\n",
        "    y_val_one_hot = tf.keras.utils.to_categorical(y_val, 3)\n",
        "\n",
        "    # CNN Model (existing implementation)\n",
        "    vocab_size = min(10000, len(keras_tokenizer.word_index) + 1)\n",
        "    embedding_dim = 100\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    #cnn_model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_length))\n",
        "    cnn_model.add(Embedding(vocab_size, embedding_dim)) # because input_length is deprecated.\n",
        "    cnn_model.add(SpatialDropout1D(0.2))\n",
        "    cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnn_model.add(MaxPooling1D(5))\n",
        "    cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnn_model.add(GlobalMaxPooling1D())\n",
        "    cnn_model.add(Dense(128, activation='relu'))\n",
        "    cnn_model.add(Dropout(0.2))\n",
        "    cnn_model.add(Dense(3, activation='softmax'))  # 3 classes: negative, neutral, positive\n",
        "\n",
        "    cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # RoBERTa Model\n",
        "    roberta_inputs = {\n",
        "        'input_ids': train_data_roberta['input_ids'],\n",
        "        'attention_mask': train_data_roberta['attention_mask']\n",
        "    }\n",
        "\n",
        "    roberta_base_model = roberta_model(roberta_inputs)[0]\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(roberta_base_model)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    roberta_output = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "    roberta_model_custom = tf.keras.Model(\n",
        "        inputs=[\n",
        "            roberta_inputs['input_ids'],\n",
        "            roberta_inputs['attention_mask']\n",
        "        ],\n",
        "        outputs=roberta_output\n",
        "    )\n",
        "\n",
        "    roberta_model_custom.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Early Stopping\n",
        "    from tensorflow.keras.callbacks import EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "    # Train CNN\n",
        "    cnn_history = cnn_model.fit(\n",
        "        train_data_cnn, y_train_one_hot,\n",
        "        validation_data=(validation_data_cnn, y_val_one_hot),\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Train RoBERTa\n",
        "    roberta_history = roberta_model_custom.fit(\n",
        "        x=[\n",
        "            train_data_roberta['input_ids'],\n",
        "            train_data_roberta['attention_mask']\n",
        "        ],\n",
        "        y=y_train_one_hot,\n",
        "        validation_data=(\n",
        "            [validation_data_roberta['input_ids'], validation_data_roberta['attention_mask']],\n",
        "            y_val_one_hot\n",
        "        ),\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Predict and Evaluate CNN\n",
        "    y_pred_probs_cnn = cnn_model.predict(test_data_cnn)\n",
        "    y_pred_cnn = np.argmax(y_pred_probs_cnn, axis=1)\n",
        "    accuracy_cnn = accuracy_score(y_test, y_pred_cnn)\n",
        "    report_cnn = classification_report(y_test, y_pred_cnn, output_dict=True)\n",
        "\n",
        "    # Predict and Evaluate RoBERTa\n",
        "    y_pred_probs_roberta = roberta_model_custom.predict([\n",
        "        test_data_roberta['input_ids'],\n",
        "        test_data_roberta['attention_mask']\n",
        "    ])\n",
        "    y_pred_roberta = np.argmax(y_pred_probs_roberta, axis=1)\n",
        "    accuracy_roberta = accuracy_score(y_test, y_pred_roberta)\n",
        "    report_roberta = classification_report(y_test, y_pred_roberta, output_dict=True)\n",
        "\n",
        "    # Store results\n",
        "    dl_sentiment_results[category] = {\n",
        "        \"CNN\": {\n",
        "            \"accuracy\": accuracy_cnn,\n",
        "            \"f1_neg\": report_cnn['0']['f1-score'] if '0' in report_cnn else 0,\n",
        "            \"f1_neu\": report_cnn['1']['f1-score'] if '1' in report_cnn else 0,\n",
        "            \"f1_pos\": report_cnn['2']['f1-score'] if '2' in report_cnn else 0,\n",
        "            \"f1_weighted\": report_cnn['weighted avg']['f1-score']\n",
        "        },\n",
        "        \"RoBERTa\": {\n",
        "            \"accuracy\": accuracy_roberta,\n",
        "            \"f1_neg\": report_roberta['0']['f1-score'] if '0' in report_roberta else 0,\n",
        "            \"f1_neu\": report_roberta['1']['f1-score'] if '1' in report_roberta else 0,\n",
        "            \"f1_pos\": report_roberta['2']['f1-score'] if '2' in report_roberta else 0,\n",
        "            \"f1_weighted\": report_roberta['weighted avg']['f1-score']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(f\"Category: {category}\")\n",
        "    print(\"CNN Results:\")\n",
        "    print(f\"Accuracy: {accuracy_cnn:.4f}, F1 (weighted): {report_cnn['weighted avg']['f1-score']:.4f}\")\n",
        "    print(\"RoBERTa Results:\")\n",
        "    print(f\"Accuracy: {accuracy_roberta:.4f}, F1 (weighted): {report_roberta['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "    # Visualization code remains the same for both models\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Confusion Matrix for CNN\n",
        "    cm_cnn = confusion_matrix(y_test, y_pred_cnn)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "    plt.title(f'CNN Confusion Matrix - {category}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Confusion Matrix for RoBERTa\n",
        "    cm_roberta = confusion_matrix(y_test, y_pred_roberta)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.heatmap(cm_roberta, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "    plt.title(f'RoBERTa Confusion Matrix - {category}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_{clean_cat}_comparison.png', dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "id": "Fdo3wj3Hn84c",
        "outputId": "f5a99963-ce82-4514-a59c-3b192b4fdc4f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "All `inputs` values must be KerasTensors. Received: inputs=[<tf.Tensor: shape=(1000, 128), dtype=int32, numpy=\narray([[    0,  2387,   676, ...,    54,    17,     2],\n       [    0,   100,   657, ...,     1,     1,     1],\n       [    0, 13624,    24, ...,     1,     1,     1],\n       ...,\n       [    0, 10365,  1365, ...,     1,     1,     1],\n       [    0, 14323,     2, ...,     1,     1,     1],\n       [    0,   100,   465, ...,     1,     1,     1]], dtype=int32)>, <tf.Tensor: shape=(1000, 128), dtype=int32, numpy=\narray([[1, 1, 1, ..., 1, 1, 1],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>] including invalid value [[    0  2387   676 ...    54    17     2]\n [    0   100   657 ...     1     1     1]\n [    0 13624    24 ...     1     1     1]\n ...\n [    0 10365  1365 ...     1     1     1]\n [    0 14323     2 ...     1     1     1]\n [    0   100   465 ...     1     1     1]] of type <class 'tensorflow.python.framework.ops.EagerTensor'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-db5bc2739ef5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mroberta_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     roberta_model_custom = tf.keras.Model(\n\u001b[0m\u001b[1;32m    117\u001b[0m         inputs=[\n\u001b[1;32m    118\u001b[0m             \u001b[0mroberta_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDotNotTrackScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    120\u001b[0m                     \u001b[0;34m\"All `inputs` values must be KerasTensors. Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                     \u001b[0;34mf\"inputs={inputs} including invalid value {x} of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All `inputs` values must be KerasTensors. Received: inputs=[<tf.Tensor: shape=(1000, 128), dtype=int32, numpy=\narray([[    0,  2387,   676, ...,    54,    17,     2],\n       [    0,   100,   657, ...,     1,     1,     1],\n       [    0, 13624,    24, ...,     1,     1,     1],\n       ...,\n       [    0, 10365,  1365, ...,     1,     1,     1],\n       [    0, 14323,     2, ...,     1,     1,     1],\n       [    0,   100,   465, ...,     1,     1,     1]], dtype=int32)>, <tf.Tensor: shape=(1000, 128), dtype=int32, numpy=\narray([[1, 1, 1, ..., 1, 1, 1],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>] including invalid value [[    0  2387   676 ...    54    17     2]\n [    0   100   657 ...     1     1     1]\n [    0 13624    24 ...     1     1     1]\n ...\n [    0 10365  1365 ...     1     1     1]\n [    0 14323     2 ...     1     1     1]\n [    0   100   465 ...     1     1     1]] of type <class 'tensorflow.python.framework.ops.EagerTensor'>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B0YrutLYGLwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "SMm9U9ZCpKBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOGISTIC REGRESSION vs CNNs vs RoBERTa**"
      ],
      "metadata": {
        "id": "Qt6iAfIcqJIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Deep Learning Imports\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Scikit-learn Imports for Logistic Regression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# RoBERTa Imports\n",
        "import transformers\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "\n",
        "# Convert to pandas for easier processing\n",
        "train_pandas = dfs_train_with_sentiment.toPandas()\n",
        "test_pandas = dfs_test_with_sentiment.toPandas()\n",
        "validation_pandas = dfs_validation_with_sentiment.toPandas()\n",
        "\n",
        "# Tokenize text for CNN - For CNN we are using Keras toknizer.\n",
        "keras_tokenizer = KerasTokenizer(num_words=10000)\n",
        "keras_tokenizer.fit_on_texts(train_pandas['text'])\n",
        "\n",
        "# Convert text into sequences for the CNN\n",
        "train_sequences = keras_tokenizer.texts_to_sequences(train_pandas['text'])\n",
        "test_sequences = keras_tokenizer.texts_to_sequences(test_pandas['text'])\n",
        "validation_sequences = keras_tokenizer.texts_to_sequences(validation_pandas['text'])\n",
        "\n",
        "# Pading the sequences for CNN\n",
        "max_seq_length = 100\n",
        "train_data_cnn = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data_cnn = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "validation_data_cnn = pad_sequences(validation_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# RoBERTa Tokenizer and Model\n",
        "roberta_model_name = 'roberta-base'   # this is robertas sentence-transformers model\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model_name)\n",
        "roberta_model = TFRobertaModel.from_pretrained(roberta_model_name)\n",
        "\n",
        "# RoBERTa Tokenization Function\n",
        "def roberta_tokenize(texts, max_length=128):\n",
        "    return roberta_tokenizer(\n",
        "        list(texts),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "# Tokenize for RoBERTa\n",
        "train_data_roberta = roberta_tokenize(train_pandas['text'])\n",
        "test_data_roberta = roberta_tokenize(test_pandas['text'])\n",
        "validation_data_roberta = roberta_tokenize(validation_pandas['text'])\n",
        "\n",
        "# Multi-model sentiment results\n",
        "sentiment_results = {}\n",
        "\n",
        "# Sentiment label creation function\n",
        "def create_sentiment_labels(df, category_col):\n",
        "    neg_col = f\"{category_col}_negative\"\n",
        "    neu_col = f\"{category_col}_neutral\"\n",
        "    pos_col = f\"{category_col}_positive\"\n",
        "\n",
        "    y = np.zeros(len(df))\n",
        "    y[df[neg_col] == 1] = 0\n",
        "    y[df[neu_col] == 1] = 1\n",
        "    y[df[pos_col] == 1] = 2\n",
        "    # Default to neutral if no sentiment is available\n",
        "    y[(df[neg_col] == 0) & (df[neu_col] == 0) & (df[pos_col] == 0)] = 1\n",
        "\n",
        "    return y\n",
        "\n",
        "# Model training and evaluation loop\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Prepare labels\n",
        "    y_train = create_sentiment_labels(train_pandas, clean_cat)\n",
        "    y_test = create_sentiment_labels(test_pandas, clean_cat)\n",
        "    y_val = create_sentiment_labels(validation_pandas, clean_cat)\n",
        "\n",
        "    # One-hot encoding for deep learning models\n",
        "    y_train_one_hot = tf.keras.utils.to_categorical(y_train, 3)\n",
        "    y_test_one_hot = tf.keras.utils.to_categorical(y_test, 3)\n",
        "    y_val_one_hot = tf.keras.utils.to_categorical(y_val, 3)\n",
        "\n",
        "    # 1. CNN Model\n",
        "    vocab_size = min(10000, len(keras_tokenizer.word_index) + 1)\n",
        "    embedding_dim = 100\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_length))\n",
        "    cnn_model.add(SpatialDropout1D(0.2))\n",
        "    cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnn_model.add(MaxPooling1D(5))\n",
        "    cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnn_model.add(GlobalMaxPooling1D())\n",
        "    cnn_model.add(Dense(128, activation='relu'))\n",
        "    cnn_model.add(Dropout(0.2))\n",
        "    cnn_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # 2. Logistic Regression Pipeline\n",
        "    lr_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
        "        ('scaler', StandardScaler(with_mean=False)),\n",
        "        ('classifier', LogisticRegression(\n",
        "            multi_class='multinomial',\n",
        "            solver='lbfgs',\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced'\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # 3. RoBERTa Model\n",
        "    roberta_inputs = {\n",
        "        'input_ids': train_data_roberta['input_ids'],\n",
        "        'attention_mask': train_data_roberta['attention_mask']\n",
        "    }\n",
        "\n",
        "    roberta_base_model = roberta_model(roberta_inputs)[0]\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(roberta_base_model)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    roberta_output = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "    roberta_model_custom = tf.keras.Model(\n",
        "        inputs=[\n",
        "            roberta_inputs['input_ids'],\n",
        "            roberta_inputs['attention_mask']\n",
        "        ],\n",
        "        outputs=roberta_output\n",
        "    )\n",
        "\n",
        "    roberta_model_custom.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Early Stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "    # Training\n",
        "    # CNN Training\n",
        "    cnn_history = cnn_model.fit(\n",
        "        train_data_cnn, y_train_one_hot,\n",
        "        validation_data=(validation_data_cnn, y_val_one_hot),\n",
        "        epochs=5,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Logistic Regression Training\n",
        "    lr_pipeline.fit(train_pandas['text'], y_train)\n",
        "\n",
        "    # RoBERTa Training\n",
        "    roberta_history = roberta_model_custom.fit(\n",
        "        x=[\n",
        "            train_data_roberta['input_ids'],\n",
        "            train_data_roberta['attention_mask']\n",
        "        ],\n",
        "        y=y_train_one_hot,\n",
        "        validation_data=(\n",
        "            [validation_data_roberta['input_ids'], validation_data_roberta['attention_mask']],\n",
        "            y_val_one_hot\n",
        "        ),\n",
        "        epochs=5,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    # CNN Predictions\n",
        "    y_pred_probs_cnn = cnn_model.predict(test_data_cnn)\n",
        "    y_pred_cnn = np.argmax(y_pred_probs_cnn, axis=1)\n",
        "    accuracy_cnn = accuracy_score(y_test, y_pred_cnn)\n",
        "    report_cnn = classification_report(y_test, y_pred_cnn, output_dict=True)\n",
        "\n",
        "    # Logistic Regression Predictions\n",
        "    y_pred_lr = lr_pipeline.predict(test_pandas['text'])\n",
        "    accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "    report_lr = classification_report(y_test, y_pred_lr, output_dict=True)\n",
        "\n",
        "    # RoBERTa Predictions\n",
        "    y_pred_probs_roberta = roberta_model_custom.predict([\n",
        "        test_data_roberta['input_ids'],\n",
        "        test_data_roberta['attention_mask']\n",
        "    ])\n",
        "    y_pred_roberta = np.argmax(y_pred_probs_roberta, axis=1)\n",
        "    accuracy_roberta = accuracy_score(y_test, y_pred_roberta)\n",
        "    report_roberta = classification_report(y_test, y_pred_roberta, output_dict=True)\n",
        "\n",
        "    # Store results\n",
        "    sentiment_results[category] = {\n",
        "        \"CNN\": {\n",
        "            \"accuracy\": accuracy_cnn,\n",
        "            \"f1_neg\": report_cnn['0']['f1-score'] if '0' in report_cnn else 0,\n",
        "            \"f1_neu\": report_cnn['1']['f1-score'] if '1' in report_cnn else 0,\n",
        "            \"f1_pos\": report_cnn['2']['f1-score'] if '2' in report_cnn else 0,\n",
        "            \"f1_weighted\": report_cnn['weighted avg']['f1-score']\n",
        "        },\n",
        "        \"Logistic Regression\": {\n",
        "            \"accuracy\": accuracy_lr,\n",
        "            \"f1_neg\": report_lr['0']['f1-score'] if '0' in report_lr else 0,\n",
        "            \"f1_neu\": report_lr['1']['f1-score'] if '1' in report_lr else 0,\n",
        "            \"f1_pos\": report_lr['2']['f1-score'] if '2' in report_lr else 0,\n",
        "            \"f1_weighted\": report_lr['weighted avg']['f1-score']\n",
        "        },\n",
        "        \"RoBERTa\": {\n",
        "            \"accuracy\": accuracy_roberta,\n",
        "            \"f1_neg\": report_roberta['0']['f1-score'] if '0' in report_roberta else 0,\n",
        "            \"f1_neu\": report_roberta['1']['f1-score'] if '1' in report_roberta else 0,\n",
        "            \"f1_pos\": report_roberta['2']['f1-score'] if '2' in report_roberta else 0,\n",
        "            \"f1_weighted\": report_roberta['weighted avg']['f1-score']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Category: {category}\")\n",
        "    print(\"CNN Results:\")\n",
        "    print(f\"Accuracy: {accuracy_cnn:.4f}, F1 (weighted): {report_cnn['weighted avg']['f1-score']:.4f}\")\n",
        "    print(\"Logistic Regression Results:\")\n",
        "    print(f\"Accuracy: {accuracy_lr:.4f}, F1 (weighted): {report_lr['weighted avg']['f1-score']:.4f}\")\n",
        "    print(\"RoBERTa Results:\")\n",
        "    print(f\"Accuracy: {accuracy_roberta:.4f}, F1 (weighted): {report_roberta['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "    # Visualization by category\n",
        "    # Confusion Matrices\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # CNN Confusion Matrix\n",
        "    plt.subplot(1, 3, 1)\n",
        "    cm_cnn = confusion_matrix(y_test, y_pred_cnn)\n",
        "    sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "    plt.title(f'CNN Confusion Matrix - {category}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Logistic Regression Confusion Matrix\n",
        "    plt.subplot(1, 3, 2)\n",
        "    cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "    sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "    plt.title(f'Logistic Regression Confusion Matrix - {category}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # RoBERTa Confusion Matrix\n",
        "    plt.subplot(1, 3, 3)\n",
        "    cm_roberta = confusion_matrix(y_test, y_pred_roberta)\n",
        "    sns.heatmap(cm_roberta, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "    plt.title(f'RoBERTa Confusion Matrix - {category}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_{clean_cat}_comparison.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Comparative Analysis Function\n",
        "def compare_model_performance(results):\n",
        "    \"\"\"\n",
        "    Compare performance of different models across categories\n",
        "    \"\"\"\n",
        "    print(\"\\nModel Performance Comparison:\")\n",
        "    for category, model_results in results.items():\n",
        "        print(f\"\\nCategory: {category}\")\n",
        "        for model_name, metrics in model_results.items():\n",
        "            print(f\"{model_name}:\")\n",
        "            print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "            print(f\"  Weighted F1: {metrics['f1_weighted']:.4f}\")\n",
        "\n",
        "# Call comparative analysis\n",
        "compare_model_performance(sentiment_results)"
      ],
      "metadata": {
        "id": "Zi_h4_VSqEv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SWAPPING IN THIS WORKING CODE FROM MY EARLIER VERSION (assn_cn7050_final7)\n",
        "# Prepare data for deep learning models\n",
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Convert to pandas for easier processing\n",
        "#train_pandas = dfs_train_with_sentiment.toPandas()\n",
        "#test_pandas = dfs_test_with_sentiment.toPandas()\n",
        "#validation_pandas = dfs_validation_with_sentiment.toPandas()\n",
        "train_pandas = dfs_train_with_sentiment.toPandas()\n",
        "test_pandas = dfs_test_with_sentiment.toPandas()\n",
        "validation_pandas = dfs_validation_with_sentiment.toPandas()\n",
        "\n",
        "# Tokenize text\n",
        "keras_tokenizer = KerasTokenizer(num_words=10000)\n",
        "keras_tokenizer.fit_on_texts(train_pandas['text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sequences = keras_tokenizer.texts_to_sequences(train_pandas['text'])\n",
        "test_sequences = keras_tokenizer.texts_to_sequences(test_pandas['text'])\n",
        "validation_sequences = keras_tokenizer.texts_to_sequences(validation_pandas['text'])\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_length = 100\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "validation_data = pad_sequences(validation_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# Deep learning model results\n",
        "dl_sentiment_results = {}\n",
        "\n",
        "# For each category, train a deep learning model\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create y labels\n",
        "    # 0: negative, 1: neutral, 2: positive\n",
        "    def create_sentiment_labels(df, category_col):\n",
        "        neg_col = f\"{category_col}_negative\"\n",
        "        neu_col = f\"{category_col}_neutral\"\n",
        "        pos_col = f\"{category_col}_positive\"\n",
        "\n",
        "        y = np.zeros(len(df))\n",
        "        y[df[neg_col] == 1] = 0\n",
        "        y[df[neu_col] == 1] = 1\n",
        "        y[df[pos_col] == 1] = 2\n",
        "        # Default to neutral if no sentiment is available\n",
        "        y[(df[neg_col] == 0) & (df[neu_col] == 0) & (df[pos_col] == 0)] = 1\n",
        "\n",
        "        return y\n",
        "\n",
        "    y_train = create_sentiment_labels(train_pandas, clean_cat)\n",
        "    y_test = create_sentiment_labels(test_pandas, clean_cat)\n",
        "    y_val = create_sentiment_labels(validation_pandas, clean_cat)\n",
        "\n",
        "    # Convert to one-hot encoding\n",
        "    y_train_one_hot = tf.keras.utils.to_categorical(y_train, 3)\n",
        "    y_test_one_hot = tf.keras.utils.to_categorical(y_test, 3)\n",
        "    y_val_one_hot = tf.keras.utils.to_categorical(y_val, 3)\n",
        "\n",
        "    # Build the CNN model\n",
        "    vocab_size = min(10000, len(keras_tokenizer.word_index) + 1)\n",
        "    embedding_dim = 100\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_length))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(5))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(3, activation='softmax'))  # 3 classes: negative, neutral, positive\n",
        "\n",
        "    # Note: to self.\n",
        "    # source (V7 Labs): Categorical Cross Entropy is also known as Softmax Loss. It's a softmax activation plus a Cross-Entropy loss used for multiclass classification.\n",
        "    # Using this loss, we can train a Convolutional Neural Network to output a probability over the N classes for each image.26 Jan 2023\n",
        "    # Source (Geeks4Geeks) ...is a powerful loss function commonly used in multi-class classification problems.\n",
        "    # By comparing the predicted probabilities to the true one-hot encoded labels, it guides the model's learning process, pushing it to make better predictions.\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train with early stopping\n",
        "    from tensorflow.keras.callbacks import EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "    # Epochs reduced for testing, but back again...\n",
        "    history = model.fit(\n",
        "        train_data, y_train_one_hot,\n",
        "        validation_data=(validation_data, y_val_one_hot),\n",
        "        epochs=3,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_probs = model.predict(test_data)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    dl_sentiment_results[category] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_neg\": report['0']['f1-score'] if '0' in report else 0,\n",
        "        \"f1_neu\": report['1']['f1-score'] if '1' in report else 0,\n",
        "        \"f1_pos\": report['2']['f1-score'] if '2' in report else 0,\n",
        "        \"f1_weighted\": report['weighted avg']['f1-score']\n",
        "    }\n",
        "\n",
        "    print(f\"Category: {category}, Accuracy: {accuracy:.4f}, F1 (weighted): {report['weighted avg']['f1-score']:.4f}\")\n",
        "    print(f\"F1 Scores - Negative: {report['0']['f1-score'] if '0' in report else 0:.4f}, \" +\n",
        "          f\"Neutral: {report['1']['f1-score'] if '1' in report else 0:.4f}, \" +\n",
        "          f\"Positive: {report['2']['f1-score'] if '2' in report else 0:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "    plt.title(f'Confusion Matrix - {category}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_{clean_cat}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title(f'Model Accuracy - {category}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tov1Wen0Q_Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ACD** FIXME NEXT!!!!!! ONLY Version - Logistic Regression only **bold text**"
      ],
      "metadata": {
        "id": "61x0qXFwGPJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# dummy data\n",
        "dataset = pd.DataFrame({\n",
        "    \"text\": [    \"Gret customer service\", \"blah\"    ],\n",
        "    \"category\": [\"customer_service\", \"website\"]\n",
        "})\n",
        "\n",
        "# Cache df\n",
        "dataset = dfs_train_with_sentiment.cache()\n",
        "dfs_test_with_sentiment.cache()\n",
        "dfs_validation_with_sentiment.cache()\n",
        "\n",
        "# temp train-test split - replace with variables above.\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    dataset[\"text\"], dataset[\"category\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert labels to numerical format\n",
        "categories = list(set(dataset[\"category\"]))\n",
        "category_to_id = {cat: idx for idx, cat in enumerate(categories)}\n",
        "train_labels = train_labels.map(category_to_id)\n",
        "test_labels = test_labels.map(category_to_id)\n",
        "\n",
        "# -----------------\n",
        "# 1. Logistic Regression Model\n",
        "# -----------------\n",
        "tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf.fit_transform(train_texts)\n",
        "X_test_tfidf = tfidf.transform(test_texts)\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_tfidf, train_labels)\n",
        "\n",
        "lr_preds = lr_model.predict(X_test_tfidf)\n",
        "lr_acc = accuracy_score(test_labels, lr_preds)\n",
        "lr_f1 = f1_score(test_labels, lr_preds, average=\"weighted\")\n",
        "\n",
        "print(f\"Logistic Regression - Accuracy: {lr_acc:.4f}, F1 Score: {lr_f1:.4f}\")\n",
        "\n",
        "# -----------------\n",
        "# 2. CNN Model\n",
        "# -----------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return encoding[\"input_ids\"].squeeze(), encoding[\"attention_mask\"].squeeze(), torch.tensor(self.labels[idx])\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.fc = nn.Linear(128 * (128 // 2), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).permute(0, 2, 1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_dataset = TextDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "cnn_model = CNNClassifier(vocab_size=30522, embed_dim=128, num_classes=len(categories))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):  # Training loop\n",
        "    for inputs, masks, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cnn_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate CNN\n",
        "cnn_preds, true_labels = [], []\n",
        "for inputs, masks, labels in test_loader:\n",
        "    with torch.no_grad():\n",
        "        outputs = cnn_model(inputs)\n",
        "    preds = torch.argmax(outputs, axis=1)\n",
        "    cnn_preds.extend(preds.tolist())\n",
        "    true_labels.extend(labels.tolist())\n",
        "\n",
        "cnn_acc = accuracy_score(true_labels, cnn_preds)\n",
        "cnn_f1 = f1_score(true_labels, cnn_preds, average=\"weighted\")\n",
        "print(f\"CNN - Accuracy: {cnn_acc:.4f}, F1 Score: {cnn_f1:.4f}\")\n",
        "\n",
        "# -----------------\n",
        "# 3. RoBERTa Model\n",
        "# -----------------\n",
        "roberta_model_name = \"roberta-base\"\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_name, num_labels=len(categories))\n",
        "\n",
        "roberta_train_encodings = roberta_tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "roberta_test_encodings = roberta_tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "train_inputs = roberta_train_encodings[\"input_ids\"]\n",
        "train_masks = roberta_train_encodings[\"attention_mask\"]\n",
        "train_labels = torch.tensor(train_labels.tolist())\n",
        "\n",
        "test_inputs = roberta_test_encodings[\"input_ids\"]\n",
        "test_masks = roberta_test_encodings[\"attention_mask\"]\n",
        "test_labels = torch.tensor(test_labels.tolist())\n",
        "\n",
        "roberta_optimizer = optim.Adam(roberta_model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(3):  # Train RoBERTa\n",
        "    optimizer.zero_grad()\n",
        "    outputs = roberta_model(input_ids=train_inputs, attention_mask=train_masks)\n",
        "    loss = criterion(outputs.logits, train_labels)\n",
        "    loss.backward()\n",
        "    roberta_optimizer.step()\n",
        "\n",
        "# Evaluate RoBERTa\n",
        "with torch.no_grad():\n",
        "    roberta_outputs = roberta_model(input_ids=test_inputs, attention_mask=test_masks)\n",
        "roberta_preds = torch.argmax(roberta_outputs.logits, axis=1)\n",
        "\n",
        "roberta_acc = accuracy_score(test_labels, roberta_preds)\n",
        "roberta_f1 = f1_score(test_labels, roberta_preds, average=\"weighted\")\n",
        "print(f\"RoBERTa - Accuracy: {roberta_acc:.4f}, F1 Score: {roberta_f1:.4f}\")\n",
        "\n",
        "# -----------------\n",
        "# Compare Results\n",
        "# -----------------\n",
        "print(\"\\n### Model Performance Comparison ###\")\n",
        "print(f\"Logistic Regression - Accuracy: {lr_acc:.4f}, F1 Score: {lr_f1:.4f}\")\n",
        "print(f\"CNN Model          - Accuracy: {cnn_acc:.4f}, F1 Score: {cnn_f1:.4f}\")\n",
        "print(f\"RoBERTa Model      - Accuracy: {roberta_acc:.4f}, F1 Score: {roberta_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "SYGkrv9bJgzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Step 1: Add sentiment labels for each category\n",
        "# We need to extract sentiment columns for each category\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Use the has_sentiment UDF to create columns for negative, neutral, and positive sentiments\n",
        "    dfs_train_with_sentiment = dfs_train_with_category.withColumn(\n",
        "        f\"{clean_cat}_negative\",\n",
        "        has_sentiment(F.col(\"category_sentiments\"), F.lit(category), F.lit(\"-1\"))\n",
        "    )\n",
        "    dfs_train_with_sentiment = dfs_train_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_neutral\",\n",
        "        has_sentiment(F.col(\"category_sentiments\"), F.lit(category), F.lit(\"0\"))\n",
        "    )\n",
        "    dfs_train_with_sentiment = dfs_train_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_positive\",\n",
        "        has_sentiment(F.col(\"category_sentiments\"), F.lit(category), F.lit(\"1\"))\n",
        "    )\n",
        "\n",
        "    # Apply the same for test set\n",
        "    dfs_test_with_sentiment = dfs_test_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_negative\",\n",
        "        has_sentiment(F.col(\"category_sentiments\"), F.lit(category), F.lit(\"-1\"))\n",
        "    )\n",
        "    dfs_test_with_sentiment = dfs_test_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_neutral\",\n",
        "        has_sentiment(F.col(\"category_sentiments\"), F.lit(category), F.lit(\"0\"))\n",
        "    )\n",
        "    dfs_test_with_sentiment = dfs_test_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_positive\",\n",
        "        has_sentiment(F.col(\"category_sentiments\"), F.lit(category), F.lit(\"1\"))\n",
        "    )\n",
        "\n",
        "# Step 2: Train ABSA sentiment classifiers for each category\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create the sentiment label column (combine negative, neutral, and positive columns)\n",
        "    @udf(returnType=DoubleType())\n",
        "    def combine_sentiment(negative, neutral, positive):\n",
        "        if negative == 1:\n",
        "            return 0.0  # Class 0 = negative\n",
        "        elif neutral == 1:\n",
        "            return 1.0  # Class 1 = neutral\n",
        "        elif positive == 1:\n",
        "            return 2.0  # Class 2 = positive\n",
        "        else:\n",
        "            return 1.0  # Default to neutral if no sentiment is available\n",
        "\n",
        "    # Create sentiment label column\n",
        "    train_data = dfs_train_with_sentiment.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            F.col(f\"{clean_cat}_negative\"),\n",
        "            F.col(f\"{clean_cat}_neutral\"),\n",
        "            F.col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "    test_data = dfs_test_with_sentiment.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            F.col(f\"{clean_cat}_negative\"),\n",
        "            F.col(f\"{clean_cat}_neutral\"),\n",
        "            F.col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Create feature vector using TF-IDF\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data).select(\"features\", \"sentiment_label\")\n",
        "    test_assembled = assembler.transform(test_data).select(\"features\", \"sentiment_label\")\n",
        "\n",
        "    # Train Logistic Regression classifier for sentiment prediction\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_label\", maxIter=20)\n",
        "    lr_model = lr.fit(train_assembled)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = lr_model.transform(test_assembled)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "    print(f\"ABSA Sentiment Analysis Accuracy for Category {category}: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "LJuyio70GLNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear SVC with CNN and RoBERTa and reduced iterations"
      ],
      "metadata": {
        "id": "SFYeuqektfQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "sentiment_results = {}\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "        return {key: val.squeeze() for key, val in encoding.items()}, torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    @udf(returnType=\"double\")\n",
        "    def combine_sentiment(negative, neutral, positive):\n",
        "        if negative == 1:\n",
        "            return 0.0  # Class 0 = negative\n",
        "        elif neutral == 1:\n",
        "            return 1.0  # Class 1 = neutral\n",
        "        elif positive == 1:\n",
        "            return 2.0  # Class 2 = positive\n",
        "        else:\n",
        "            return 1.0\n",
        "\n",
        "    train_data = train_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    test_data = test_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data).select(\"features\", \"sentiment_label\")\n",
        "    test_assembled = assembler.transform(test_data).select(\"features\", \"sentiment_label\")\n",
        "\n",
        "    svc = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment_label\", maxIter=20)\n",
        "    svc_model = svc.fit(train_assembled)\n",
        "    predictions_svc = svc_model.transform(test_assembled)\n",
        "\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1_svc = evaluator.evaluate(predictions_svc)\n",
        "    precision_svc = evaluator.evaluate(predictions_svc, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall_svc = evaluator.evaluate(predictions_svc, {evaluator.metricName: \"weightedRecall\"})\n",
        "    accuracy_svc = evaluator.evaluate(predictions_svc, {evaluator.metricName: \"accuracy\"})\n",
        "\n",
        "    # RoBERTa\n",
        "    train_texts = train_data.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
        "    train_labels = train_data.select(\"sentiment_label\").rdd.flatMap(lambda x: x).collect()\n",
        "    test_texts = test_data.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
        "    test_labels = test_data.select(\"sentiment_label\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels)\n",
        "    test_dataset = ReviewDataset(test_texts, test_labels)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy_roberta = correct / total\n",
        "\n",
        "    sentiment_results[category] = {\n",
        "        \"F1 (SVC)\": f1_svc,\n",
        "        \"Precision (SVC)\": precision_svc,\n",
        "        \"Recall (SVC)\": recall_svc,\n",
        "        \"Accuracy (SVC)\": accuracy_svc,\n",
        "        \"Accuracy (RoBERTa)\": accuracy_roberta\n",
        "    }\n",
        "\n",
        "# Convert results to Pandas DataFrame\n",
        "df_results = pd.DataFrame.from_dict(sentiment_results, orient=\"index\").reset_index()\n",
        "df_results.columns = [\"Category\", \"F1 (SVC)\", \"Precision (SVC)\", \"Recall (SVC)\", \"Accuracy (SVC)\", \"Accuracy (RoBERTa)\"]\n",
        "\n",
        "from IPython.display import display\n",
        "display(df_results)\n"
      ],
      "metadata": {
        "id": "_dn6xMaytoMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Reg with CNN and too many epochs\n"
      ],
      "metadata": {
        "id": "XWe6g3pdl58y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "sentiment_results = {}\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create a combined label with 3 classes: -1, 0, 1\n",
        "    @udf(returnType=\"double\")\n",
        "    def combine_sentiment(negative, neutral, positive):\n",
        "        if negative == 1:\n",
        "            return 0.0  # Class 0 = negative\n",
        "        elif neutral == 1:\n",
        "            return 1.0  # Class 1 = neutral\n",
        "        elif positive == 1:\n",
        "            return 2.0  # Class 2 = positive\n",
        "        else:\n",
        "            return 1.0  # Default to neutral if no sentiment is available\n",
        "\n",
        "    # Create multiclass label column\n",
        "    train_data = train_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    test_data = test_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Use TF-IDF features for prediction\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data).select(\"features\", \"sentiment_label\")\n",
        "    test_assembled = assembler.transform(test_data).select(\"features\", \"sentiment_label\")\n",
        "\n",
        "    # Convert Spark DataFrames to NumPy for TensorFlow processing\n",
        "    X_train = np.array(train_assembled.select(\"features\").rdd.map(lambda x: x[0]).collect())\n",
        "    y_train = np.array(train_assembled.select(\"sentiment_label\").rdd.map(lambda x: x[0]).collect())\n",
        "\n",
        "    X_test = np.array(test_assembled.select(\"features\").rdd.map(lambda x: x[0]).collect())\n",
        "    y_test = np.array(test_assembled.select(\"sentiment_label\").rdd.map(lambda x: x[0]).collect())\n",
        "\n",
        "    # One-hot encoding for CNN classification\n",
        "    #encoder = OneHotEncoder(sparse=False)\n",
        "    #encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # Set sparse_output to false\n",
        "\n",
        "    y_train_enc = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "    y_test_enc = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # ------------------ Logistic Regression ------------------\n",
        "    #lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_label\", maxIter=20)\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_label\", maxIter=5)\n",
        "    predictions = lr_model.transform(test_assembled)\n",
        "\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1_lr = evaluator.evaluate(predictions)\n",
        "    precision_lr = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall_lr = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "    accuracy_lr = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "    fa_lr = 1 - accuracy_lr  # False Alarm rate\n",
        "\n",
        "    # ------------------ CNN Model ------------------\n",
        "    input_dim = X_train.shape[1]  # Feature dimension\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim, 1)),  # 1D input\n",
        "        layers.Conv1D(filters=64, kernel_size=3, activation=\"relu\"),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Conv1D(filters=128, kernel_size=3, activation=\"relu\"),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(3, activation=\"softmax\")  # 3 sentiment classes\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Reshape data for CNN\n",
        "    X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "    X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "\n",
        "    #model.fit(X_train_reshaped, y_train_enc, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test_enc), verbose=1)\n",
        "    # Reducing iterations for sanity purposes\n",
        "    model.fit(X_train_reshaped, y_train_enc, epochs=3, batch_size=32, validation_data=(X_test_reshaped, y_test_enc), verbose=1)\n",
        "\n",
        "    # Evaluate CNN\n",
        "    test_loss, accuracy_cnn = model.evaluate(X_test_reshaped, y_test_enc, verbose=0)\n",
        "    y_pred_cnn = np.argmax(model.predict(X_test_reshaped), axis=1)\n",
        "    y_true_cnn = np.argmax(y_test_enc, axis=1)\n",
        "\n",
        "    # Calculate CNN performance metrics\n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "    f1_cnn = f1_score(y_true_cnn, y_pred_cnn, average=\"weighted\")\n",
        "    precision_cnn = precision_score(y_true_cnn, y_pred_cnn, average=\"weighted\")\n",
        "    recall_cnn = recall_score(y_true_cnn, y_pred_cnn, average=\"weighted\")\n",
        "    fa_cnn = 1 - accuracy_cnn  # False Alarm rate\n",
        "\n",
        "    # Store results for comparison\n",
        "    sentiment_results[category] = {\n",
        "        \"Logistic Regression\": {\"F1\": f1_lr, \"Precision\": precision_lr, \"Recall\": recall_lr, \"Accuracy\": accuracy_lr, \"FA\": fa_lr},\n",
        "        \"CNN\": {\"F1\": f1_cnn, \"Precision\": precision_cnn, \"Recall\": recall_cnn, \"Accuracy\": accuracy_cnn, \"FA\": fa_cnn},\n",
        "    }\n",
        "\n",
        "# Convert results to Pandas DataFrame\n",
        "df_results = pd.DataFrame.from_dict(sentiment_results, orient=\"index\").reset_index()\n",
        "df_results.columns = [\"Category\", \"Model\", \"F1\", \"Precision\", \"Recall\", \"Accuracy\", \"False Alarm\"]\n",
        "\n",
        "# Display results\n",
        "from IPython.display import display\n",
        "display(df_results)\n"
      ],
      "metadata": {
        "id": "C4OZwt1PmN0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "VrcQQBXrr0Nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Reg Version WORKING"
      ],
      "metadata": {
        "id": "L0tBx0hlGMnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "sentiment_results = {}\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create a combined label with 3 classes: -1, 0, 1\n",
        "    @udf(returnType=\"double\")\n",
        "    def combine_sentiment(negative, neutral, positive):\n",
        "        if negative == 1:\n",
        "            return 0.0  # Class 0 = negative\n",
        "        elif neutral == 1:\n",
        "            return 1.0  # Class 1 = neutral\n",
        "        elif positive == 1:\n",
        "            return 2.0  # Class 2 = positive\n",
        "        else:\n",
        "            return 1.0  # Default to neutral if no sentiment is available\n",
        "\n",
        "    # Create multiclass label column\n",
        "    train_data = train_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    test_data = test_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Use TF-IDF features for prediction\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data).select(\"features\", \"sentiment_label\")\n",
        "    test_assembled = assembler.transform(test_data).select(\"features\", \"sentiment_label\")\n",
        "\n",
        "    # Train multiclass classifier - LogisticRegression, reducing the number of iterations (on purpose)\n",
        "    #lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_label\", maxIter=20)\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_label\", maxIter=5)\n",
        "    lr_model = lr.fit(train_assembled)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = lr_model.transform(test_assembled)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1 = evaluator.evaluate(predictions)\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "\n",
        "    sentiment_results[category] = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"accuracy\": accuracy\n",
        "    }\n",
        "\n",
        "# Convert results to Pandas DataFrame\n",
        "df_results = pd.DataFrame.from_dict(sentiment_results, orient=\"index\").reset_index()\n",
        "df_results.columns = [\"Category\", \"F1\", \"Precision\", \"Recall\", \"Accuracy\"]\n",
        "\n",
        "# Display the table in Colab\n",
        "from IPython.display import display\n",
        "display(df_results)\n"
      ],
      "metadata": {
        "id": "UTQGSRGWb9qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "df_results['Accuracy'].plot(kind='hist', bins=20, title='Accuracy')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tk9fttSDi5gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# @title F1 vs Precision\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df_results.plot(kind='scatter', x='F1', y='Precision', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "hRSmZ0fViyMF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cskNV-tThWZ"
      },
      "outputs": [],
      "source": [
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create a combined label with 3 classes: -1, 0, 1\n",
        "    # We'll use a UDF to convert the 3 binary columns into a single multiclass label\n",
        "    @udf(returnType=\"double\")\n",
        "    def combine_sentiment(negative, neutral, positive):\n",
        "        if negative == 1:\n",
        "            return 0.0  # Class 0 = negative\n",
        "        elif neutral == 1:\n",
        "            return 1.0  # Class 1 = neutral\n",
        "        elif positive == 1:\n",
        "            return 2.0  # Class 2 = positive\n",
        "        else:\n",
        "            return 1.0  # Default to neutral if no sentiment is available\n",
        "\n",
        "    # Create multiclass label column\n",
        "    train_data = train_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    test_data = test_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Use TF-IDF features for prediction\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data).select(\"features\", \"sentiment_label\")\n",
        "    test_assembled = assembler.transform(test_data).select(\"features\", \"sentiment_label\")\n",
        "\n",
        "    # Train multiclass classifier - LogisticRegression for 3 classes\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_label\", maxIter=10)\n",
        "    lr_model = lr.fit(train_assembled)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = lr_model.transform(test_assembled)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1 = evaluator.evaluate(predictions)\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "\n",
        "    sentiment_results[category] = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"accuracy\": accuracy\n",
        "    }\n",
        "\n",
        "# Generate HTML table\n",
        "html_table = \"<table><tr><th>Category</th><th>F1</th><th>Precision</th><th>Recall</th><th>Accuracy</th></tr>\"\n",
        "for category, metrics in sentiment_results.items():\n",
        "    html_table += f\"<tr><td>{category}</td><td>{metrics['f1']:.4f}</td><td>{metrics['precision']:.4f}</td><td>{metrics['recall']:.4f}</td><td>{metrics['accuracy']:.4f}</td></tr>\"\n",
        "html_table += \"</table>\"\n",
        "\n",
        "print(html_table)\n",
        "    #print(f\"Category: {category}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6sdgS8osCPo"
      },
      "source": [
        " **Evaluate the model -  Accuracy** - SPLIT THIS LATER\n",
        "\n",
        "We send the predictions to the evaluator and get an accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "\n",
        "# Load pre-trained ABSA model\n",
        "MODEL_NAME = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def classify_aspect_sentiment(text, aspect):\n",
        "    aspect_text = f\"{aspect}: {text}\"\n",
        "    tokens = tokenizer(aspect_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "    scores = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze()\n",
        "    sentiment = torch.argmax(scores).item() - 1  # Convert label to (-1, 0, 1)\n",
        "    return sentiment\n",
        "\n",
        "# Define aspects\n",
        "#all_categories = [\"blah\", \"customer service\", \"website\"]\n",
        "\n",
        "# dataset\n",
        "reviews = [\n",
        "    \"The battery life is amazing, but the camera quality is poor.\",\n",
        "    \"Customer service was unhelpful and rude.\",\n",
        "    \"The website is easy to navigate and very user-friendly.\"\n",
        "]\n",
        "\n",
        "# Apply aspect-based sentiment classification\n",
        "results = []\n",
        "for review in reviews:\n",
        "    aspect_results = {\"Review\": review}\n",
        "    for category in all_categories:\n",
        "        sentiment = classify_aspect_sentiment(review, category)\n",
        "        aspect_results[category] = sentiment\n",
        "    results.append(aspect_results)\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "EA4ciID3QrjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm4s4n2ir_Js"
      },
      "source": [
        "**Deep learning model for sentiment analysis**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvbWDYKYr-Iv"
      },
      "outputs": [],
      "source": [
        "# Step 6: Prepare data for deep learning models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Convert to pandas for easier processing\n",
        "train_pandas = dfs_train_with_sentiment.toPandas()\n",
        "test_pandas = dfs_test_with_sentiment.toPandas()\n",
        "validation_pandas = dfs_validation_with_sentiment.toPandas()\n",
        "\n",
        "# Tokenize text\n",
        "keras_tokenizer = KerasTokenizer(num_words=10000)\n",
        "keras_tokenizer.fit_on_texts(train_pandas['text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sequences = keras_tokenizer.texts_to_sequences(train_pandas['text'])\n",
        "test_sequences = keras_tokenizer.texts_to_sequences(test_pandas['text'])\n",
        "validation_sequences = keras_tokenizer.texts_to_sequences(validation_pandas['text'])\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_length = 100\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "validation_data = pad_sequences(validation_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# Deep learning model results\n",
        "dl_sentiment_results = {}\n",
        "\n",
        "# For each category, train a deep learning model\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create y labels\n",
        "    # 0: negative, 1: neutral, 2: positive\n",
        "    def create_sentiment_labels(df, category_col):\n",
        "        neg_col = f\"{category_col}_negative\"\n",
        "        neu_col = f\"{category_col}_neutral\"\n",
        "        pos_col = f\"{category_col}_positive\"\n",
        "\n",
        "        y = np.zeros(len(df))\n",
        "        y[df[neg_col] == 1] = 0\n",
        "        y[df[neu_col] == 1] = 1\n",
        "        y[df[pos_col] == 1] = 2\n",
        "        # Default to neutral if no sentiment is available\n",
        "        y[(df[neg_col] == 0) & (df[neu_col] == 0) & (df[pos_col] == 0)] = 1\n",
        "\n",
        "        return y\n",
        "\n",
        "    y_train = create_sentiment_labels(train_pandas, clean_cat)\n",
        "    y_test = create_sentiment_labels(test_pandas, clean_cat)\n",
        "    y_val = create_sentiment_labels(validation_pandas, clean_cat)\n",
        "\n",
        "    # Convert to one-hot encoding\n",
        "    y_train_one_hot = tf.keras.utils.to_categorical(y_train, 3)\n",
        "    y_test_one_hot = tf.keras.utils.to_categorical(y_test, 3)\n",
        "    y_val_one_hot = tf.keras.utils.to_categorical(y_val, 3)\n",
        "\n",
        "    # Build CNN model\n",
        "    vocab_size = min(10000, len(keras_tokenizer.word_index) + 1)\n",
        "    embedding_dim = 100\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_length))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(5))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(3, activation='softmax'))  # 3 classes: negative, neutral, positive\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train with early stopping\n",
        "    from tensorflow.keras.callbacks import EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data, y_train_one_hot,\n",
        "        validation_data=(validation_data, y_val_one_hot),\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_probs = model.predict(test_data)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    dl_sentiment_results[category] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_neg\": report['0']['f1-score'] if '0' in report else 0,\n",
        "        \"f1_neu\": report['1']['f1-score'] if '1' in report else 0,\n",
        "        \"f1_pos\": report['2']['f1-score'] if '2' in report else 0,\n",
        "        \"f1_weighted\": report['weighted avg']['f1-score']\n",
        "    }\n",
        "\n",
        "    print(f\"Category: {category}, Accuracy: {accuracy:.4f}, F1 (weighted): {report['weighted avg']['f1-score']:.4f}\")\n",
        "    print(f\"F1 Scores - Negative: {report['0']['f1-score'] if '0' in report else 0:.4f}, \" +\n",
        "          f\"Neutral: {report['1']['f1-score'] if '1' in report else 0:.4f}, \" +\n",
        "          f\"Positive: {report['2']['f1-score'] if '2' in report else 0:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "    plt.title(f'Confusion Matrix - {category}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_{clean_cat}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title(f'Model Accuracy - {category}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9RtqvzDtjB_"
      },
      "outputs": [],
      "source": [
        "# Step 8: Perform cross-category sentiment analysis\n",
        "def analyze_cross_category_sentiment_correlation(df, all_categories):\n",
        "    \"\"\"Analyze correlation between sentiment predictions across different categories\"\"\"\n",
        "    # Create a dataframe with sentiment scores for each category\n",
        "    sentiment_scores = {}\n",
        "\n",
        "    for category in all_categories:\n",
        "        clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "        neg_col = f\"{clean_cat}_negative\"\n",
        "        neu_col = f\"{clean_cat}_neutral\"\n",
        "        pos_col = f\"{clean_cat}_positive\"\n",
        "\n",
        "        # If the columns exist, calculate a simple sentiment score\n",
        "        if neg_col in df.columns and neu_col in df.columns and pos_col in df.columns:\n",
        "            # Convert to numeric sentiment score: -1 for negative, 0 for neutral, 1 for positive\n",
        "            df[f\"{clean_cat}_score\"] = df[pos_col].astype(int) - df[neg_col].astype(int)\n",
        "            sentiment_scores[category] = f\"{clean_cat}_score\"\n",
        "\n",
        "    # Create correlation matrix\n",
        "    score_columns = list(sentiment_scores.values())\n",
        "    if len(score_columns) > 1:\n",
        "        correlation_matrix = df[score_columns].corr()\n",
        "\n",
        "        # Plot heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
        "                    mask=mask, vmin=-1, vmax=1, center=0,\n",
        "                    xticklabels=sentiment_scores.keys(),\n",
        "                    yticklabels=sentiment_scores.keys())\n",
        "        plt.title('Cross-Category Sentiment Correlation', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('sentiment_correlation_heatmap.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        return correlation_matrix\n",
        "    return None\n",
        "\n",
        "# Analyze cross-category correlations\n",
        "correlation_matrix = analyze_cross_category_sentiment_correlation(train_pandas, all_categories)\n",
        "\n",
        "# Step 9: Error analysis\n",
        "def perform_error_analysis(model, tokenizer, texts, true_labels, category, label_names=['Negative', 'Neutral', 'Positive']):\n",
        "    \"\"\"Perform error analysis on incorrect predictions\"\"\"\n",
        "    # Make predictions\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    predictions = model.predict(padded_sequences)\n",
        "    pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Find incorrect predictions\n",
        "    incorrect_indices = np.where(pred_labels != true_labels)[0]\n",
        "\n",
        "    if len(incorrect_indices) == 0:\n",
        "        print(f\"No incorrect predictions found for {category}\")\n",
        "        return\n",
        "\n",
        "    # Sample up to 10 incorrect predictions\n",
        "    sample_size = min(10, len(incorrect_indices))\n",
        "    sample_indices = np.random.choice(incorrect_indices, size=sample_size, replace=False)\n",
        "\n",
        "    # Create a dataframe for analysis\n",
        "    error_df = pd.DataFrame({\n",
        "        'Text': [texts[i] for i in sample_indices],\n",
        "        'True Label': [label_names[true_labels[i]] for i in sample_indices],\n",
        "        'Predicted Label': [label_names[pred_labels[i]] for i in sample_indices],\n",
        "        'Confidence': [np.max(predictions[i]) for i in sample_indices]\n",
        "    })\n",
        "\n",
        "    # Create and plot confusion matrix\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_names, yticklabels=label_names)\n",
        "    plt.title(f'Confusion Matrix for {category}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_{category}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Print error analysis\n",
        "    print(f\"\\n--- Error Analysis for {category} ---\")\n",
        "    print(f\"Total samples: {len(texts)}\")\n",
        "    print(f\"Incorrect predictions: {len(incorrect_indices)} ({len(incorrect_indices)/len(texts)*100:.2f}%)\")\n",
        "    print(\"\\nSample of incorrect predictions:\")\n",
        "    print(error_df)\n",
        "\n",
        "    return error_df\n",
        "\n",
        "# Perform error analysis for a few selected categories\n",
        "selected_categories = all_categories[:3]  # Choose first 3 categories for analysis\n",
        "for category in selected_categories:\n",
        "    # Get test data for this category\n",
        "    # Note: You would need to extract test data for each category\n",
        "    # This is a placeholder - adapt to your data structure\n",
        "    test_texts = test_texts_by_category.get(category, [])\n",
        "    test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "    if len(test_texts) > 0:\n",
        "        perform_error_analysis(model, keras_tokenizer, test_texts, test_labels, category)\n",
        "\n",
        "# Step 10: Analyze prediction confidence\n",
        "def analyze_prediction_confidence(model, tokenizer, texts, true_labels, category):\n",
        "    \"\"\"Analyze prediction confidence and its relationship with accuracy\"\"\"\n",
        "    # Make predictions\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    predictions = model.predict(padded_sequences)\n",
        "    pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Get confidence of predictions (max probability)\n",
        "    confidences = np.max(predictions, axis=1)\n",
        "\n",
        "    # Create a dataframe\n",
        "    confidence_df = pd.DataFrame({\n",
        "        'Confidence': confidences,\n",
        "        'Correct': pred_labels == true_labels\n",
        "    })\n",
        "\n",
        "    # Plot confidence distribution\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Overall confidence distribution\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(confidences, bins=20, kde=True)\n",
        "    plt.title(f'Prediction Confidence Distribution - {category}')\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Confidence by correctness\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.boxplot(x='Correct', y='Confidence', data=confidence_df)\n",
        "    plt.title('Confidence by Prediction Correctness')\n",
        "    plt.xlabel('Prediction Correct')\n",
        "    plt.ylabel('Confidence')\n",
        "\n",
        "    # Accuracy by confidence bucket\n",
        "    plt.subplot(2, 2, 3)\n",
        "    confidence_df['ConfidenceBucket'] = pd.cut(confidence_df['Confidence'], bins=10)\n",
        "    accuracy_by_bucket = confidence_df.groupby('ConfidenceBucket')['Correct'].mean()\n",
        "\n",
        "    sns.barplot(x=accuracy_by_bucket.index.astype(str), y=accuracy_by_bucket.values)\n",
        "    plt.title('Accuracy by Confidence Range')\n",
        "    plt.xlabel('Confidence Range')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Count by confidence bucket\n",
        "    plt.subplot(2, 2, 4)\n",
        "    count_by_bucket = confidence_df.groupby('ConfidenceBucket').size()\n",
        "    sns.barplot(x=count_by_bucket.index.astype(str), y=count_by_bucket.values)\n",
        "    plt.title('Sample Count by Confidence Range')\n",
        "    plt.xlabel('Confidence Range')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confidence_analysis_{category}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    return confidence_df\n",
        "\n",
        "# Analyze prediction confidence for selected categories\n",
        "for category in selected_categories:\n",
        "    test_texts = test_texts_by_category.get(category, [])\n",
        "    test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "    if len(test_texts) > 0:\n",
        "        analyze_prediction_confidence(model, keras_tokenizer, test_texts, test_labels, category)\n",
        "\n",
        "# Step 11: Text length vs accuracy analysis\n",
        "def analyze_text_length_vs_accuracy(model, tokenizer, texts, true_labels, category):\n",
        "    \"\"\"Analyze the relationship between text length and prediction accuracy\"\"\"\n",
        "    # Make predictions\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    predictions = model.predict(padded_sequences)\n",
        "    pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate text lengths\n",
        "    text_lengths = [len(text.split()) for text in texts]\n",
        "\n",
        "    # Create dataframe\n",
        "    length_df = pd.DataFrame({\n",
        "        'TextLength': text_lengths,\n",
        "        'Correct': pred_labels == true_labels\n",
        "    })\n",
        "\n",
        "    # Create length buckets\n",
        "    length_df['LengthBucket'] = pd.cut(length_df['TextLength'], bins=10)\n",
        "\n",
        "    # Calculate accuracy by length bucket\n",
        "    accuracy_by_length = length_df.groupby('LengthBucket')['Correct'].mean()\n",
        "    count_by_length = length_df.groupby('LengthBucket').size()\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Text length distribution\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(text_lengths, bins=20, kde=True)\n",
        "    plt.title(f'Text Length Distribution - {category}')\n",
        "    plt.xlabel('Text Length (words)')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Text length vs accuracy (scatter)\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.scatterplot(x='TextLength', y='Correct', data=length_df, alpha=0.3)\n",
        "    plt.title('Text Length vs Prediction Correctness')\n",
        "    plt.xlabel('Text Length (words)')\n",
        "    plt.ylabel('Prediction Correct')\n",
        "\n",
        "    # Accuracy by length bucket\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.barplot(x=accuracy_by_length.index.astype(str), y=accuracy_by_length.values)\n",
        "    plt.title('Accuracy by Text Length Range')\n",
        "    plt.xlabel('Text Length Range')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Count by length bucket\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.barplot(x=count_by_length.index.astype(str), y=count_by_length.values)\n",
        "    plt.title('Sample Count by Text Length Range')\n",
        "    plt.xlabel('Text Length Range')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'length_analysis_{category}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    return length_df\n",
        "\n",
        "# Analyze text length vs accuracy for selected categories\n",
        "for category in selected_categories:\n",
        "    test_texts = test_texts_by_category.get(category, [])\n",
        "    test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "    if len(test_texts) > 0:\n",
        "        analyze_text_length_vs_accuracy(model, keras_tokenizer, test_texts, test_labels, category)\n",
        "\n",
        "# Step 12: Create interactive dashboard (optional - if using in a Jupyter notebook)\n",
        "# Note: This requires ipywidgets and is best used in a Jupyter environment\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "\n",
        "    # Create category selector\n",
        "    category_dropdown = widgets.Dropdown(\n",
        "        options=all_categories,\n",
        "        description='Category:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create visualization type selector\n",
        "    viz_dropdown = widgets.Dropdown(\n",
        "        options=['Performance Metrics', 'Confusion Matrix', 'Important Words', 'Error Examples'],\n",
        "        description='Visualization:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Function to update visualization based on selections\n",
        "    def update_visualization(category, viz_type):\n",
        "        if viz_type == 'Performance Metrics':\n",
        "            # Display performance metrics for selected category\n",
        "            if category in dl_sentiment_results:\n",
        "                metrics = dl_sentiment_results[category]\n",
        "                print(f\"Performance metrics for {category}:\")\n",
        "                for metric, value in metrics.items():\n",
        "                    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "        elif viz_type == 'Confusion Matrix':\n",
        "            # Display confusion matrix for selected category\n",
        "            test_texts = test_texts_by_category.get(category, [])\n",
        "            test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "            if len(test_texts) > 0:\n",
        "                # Make predictions\n",
        "                sequences = keras_tokenizer.texts_to_sequences(test_texts)\n",
        "                padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "                predictions = model.predict(padded_sequences)\n",
        "                pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "                # Create and plot confusion matrix\n",
        "                cm = confusion_matrix(test_labels, pred_labels)\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "                plt.title(f'Confusion Matrix for {category}')\n",
        "                plt.xlabel('Predicted Label')\n",
        "                plt.ylabel('True Label')\n",
        "                plt.show()\n",
        "\n",
        "        elif viz_type == 'Important Words':\n",
        "            # Display important words for each sentiment\n",
        "            for sentiment_class in range(3):\n",
        "                visualize_important_words_for_sentiment(model, keras_tokenizer, category, sentiment_class)\n",
        "\n",
        "        elif viz_type == 'Error Examples':\n",
        "            # Display error examples\n",
        "            test_texts = test_texts_by_category.get(category, [])\n",
        "            test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "            if len(test_texts) > 0:\n",
        "                perform_error_analysis(model, keras_tokenizer, test_texts, test_labels, category)\n",
        "\n",
        "    # Create interactive output\n",
        "    interactive_output = widgets.interactive_output(\n",
        "        update_visualization,\n",
        "        {'category': category_dropdown, 'viz_type': viz_dropdown}\n",
        "    )\n",
        "\n",
        "    # Display widgets\n",
        "    display(widgets.HBox([category_dropdown, viz_dropdown]))\n",
        "    display(interactive_output)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Interactive visualizations require ipywidgets. Install with: pip install ipywidgets\")\n",
        "\n",
        "# Step 13: Save model results summary to file\n",
        "def save_results_summary(ml_results, dl_results, filename='sentiment_analysis_results.csv'):\n",
        "    \"\"\"Save a summary of results to CSV file\"\"\"\n",
        "    results_data = []\n",
        "\n",
        "    # Combine results from both models\n",
        "    for category in all_categories:\n",
        "        row = {'Category': category}\n",
        "\n",
        "        # Add ML results if available\n",
        "        if category in ml_results:\n",
        "            for metric, value in ml_results[category].items():\n",
        "                row[f'ML_{metric}'] = value\n",
        "\n",
        "        # Add DL results if available\n",
        "        if category in dl_results:\n",
        "            for metric, value in dl_results[category].items():\n",
        "                row[f'DL_{metric}'] = value\n",
        "\n",
        "        results_data.append(row)\n",
        "\n",
        "    # Convert to DataFrame and save\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "    results_df.to_csv(filename, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LLM Version - Using RoBERTa LLM model for ACD - better than BERT at text**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lW8uirUbKeqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Custom dataset class for multi-label classification\n",
        "class AspectCategoryDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Remove batch dimension added by tokenizer\n",
        "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "\n",
        "        # Add label\n",
        "        encoding['labels'] = torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "        return encoding\n",
        "\n",
        "# Preprocessing function to prepare data\n",
        "def preprocess_data(df, label_codes_col='label_codes'):\n",
        "    # Extract categories from label codes\n",
        "    def extract_categories(label_codes):\n",
        "        if isinstance(label_codes, str):\n",
        "            codes = ast.literal_eval(label_codes)\n",
        "            # Extract category part (remove the sentiment indicator)\n",
        "            categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "            return categories\n",
        "        return []\n",
        "\n",
        "    # Apply the function to create a categories column\n",
        "    df['categories'] = df[label_codes_col].apply(extract_categories)\n",
        "\n",
        "    # Get all unique categories\n",
        "    all_categories = []\n",
        "    for cats in df['categories']:\n",
        "        all_categories.extend(cats)\n",
        "    all_categories = sorted(list(set(all_categories)))\n",
        "\n",
        "    # Create binary labels for each category\n",
        "    for category in all_categories:\n",
        "        col_name = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "        df[col_name] = df['categories'].apply(lambda x: 1 if category in x else 0)\n",
        "\n",
        "    # Return DataFrame and list of categories\n",
        "    return df, all_categories\n",
        "\n",
        "# Function to train the model\n",
        "def train_roberta_model(train_loader, val_loader, num_labels, epochs=3):\n",
        "    # Load pre-trained RoBERTa model\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        'roberta-base',\n",
        "        num_labels=num_labels,\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Set up optimizer\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # Calculate total training steps\n",
        "    total_steps = len(train_loader) * epochs\n",
        "\n",
        "    # Create learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_f1 = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Update loss\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(val_loader, desc=\"Validation\")\n",
        "            for batch in progress_bar:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Convert logits to predictions (apply sigmoid and threshold)\n",
        "                preds = torch.sigmoid(logits).cpu().numpy()\n",
        "                labels = batch['labels'].cpu().numpy()\n",
        "\n",
        "                all_labels.append(labels)\n",
        "                all_preds.append(preds)\n",
        "\n",
        "        # Concatenate all batches\n",
        "        all_labels = np.vstack(all_labels)\n",
        "        all_preds = np.vstack(all_preds)\n",
        "\n",
        "        # Convert predictions to binary (threshold 0.5)\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            all_labels, all_preds_binary, average='micro')\n",
        "\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            torch.save(model.state_dict(), 'roberta_acd_best_model.pt')\n",
        "            print(\"Saved best model!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to evaluate on test set\n",
        "def evaluate_model(model, test_loader, category_names):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Convert logits to predictions\n",
        "            preds = torch.sigmoid(logits).cpu().numpy()\n",
        "            labels = batch['labels'].cpu().numpy()\n",
        "\n",
        "            all_labels.append(labels)\n",
        "            all_preds.append(preds)\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_labels = np.vstack(all_labels)\n",
        "    all_preds = np.vstack(all_preds)\n",
        "\n",
        "    # Convert predictions to binary\n",
        "    all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds_binary, average='micro')\n",
        "\n",
        "    print(f\"Overall - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    # Calculate per-category metrics\n",
        "    results = []\n",
        "    for i, category in enumerate(category_names):\n",
        "        cat_precision, cat_recall, cat_f1, _ = precision_recall_fscore_support(\n",
        "            all_labels[:, i], all_preds_binary[:, i], average='binary')\n",
        "\n",
        "        results.append({\n",
        "            'category': category,\n",
        "            'precision': cat_precision,\n",
        "            'recall': cat_recall,\n",
        "            'f1': cat_f1\n",
        "        })\n",
        "\n",
        "    # Create DataFrame with results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return results_df, precision, recall, f1\n",
        "\n",
        "#\n",
        "def roberta():\n",
        "    # Load data (spark DataFrame converted to pandas)\n",
        "    # df = pd.read_csv(\"export_45.csv\")\n",
        "\n",
        "    # Preprocess data\n",
        "    print(\"Preprocessing data...\")\n",
        "    df_processed, all_categories = preprocess_data(df)\n",
        "\n",
        "    # Create column names for binary features\n",
        "    binary_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "    # Split data\n",
        "    X = df_processed['text'].values\n",
        "    y = df_processed[binary_cols].values\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    print(f\"Train: {len(X_train)} samples\")\n",
        "    print(f\"Validation: {len(X_val)} samples\")\n",
        "    print(f\"Test: {len(X_test)} samples\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = AspectCategoryDataset(X_train, y_train, tokenizer)\n",
        "    val_dataset = AspectCategoryDataset(X_val, y_val, tokenizer)\n",
        "    test_dataset = AspectCategoryDataset(X_test, y_test, tokenizer)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining RoBERTa model...\")\n",
        "    model = train_roberta_model(train_loader, val_loader, num_labels=len(all_categories), epochs=3)\n",
        "\n",
        "    # Load best model for evaluation\n",
        "    best_model = RobertaForSequenceClassification.from_pretrained(\n",
        "        'roberta-base',\n",
        "        num_labels=len(all_categories),\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "    best_model.load_state_dict(torch.load('roberta_acd_best_model.pt'))\n",
        "    best_model.to(device)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    results_df, precision, recall, f1 = evaluate_model(best_model, test_loader, all_categories)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nPer-category results:\")\n",
        "    print(results_df.sort_values('f1', ascending=False).head(10))\n",
        "\n",
        "    print(f\"\\nOverall Results - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    return results_df, precision, recall, f1, best_model\n",
        "\n",
        "roberta()\n"
      ],
      "metadata": {
        "id": "GGeGP-1cKpa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACSA ANALYSIS**"
      ],
      "metadata": {
        "id": "PPiUf0As17r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define a custom dataset class for our data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            self.texts[item],\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=False,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[item])\n",
        "        }\n",
        "\n",
        "# Load pre-trained RoBERTa model and tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(all_categories))\n",
        "\n",
        "# Prepare data for RoBERTa model\n",
        "roberta_dataset = TextDataset(train_texts, train_labels, tokenizer, max_seq_length)\n",
        "roberta_test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_seq_length)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16\n",
        "roberta_train_dataloader = DataLoader(roberta_dataset, batch_size=batch_size, shuffle=True)\n",
        "roberta_test_dataloader = DataLoader(roberta_test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(roberta_train_dataloader))\n",
        "\n",
        "# Train RoBERTa model\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in roberta_train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(roberta_train_dataloader)}')\n",
        "\n",
        "# Evaluate RoBERTa model\n",
        "model.eval()\n",
        "roberta_predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in roberta_test_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.sigmoid(logits)\n",
        "\n",
        "        roberta_predictions.extend(torch.round(torch.sigmoid(logits)).cpu().numpy())\n",
        "\n",
        "# Calculate metrics for RoBERTa\n",
        "roberta_results = {}\n",
        "for i, category in enumerate(all_categories):\n",
        "    f1 = f1_score(test_labels[:, i], [pred[i] for pred in roberta_predictions])\n",
        "    report = classification_report(test_labels[:, i], [pred[i] for pred in roberta_predictions], output_dict=True)\n",
        "    roberta_results[category] = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": report['1']['precision'] if 1 in report else 0,\n",
        "        \"recall\": report['1']['recall'] if 1 in report else 0\n",
        "    }\n",
        "    print(f\"RoBERTa - Category: {category}, F1: {f1}\")\n",
        "\n",
        "# Update plot_model_comparison function to include RoBERTa results\n",
        "def plot_model_comparison(svm_results, cnn_results, lstm_results, roberta_results, all_categories):\n",
        "    categories = [cat.replace('-', '_').replace('.', '_') for cat in all_categories]\n",
        "    svm_f1 = [svm_results[cat]['f1'] for cat in all_categories]\n",
        "    cnn_f1 = [cnn_results[cat]['f1'] for cat in all_categories]\n",
        "    lstm_f1 = [lstm_results[cat]['f1'] for cat in all_categories]\n",
        "    roberta_f1 = [roberta_results[cat]['f1'] for cat in all_categories]\n",
        "\n",
        "    avg_f1 = [(svm_f1[i] + cnn_f1[i] + lstm_f1[i] + roberta_f1[i])/4 for i in range(len(all_categories))]\n",
        "    indices = np.argsort(avg_f1)[::-1]\n",
        "\n",
        "    categories = [categories[i] for i in indices]\n",
        "    svm_f1 = [svm_f1[i] for i in indices]\n",
        "    cnn_f1 = [cnn_f1[i] for i in indices]\n",
        "    lstm_f1 = [lstm_f1[i] for i in indices]\n",
        "    roberta_f1 = [roberta_f1[i] for i in indices]\n",
        "\n",
        "    x = np.arange(len(categories))\n",
        "    width = 0.2\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    ax.bar(x - width, svm_f1, width, label='SVM', color='#4e79a7')\n",
        "    ax.bar(x, cnn_f1, width, label='CNN', color='#f28e2b')\n",
        "    ax.bar(x + width, lstm_f1, width, label='LSTM', color='#59a14f')\n",
        "    ax.bar(x + 2*width, roberta_f1, width, label='RoBERTa', color='#e15759')\n",
        "\n",
        "    ax.set_xlabel('Categories', fontsize=12)\n",
        "    ax.set_ylabel('F1 Score', fontsize=12)\n",
        "    ax.set_title('Model Performance Comparison by Category', fontsize=16)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(categories, rotation=60, ha='right')\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Update plot_model_comparison function call\n",
        "plot_model_comparison(svm_results, cnn_results, lstm_results, roberta_results, all_categories)\n",
        "roberta_dataset = TextDataset(train_texts, train_labels, tokenizer, max_seq_length)\n",
        "roberta_test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_seq_length)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16\n",
        "roberta_train_dataloader = DataLoader(roberta_dataset, batch_size=batch_size, shuffle=True)\n",
        "roberta_test_dataloader = DataLoader(roberta_test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(roberta_train_dataloader))\n",
        "\n",
        "# Train RoBERTa model\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in roberta_train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(roberta_train_dataloader)}')\n",
        "\n",
        "# Evaluate RoBERTa model\n",
        "model.eval()\n",
        "roberta_predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in roberta_test_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.sigmoid(logits)\n",
        "\n",
        "        roberta_predictions.extend(torch.round(torch.sigmoid(logits)).cpu().numpy())\n",
        "\n",
        "# Calculate metrics for RoBERTa\n",
        "roberta_results = {}\n",
        "for i, category in enumerate(all_categories):\n",
        "    f1 = f1_score(test_labels[:, i], roberta_predictions[:, i])\n",
        "    report = classification_report(test_labels[:, i], roberta_predictions[:, i], output_dict=True)\n",
        "    roberta_results[category] = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": report['1']['precision'] if 1 in report else 0,\n",
        "        \"recall\": report['1']['recall'] if 1 in report else 0\n",
        "    }\n",
        "    print(f\"RoBERTa - Category: {category}, F1: {f1}\")\n",
        "\n",
        "# Update plot_model_comparison function to include RoBERTa results\n",
        "def plot_model_comparison(svm_results, cnn_results, lstm_results, roberta_results, all_categories):\n",
        "    categories = [cat.replace('-', '_').replace('.', '_') for cat in all_categories]\n",
        "    svm_f1 = [svm_results[cat]['f1'] for cat in all_categories]\n",
        "    cnn_f1 = [cnn_results[cat]['f1'] for cat in all_categories]\n",
        "    lstm_f1 = [lstm_results[cat]['f1'] for cat in all_categories]\n",
        "    roberta_f1 = [roberta_results[cat]['f1'] for cat in all_categories]\n",
        "\n",
        "    avg_f1 = [(svm_f1[i] + cnn_f1[i] + lstm_f1[i] + roberta_f1[i])/4 for i in range(len(all_categories))]\n",
        "    indices = np.argsort(avg_f1)[::-1]\n",
        "\n",
        "    categories = [categories[i] for i in indices]\n",
        "    svm_f1 = [svm_f1[i] for i in indices]\n",
        "    cnn_f1 = [cnn_f1[i] for i in indices]\n",
        "    lstm_f1 = [lstm_f1[i] for i in indices]\n",
        "    roberta_f1 = [roberta_f1[i] for i in indices]\n",
        "\n",
        "    x = np.arange(len(categories))\n",
        "    width = 0.2\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    ax.bar(x - width, svm_f1, width, label='SVM', color='#4e79a7')\n",
        "    ax.bar(x, cnn_f1, width, label='CNN', color='#f28e2b')\n",
        "    ax.bar(x + width, lstm_f1, width, label='LSTM', color='#59a14f')\n",
        "    ax.bar(x + 2*width, roberta_f1, width, label='RoBERTa', color='#e15759')\n",
        "\n",
        "    ax.set_xlabel('Categories', fontsize=12)\n",
        "    ax.set_ylabel('F1 Score', fontsize=12)\n",
        "    ax.set_title('Model Performance Comparison by Category', fontsize=16)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(categories, rotation=60, ha='right')\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Update plot_model_comparison function call\n",
        "plot_model_comparison(svm_results, cnn_results, lstm_results, roberta_results, all_categories)"
      ],
      "metadata": {
        "id": "caHRqbwv8-LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM Vs CNN Vs LSTM (Working)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nhxJnu6n2NdO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l0FCDzLdw1Z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, array_contains, explode, udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.ml.feature import VectorAssembler, HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Extract and clean categories (same as before)\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [code.rsplit('.', 1)[0] for code in codes]\n",
        "    return []\n",
        "\n",
        "# Apply the UDF to extract categories\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Step 2: Get all unique categories with consistent naming\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Step 3: Create binary indicator columns (same as before)\n",
        "for category in all_categories:\n",
        "\n",
        "    # replacing the hyphens, dots with underscores\n",
        "    col_name = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache the dataframes again.\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "\n",
        "# Step 4: TF-IDF Features for traditional ML model (SVM)\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\", minDocFreq=2)\n",
        "\n",
        "# Apply transformations\n",
        "train_tokenized = tokenizer.transform(dfs_train_clean)\n",
        "test_tokenized = tokenizer.transform(dfs_test_clean)\n",
        "train_hashed = hashingTF.transform(train_tokenized)\n",
        "test_hashed = hashingTF.transform(test_tokenized)\n",
        "idf_model = idf.fit(train_hashed)\n",
        "train_tfidf = idf_model.transform(train_hashed)\n",
        "test_tfidf = idf_model.transform(test_hashed)\n",
        "\n",
        "# Step 5: Prepare data for deep learning models (CNN and LSTM)\n",
        "# Convert to pandas for easier processing\n",
        "train_pandas = dfs_train_clean.toPandas()\n",
        "test_pandas = dfs_test_clean.toPandas()\n",
        "validation_pandas = dfs_validation_clean.toPandas()\n",
        "\n",
        "# Prepare text data for deep learning models\n",
        "keras_tokenizer = KerasTokenizer(num_words=10000)\n",
        "keras_tokenizer.fit_on_texts(train_pandas['text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sequences = keras_tokenizer.texts_to_sequences(train_pandas['text'])\n",
        "test_sequences = keras_tokenizer.texts_to_sequences(test_pandas['text'])\n",
        "validation_sequences = keras_tokenizer.texts_to_sequences(validation_pandas['text'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_seq_length = 100\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "validation_data = pad_sequences(validation_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# Prepare multi-label targets for deep learning\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "train_labels = train_pandas[feature_cols].values\n",
        "test_labels = test_pandas[feature_cols].values\n",
        "validation_labels = validation_pandas[feature_cols].values\n",
        "\n",
        "# Step 6: Build and train different models\n",
        "\n",
        "# 6.1: SVM Model for each category (similar to previous code)\n",
        "svm_results = {}\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    col_name = f\"has_{clean_cat}\"\n",
        "\n",
        "    # Create label column for this specific category\n",
        "    train_data_svm = train_tfidf.withColumn(\"label\", col(col_name))\n",
        "    test_data_svm = test_tfidf.withColumn(\"label\", col(col_name))\n",
        "\n",
        "    # Use only TF-IDF features for prediction\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data_svm).select(\"features\", \"label\")\n",
        "    test_assembled = assembler.transform(test_data_svm).select(\"features\", \"label\")\n",
        "\n",
        "    # Train binary classifier\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=20, regParam=0.1)\n",
        "    svm_model = svm.fit(train_assembled)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = svm_model.transform(test_assembled)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1 = evaluator.evaluate(predictions)\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "    svm_results[category] = {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "    print(f\"SVM - Category: {category}, F1: {f1}, Precision: {precision}, Recall: {recall}\")\n",
        "\n",
        "# 6.2: CNN Model for multi-label classification\n",
        "def build_cnn_model(vocab_size, embedding_dim, max_length, num_categories):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(5))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_categories, activation='sigmoid'))  # Sigmoid for multi-label\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and train the CNN model\n",
        "vocab_size = min(10000, len(keras_tokenizer.word_index) + 1)\n",
        "embedding_dim = 100\n",
        "num_categories = len(all_categories)\n",
        "\n",
        "cnn_model = build_cnn_model(vocab_size, embedding_dim, max_seq_length, num_categories)\n",
        "print(cnn_model.summary())\n",
        "\n",
        "# Train with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history_cnn = cnn_model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(validation_data, validation_labels),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate CNN model\n",
        "cnn_predictions = cnn_model.predict(test_data)\n",
        "cnn_predictions_binary = (cnn_predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics for CNN\n",
        "cnn_results = {}\n",
        "for i, category in enumerate(all_categories):\n",
        "    f1 = f1_score(test_labels[:, i], cnn_predictions_binary[:, i])\n",
        "    report = classification_report(test_labels[:, i], cnn_predictions_binary[:, i], output_dict=True)\n",
        "    cnn_results[category] = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": report['1']['precision'] if 1 in report else 0,\n",
        "        \"recall\": report['1']['recall'] if 1 in report else 0\n",
        "    }\n",
        "    print(f\"CNN - Category: {category}, F1: {f1}\")\n",
        "\n",
        "# 6.3: LSTM Model for multi-label classification\n",
        "def build_lstm_model(vocab_size, embedding_dim, max_length, num_categories):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_categories, activation='sigmoid'))  # Sigmoid for multi-label\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and train LSTM model\n",
        "lstm_model = build_lstm_model(vocab_size, embedding_dim, max_seq_length, num_categories)\n",
        "print(lstm_model.summary())\n",
        "\n",
        "# Train with early stopping\n",
        "history_lstm = lstm_model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(validation_data, validation_labels),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate LSTM model\n",
        "lstm_predictions = lstm_model.predict(test_data)\n",
        "lstm_predictions_binary = (lstm_predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics for LSTM\n",
        "lstm_results = {}\n",
        "for i, category in enumerate(all_categories):\n",
        "    f1 = f1_score(test_labels[:, i], lstm_predictions_binary[:, i])\n",
        "    report = classification_report(test_labels[:, i], lstm_predictions_binary[:, i], output_dict=True)\n",
        "    lstm_results[category] = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": report['1']['precision'] if 1 in report else 0,\n",
        "        \"recall\": report['1']['recall'] if 1 in report else 0\n",
        "    }\n",
        "    print(f\"LSTM - Category: {category}, F1: {f1}\")\n",
        "\n",
        "# Step 7: Visualize and compare model performance\n",
        "def plot_model_comparison(svm_results, cnn_results, lstm_results, all_categories):\n",
        "    # Prepare data\n",
        "    categories = [cat.replace('-', '_').replace('.', '_') for cat in all_categories]\n",
        "    svm_f1 = [svm_results[cat]['f1'] for cat in all_categories]\n",
        "    cnn_f1 = [cnn_results[cat]['f1'] for cat in all_categories]\n",
        "    lstm_f1 = [lstm_results[cat]['f1'] for cat in all_categories]\n",
        "\n",
        "    # Sort by average F1 score\n",
        "    avg_f1 = [(svm_f1[i] + cnn_f1[i] + lstm_f1[i])/3 for i in range(len(all_categories))]\n",
        "    indices = np.argsort(avg_f1)[::-1]\n",
        "\n",
        "    categories = [categories[i] for i in indices]\n",
        "    svm_f1 = [svm_f1[i] for i in indices]\n",
        "    cnn_f1 = [cnn_f1[i] for i in indices]\n",
        "    lstm_f1 = [lstm_f1[i] for i in indices]\n",
        "\n",
        "    # Plot\n",
        "    x = np.arange(len(categories))\n",
        "    width = 0.25\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    ax.bar(x - width, svm_f1, width, label='SVM', color='#4e79a7')\n",
        "    ax.bar(x, cnn_f1, width, label='CNN', color='#f28e2b')\n",
        "    ax.bar(x + width, lstm_f1, width, label='LSTM', color='#59a14f')\n",
        "\n",
        "    ax.set_xlabel('Categories', fontsize=12)\n",
        "    ax.set_ylabel('F1 Score', fontsize=12)\n",
        "    ax.set_title('Model Performance Comparison by Category', fontsize=16)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(categories, rotation=60, ha='right')\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Plot training history for deep learning models\n",
        "def plot_training_history(history_cnn, history_lstm):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # CNN training history\n",
        "    ax1.plot(history_cnn.history['accuracy'], label='Train Accuracy')\n",
        "    ax1.plot(history_cnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_title('CNN Model Accuracy')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend()\n",
        "    ax1.grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "    # LSTM training history\n",
        "    ax2.plot(history_lstm.history['accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(history_lstm.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax2.set_title('LSTM Model Accuracy')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend()\n",
        "    ax2.grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Execute visualization functions\n",
        "plot_model_comparison(svm_results, cnn_results, lstm_results, all_categories)\n",
        "plot_training_history(history_cnn, history_lstm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ex5spSTw7NO"
      },
      "source": [
        "INSERT ABSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH7S22OYw6N0"
      },
      "outputs": [],
      "source": [
        "# Step 1: Extract and clean categories - use a consistent naming convention\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [code.rsplit('.', 1)[0] for code in codes]\n",
        "    return []\n",
        "\n",
        "# Apply the UDF to extract categories\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Step 2: Get all unique categories with consistent naming\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Step 3: Create binary indicator columns with consistent naming\n",
        "for category in all_categories:\n",
        "    # Use the original category name in the column name to avoid confusion\n",
        "    col_name = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "dfs_train_clean.show(10)\n",
        "\n",
        "# Step 4: Text vectorization with TF-IDF\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\", minDocFreq=2)\n",
        "\n",
        "# Apply transformations\n",
        "train_tokenized = tokenizer.transform(dfs_train_clean)\n",
        "test_tokenized = tokenizer.transform(dfs_test_clean)\n",
        "train_hashed = hashingTF.transform(train_tokenized)\n",
        "test_hashed = hashingTF.transform(test_tokenized)\n",
        "idf_model = idf.fit(train_hashed)\n",
        "train_tfidf = idf_model.transform(train_hashed)\n",
        "test_tfidf = idf_model.transform(test_hashed)\n",
        "\n",
        "# Step 5: Select the correct feature columns based on the actual column names\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Step 6: Build a multi-label classifier for each category\n",
        "# For each category, train a separate binary classifier\n",
        "results = {}\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    col_name = f\"has_{clean_cat}\"\n",
        "\n",
        "    # Create label column for this specific category\n",
        "    train_data = train_tfidf.withColumn(\"label\", col(col_name))\n",
        "    test_data = test_tfidf.withColumn(\"label\", col(col_name))\n",
        "\n",
        "    # Use only TF-IDF features for prediction\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data).select(\"features\", \"label\")\n",
        "    test_assembled = assembler.transform(test_data).select(\"features\", \"label\")\n",
        "\n",
        "    # Train binary classifier\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=20, regParam=0.1)\n",
        "    svm_model = svm.fit(train_assembled)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = svm_model.transform(test_assembled)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1 = evaluator.evaluate(predictions)\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "    results[category] = {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "    print(f\"Category: {category}, F1: {f1}, Precision: {precision}, Recall: {recall}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u814LsIoy0U9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'results' is the dictionary containing performance metrics\n",
        "categories = list(results.keys())\n",
        "f1_scores = [results[cat]['f1'] for cat in categories]\n",
        "precision_scores = [results[cat]['precision'] for cat in categories]\n",
        "recall_scores = [results[cat]['recall'] for cat in categories]\n",
        "\n",
        "# Create a DataFrame for easy plotting\n",
        "df = pd.DataFrame({\n",
        "    'Category': categories,\n",
        "    'F1 Score': f1_scores,\n",
        "    'Precision': precision_scores,\n",
        "    'Recall': recall_scores\n",
        "})\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "x = np.arange(len(categories))\n",
        "width = 0.25\n",
        "\n",
        "plt.bar(x - width, f1_scores, width, label='F1 Score')\n",
        "plt.bar(x, precision_scores, width, label='Precision')\n",
        "plt.bar(x + width, recall_scores, width, label='Recall')\n",
        "\n",
        "plt.xlabel('Categories')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Performance Metrics by Category')\n",
        "plt.xticks(x, categories, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('category_performance.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbORQisu09w9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# 1. Performance Metrics by Category\n",
        "def plot_category_performance(results, all_categories):\n",
        "    # Prepare cats again\n",
        "    categories = [cat.replace('-', '_').replace('.', '_') for cat in all_categories]\n",
        "    metrics = pd.DataFrame({\n",
        "        'Category': categories,\n",
        "        'F1 Score': [results[cat]['f1'] for cat in all_categories],\n",
        "        'Precision': [results[cat]['precision'] for cat in all_categories],\n",
        "        'Recall': [results[cat]['recall'] for cat in all_categories]\n",
        "    })\n",
        "\n",
        "    # Sort by F1 score\n",
        "    metrics = metrics.sort_values('F1 Score', ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.25\n",
        "\n",
        "    plt.bar(x - width, metrics['F1 Score'], width, label='F1 Score', color='#4e79a7')\n",
        "    plt.bar(x, metrics['Precision'], width, label='Precision', color='#f28e2b')\n",
        "    plt.bar(x + width, metrics['Recall'], width, label='Recall', color='#59a14f')\n",
        "\n",
        "    plt.xlabel('Categories', fontsize=12)\n",
        "    plt.ylabel('Score', fontsize=12)\n",
        "    plt.title('Performance Metrics by Category', fontsize=16)\n",
        "    plt.xticks(x, metrics['Category'], rotation=60, ha='right')\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('category_performance.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 2. Category Distribution Heatmap\n",
        "def plot_category_distribution(df_train):\n",
        "    # Get category counts\n",
        "    category_counts = {}\n",
        "    for category in all_categories:\n",
        "        col_name = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "        category_counts[category] = df_train.filter(col(col_name) == 1).count()\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    counts_df = pd.DataFrame(list(category_counts.items()), columns=['Category', 'Count'])\n",
        "    counts_df = counts_df.sort_values('Count', ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Count', y='Category', data=counts_df, palette='viridis')\n",
        "    plt.title('Distribution of Categories in Training Data', fontsize=16)\n",
        "    plt.xlabel('Number of Instances', fontsize=12)\n",
        "    plt.ylabel('Category', fontsize=12)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('category_distribution.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 3. Co-occurrence Matrix Heatmap (Cite: https://www.researchgate.net/figure/Heat-maps-of-co-occurrence-matrix-between-different-labels-reflecting-the-frequency-of_fig5_340343785)\n",
        "def plot_category_cooccurrence(df_train):\n",
        "    # Convert to pandas for easier processing\n",
        "    feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "    df_pandas = df_train.select(feature_cols).toPandas()\n",
        "\n",
        "    # Calculate co-occurrence\n",
        "    cooccurrence = df_pandas.T.dot(df_pandas)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    mask = np.triu(np.ones_like(cooccurrence, dtype=bool))\n",
        "    with sns.axes_style(\"white\"):\n",
        "        sns.heatmap(cooccurrence, mask=mask, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n",
        "                    xticklabels=all_categories, yticklabels=all_categories)\n",
        "    plt.title('Category Co-occurrence Matrix', fontsize=16)\n",
        "    plt.xticks(rotation=60, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('category_cooccurrence.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 4. Word Importance Visualization\n",
        "def plot_word_importance(vectorizer, model, category, top_n=20):\n",
        "    \"\"\"Visualize the most important words for a specific category.\"\"\"\n",
        "    # Get feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Get coefficients\n",
        "    coef = model.coef_[0]\n",
        "\n",
        "    # Create DataFrame with words and their importance\n",
        "    word_importance = pd.DataFrame(\n",
        "        {'Word': feature_names, 'Importance': coef}\n",
        "    ).sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Plot top positive and negative words\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Top positive words (indicating presence of category)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    top_positive = word_importance.head(top_n)\n",
        "    sns.barplot(x='Importance', y='Word', data=top_positive, palette='Blues_d')\n",
        "    plt.title(f'Top {top_n} Words Indicating \"{category}\"', fontsize=14)\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Top negative words (indicating absence of category)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    top_negative = word_importance.tail(top_n).sort_values('Importance')\n",
        "    sns.barplot(x='Importance', y='Word', data=top_negative, palette='Reds_d')\n",
        "    plt.title(f'Top {top_n} Words Against \"{category}\"', fontsize=14)\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'word_importance_{category}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 5. Confusion Matrix for Each Category\n",
        "def plot_confusion_matrices(y_true, y_pred, all_categories):\n",
        "    \"\"\"Confusion matrices - all categories.\"\"\"\n",
        "    # Create a grid of confusion matrices\n",
        "    n_categories = len(all_categories)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_categories + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, category in enumerate(all_categories):\n",
        "        if i < len(axes):\n",
        "            # Get true and predicted values for this category\n",
        "            true = y_true[:, i]\n",
        "            pred = y_pred[:, i]\n",
        "\n",
        "            # Calculate confusion matrix\n",
        "            cm = confusion_matrix(true, pred)\n",
        "\n",
        "            # Plot\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])\n",
        "            axes[i].set_title(f'Confusion Matrix: {category}')\n",
        "            axes[i].set_xlabel('Predicted')\n",
        "            axes[i].set_ylabel('True')\n",
        "            axes[i].set_xticklabels(['No', 'Yes'])\n",
        "            axes[i].set_yticklabels(['No', 'Yes'])\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for i in range(n_categories, len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 6. ROC Curves\n",
        "def plot_roc_curves(models, X_test, y_test, all_categories):\n",
        "    \"\"\"Plot ROC curves for all category classifiers.\"\"\"\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    for i, category in enumerate(all_categories):\n",
        "        # Get true values\n",
        "        y_true = y_test[:, i]\n",
        "\n",
        "        # Get predictions\n",
        "        y_score = models[category].decision_function(X_test)\n",
        "\n",
        "        # Calculate ROC curve\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plot\n",
        "        plt.plot(fpr, tpr, lw=2,\n",
        "                 label=f'{category} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for All Categories')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('roc_curves.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 7. Learning Curve\n",
        "def plot_learning_curve(model, X, y, category, cv=5):\n",
        "    \"\"\"Learning curve which shows model performance with increasing data.\"\"\"\n",
        "    from sklearn.model_selection import learning_curve\n",
        "\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        model, X, y, cv=cv, n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10), scoring='f1')\n",
        "\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='r', label='Training score')\n",
        "    plt.plot(train_sizes, test_mean, 'o-', color='g', label='Cross-validation score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')\n",
        "    plt.xlabel('Training set size')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title(f'Learning Curve for {category}')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'learning_curve_{category}.png', dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I2e9Cbncf8f"
      },
      "source": [
        "OLDER CODE:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aANKu0tA1bNA"
      },
      "outputs": [],
      "source": [
        "dfs_train_clean.show(5)\n",
        "dfs_test_clean.show(5)\n",
        "plot_category_distribution(dfs_test_clean)\n",
        "plot_category_cooccurrence(dfs_test_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTg0cCjYNd_P"
      },
      "outputs": [],
      "source": [
        "#Cleaning and Extracting functions\n",
        "def clean_categories(categories):\n",
        "    return categories.replace('-', '_').replace('.', '_')\n",
        "\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "        return [clean_categories(cat) for cat in categories]\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEDS52iDOj18"
      },
      "outputs": [],
      "source": [
        "#dfs_train_clean = dfs_train_clean.limit(5000)\n",
        "#\n",
        "# Extract categories for each DataFrame\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "dfs_train_clean = dfs_train_clean.limit(80)\n",
        "\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "dfs_test_clean = dfs_test_clean.limit(80)\n",
        "\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "dfs_validation_clean = dfs_validation_clean.limit(80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3TLXIdil6uR"
      },
      "outputs": [],
      "source": [
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames to avoid recomputation\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYcDfHqTmI-J"
      },
      "outputs": [],
      "source": [
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z43w955emZB5"
      },
      "outputs": [],
      "source": [
        "dfs_train_clean.show(5)\n",
        "dfs_test_clean.show(5)\n",
        "\n",
        "# Cache DataFrames to avoid recomputation\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "\n",
        "dfs_train_clean.select(\"data_source\", \"categories\").show(20, truncate=False)\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "print(f\"Feature cols=\",feature_cols)\n",
        "\n",
        "\n",
        "dfs_train_clean.select(\"data_source\", \"categories\").show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYBtBgDNuquY"
      },
      "outputs": [],
      "source": [
        "# SKIP\n",
        "# 3. Train Classifiers\n",
        "#models = {}\n",
        "#for category in all_categories:\n",
        "#    lr = LogisticRegression(featuresCol=\"features\", labelCol=category)\n",
        "#    pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, lr])\n",
        "#    models[category] = pipeline.fit(df)#\n",
        "#\n",
        "## 4. Test on New Data (new_text_df)\n",
        "#for category, model in models.items():\n",
        "#    predictions = model.transform(new_text_df)\n",
        "#    predictions.select(\"text\", category, \"prediction\").show() #show the text, the real category, and the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tonS2_OvtJYV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, array_contains, lit\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmvS2HLNs_q2"
      },
      "outputs": [],
      "source": [
        "# Train SVM Model\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train_data = assembler.transform(dfs_train_clean).select(\"features\", \"id\")\n",
        "\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"id\", maxIter=20, regParam=0.1)\n",
        "svm_model = svm.fit(train_data)\n",
        "\n",
        "# Test on test set\n",
        "test_data = assembler.transform(dfs_test_clean).select(\"features\", \"id\")\n",
        "svm_predictions = svm_model.transform(test_data)\n",
        "\n",
        "# Evaluate SVM\n",
        "svm_evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1_svm = svm_evaluator.evaluate(svm_predictions)\n",
        "precision_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"SVM F1 Score: {f1_svm}\")\n",
        "print(f\"SVM Precision: {precision_svm}\")\n",
        "print(f\"SVM Recall: {recall_svm}\")\n",
        "\n",
        "# Convert to Pandas for LSTM and CNN\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "X_train = train_pandas[feature_cols].values\n",
        "X_test = test_pandas[feature_cols].values\n",
        "\n",
        "y_train = train_pandas['id'].values\n",
        "y_test = test_pandas['id'].values\n",
        "\n",
        "# Reshape for LSTM and CNN\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1)),\n",
        "    Dropout(0.2),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"LSTM Accuracy: {lstm_acc}\")\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "cnn_loss, cnn_acc = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"CNN Accuracy: {cnn_acc}\")\n",
        "\n",
        "# Clear cache to free memory\n",
        "dfs_train_clean.unpersist()\n",
        "dfs_test_clean.unpersist()\n",
        "dfs_validation_clean.unpersist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK-BA9GQve4r"
      },
      "outputs": [],
      "source": [
        "\n",
        "#lr = LogisticRegression(featuresCol=\"features\", labelCol=\"id\", maxIter=20, regParam=0.1)\n",
        "#model = lr.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJtWWzN-o_VR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0UXCRfrPuzH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Test on test set\n",
        "test_data = assembler.transform(dfs_test_clean).select(\"features\", \"id\")\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1 = evaluator.evaluate(predictions)\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Validate on validation set\n",
        "validation_data = assembler.transform(dfs_validation_clean).select(\"features\", \"id\")\n",
        "validation_predictions = model.transform(validation_data)\n",
        "\n",
        "f1_val = evaluator.evaluate(validation_predictions)\n",
        "precision_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"Validation F1 Score: {f1_val}\")\n",
        "print(f\"Validation Precision: {precision_val}\")\n",
        "print(f\"Validation Recall: {recall_val}\")\n",
        "\n",
        "# Clear cache to free memory\n",
        "dfs_train_clean.unpersist()\n",
        "dfs_test_clean.unpersist()\n",
        "dfs_validation_clean.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEP3xhGmKyX-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Train Logistic Regression Model\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train_data = assembler.transform(dfs_train_clean).select(\"features\", \"id\")\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"id\")\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# Test on test set\n",
        "test_data = assembler.transform(dfs_test_clean).select(\"features\", \"id\")\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1 = evaluator.evaluate(predictions)\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Validate on validation set\n",
        "validation_data = assembler.transform(dfs_validation_clean).select(\"features\", \"id\")\n",
        "validation_predictions = model.transform(validation_data)\n",
        "\n",
        "f1_val = evaluator.evaluate(validation_predictions)\n",
        "precision_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"Validation F1 Score: {f1_val}\")\n",
        "print(f\"Validation Precision: {precision_val}\")\n",
        "print(f\"Validation Recall: {recall_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a23wEzZimqsp"
      },
      "outputs": [],
      "source": [
        "#This one contains text vectorization embeddings\n",
        "# ACD-Only Classifier Pipeline\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains, lit\n",
        "from pyspark.ml.feature import VectorAssembler, HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# Step 1: Extract Categories\n",
        "\n",
        "def clean_categories(categories):\n",
        "    return categories.replace('-', '_').replace('.', '_')\n",
        "\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "        return [clean_categories(cat) for cat in categories]\n",
        "    return []\n",
        "\n",
        "# Extract categories for each DataFrame\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames to avoid recomputation\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "\n",
        "# Text vectorization using TF-IDF\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
        "\n",
        "# Apply transformations\n",
        "train_tokenized = tokenizer.transform(dfs_train_clean)\n",
        "test_tokenized = tokenizer.transform(dfs_test_clean)\n",
        "\n",
        "train_hashed = hashingTF.transform(train_tokenized)\n",
        "test_hashed = hashingTF.transform(test_tokenized)\n",
        "\n",
        "idf_model = idf.fit(train_hashed)\n",
        "train_tfidf = idf_model.transform(train_hashed)\n",
        "test_tfidf = idf_model.transform(test_hashed)\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Combine binary and text-based features\n",
        "assembler = VectorAssembler(inputCols=feature_cols + [\"tfidf_features\"], outputCol=\"features\")\n",
        "train_data = assembler.transform(train_tfidf).select(\"features\", \"id\")\n",
        "test_data = assembler.transform(test_tfidf).select(\"features\", \"id\")\n",
        "\n",
        "# Train SVM Model\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"id\", maxIter=20, regParam=0.1)\n",
        "svm_model = svm.fit(train_data)\n",
        "\n",
        "# Test on test set\n",
        "svm_predictions = svm_model.transform(test_data)\n",
        "\n",
        "# Evaluate SVM\n",
        "svm_evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1_svm = svm_evaluator.evaluate(svm_predictions)\n",
        "precision_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"SVM F1 Score: {f1_svm}\")\n",
        "print(f\"SVM Precision: {precision_svm}\")\n",
        "print(f\"SVM Recall: {recall_svm}\")\n",
        "\n",
        "# Convert to Pandas for LSTM and CNN\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "X_train = train_pandas[feature_cols].values\n",
        "X_test = test_pandas[feature_cols].values\n",
        "\n",
        "y_train = train_pandas['id'].values\n",
        "y_test = test_pandas['id'].values\n",
        "\n",
        "# Reshape for LSTM and CNN\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1)),\n",
        "    Dropout(0.2),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"LSTM Accuracy: {lstm_acc}\")\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "cnn_loss, cnn_acc = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"CNN Accuracy: {cnn_acc}\")\n",
        "\n",
        "# Clear cache to free memory\n",
        "dfs_train_clean.unpersist()\n",
        "dfs_test_clean.unpersist()\n",
        "dfs_validation_clean.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd7M82pWT78Y"
      },
      "outputs": [],
      "source": [
        "# ACD-Only Classifier Pipeline\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains, lit\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# Step 1: Extract Categories\n",
        "\n",
        "def clean_categories(categories):\n",
        "    return categories.replace('-', '_').replace('.', '_')\n",
        "\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "        return [clean_categories(cat) for cat in categories]\n",
        "    return []\n",
        "\n",
        "# Extract categories for each DataFrame\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames to avoid recomputation\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Train SVM Model\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train_data = assembler.transform(dfs_train_clean).select(\"features\", \"text\")\n",
        "\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"text\", maxIter=20, regParam=0.1)\n",
        "svm_model = svm.fit(train_data)\n",
        "\n",
        "# Test on test set\n",
        "test_data = assembler.transform(dfs_test_clean).select(\"features\", \"text\")\n",
        "svm_predictions = svm_model.transform(test_data)\n",
        "\n",
        "# Evaluate SVM\n",
        "svm_evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1_svm = svm_evaluator.evaluate(svm_predictions)\n",
        "precision_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"SVM F1 Score: {f1_svm}\")\n",
        "print(f\"SVM Precision: {precision_svm}\")\n",
        "print(f\"SVM Recall: {recall_svm}\")\n",
        "\n",
        "# Convert to Pandas for LSTM and CNN\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "X_train = train_pandas[feature_cols].values\n",
        "X_test = test_pandas[feature_cols].values\n",
        "\n",
        "y_train = train_pandas['id'].values\n",
        "y_test = test_pandas['id'].values\n",
        "\n",
        "# Reshape for LSTM and CNN\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1)),\n",
        "    Dropout(0.2),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"LSTM Accuracy: {lstm_acc}\")\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "cnn_loss, cnn_acc = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"CNN Accuracy: {cnn_acc}\")\n",
        "\n",
        "# Clear cache to free memory\n",
        "dfs_train_clean.unpersist()\n",
        "dfs_test_clean.unpersist()\n",
        "dfs_validation_clean.unpersist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky-gnLs4JDV1"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "# Convert 'label_codes' from string to list\n",
        "dfs_train['label_codes'] = dfs_train['label_codes'].apply(\n",
        "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "# Extract categories and clean them\n",
        "def extract_categories(label_codes):\n",
        "    return [\n",
        "        code.rsplit('.', 1)[0].replace('-', '_').replace('.', '_')\n",
        "        for code in label_codes\n",
        "    ]\n",
        "\n",
        "# Apply the function to create a 'categories' column\n",
        "dfp_train['categories'] = dfp_train['label_codes'].apply(extract_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH6mayVWnipM"
      },
      "outputs": [],
      "source": [
        "\n",
        "#dfs_train = spark.createDataFrame(dfp_train)     #pandas df has no ... withColumn so we need to convert it - Oops it's not a pandas df, it's a pyarrow Table that needs to be first converted to a pandas df\n",
        "#dfp_train = dfp_train.to_pandas()\n",
        "#dfs_train = spark.createDataFrame(dfp_train)\n",
        "##It seems that the to_pandas method is not available for Spark DataFrames -  use the collect method to convert the Spark DataFrame to a Pandas DataFrame:\n",
        "\n",
        "# Convert Spark DataFrame to Pandas\n",
        "dfp_train = spark.read.parquet(parquet_train_file).toPandas()\n",
        "print ( dfp_train.columns)\n",
        "# For each category, create a binary indicator column (1 if present, 0 if not)\n",
        "print(dfp_train.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSDhKNxTpY3R"
      },
      "outputs": [],
      "source": [
        "#df_with_categories = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "# Create a list of all unique categories from the dataset\n",
        "df_with_categories.printSchema()\n",
        "#all_categories = df_with_categories.select(explode(\"categories\")).distinct().collect()\n",
        "#all_categories = [row[0] for row in all_categories]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7h538G6hGd_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cye52wMXoiFh"
      },
      "outputs": [],
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) with Clean Column Names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: Extract Sentiment for Each Category\n",
        "\n",
        "# Extract 'label_codes' from the original Spark DataFrame\n",
        "#label_codes_df = df_with_categories.select('text', 'label_codes').toPandas()\n",
        "label_codes_df = df_with_categories.select('text', 'label_codes', 'categories').toPandas()\n",
        "print(label_codes_df)\n",
        "\n",
        "# Merge with train_pandas using the 'id' column\n",
        "train_pandas = train_pandas.merge(label_codes_df, on='id', how='left')\n",
        "\n",
        "def extract_aspect_and_sentiment(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [(code.rsplit('.', 1)[0], int(code.rsplit('.', 1)[-1])) for code in codes]\n",
        "    return []\n",
        "\n",
        "train_pandas['aspect_sentiment_pairs'] = train_pandas['label_codes'].apply(extract_aspect_and_sentiment)\n",
        "\n",
        "# For each category, create a sentiment column (-1, 0, or 1)\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    train_pandas[f\"sentiment_{clean_aspect}\"] = train_pandas['aspect_sentiment_pairs'].apply(\n",
        "        lambda x: next((sent for asp, sent in x if asp == aspect), 0)\n",
        "    )\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "#train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "#test_pandas = test_data.limit(200).toPandas()\n",
        "train_pandas = df_with_categories.limit(1000).toPandas()\n",
        "test_pandas = df_with_categories.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery_life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfeFLTrL-i_U"
      },
      "source": [
        "PRINT ALL CATEGORIES FOUND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd0-2x4Lu2ys"
      },
      "outputs": [],
      "source": [
        "for cat in all_categories:\n",
        "  print(cat + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suGL6w1BwrI8"
      },
      "outputs": [],
      "source": [
        "# For each category, create a binary indicator column (1 if present, 0 if not)\n",
        "for cat in all_categories:\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{cat.replace('-', '_').replace('.', '_')}\",\n",
        "        array_contains(col(\"categories\"), cat).cast(\"integer\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0u6elBY-uWj"
      },
      "outputs": [],
      "source": [
        "for cat in all_categories:\n",
        "  print(cat + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM7ZkSlToFeK"
      },
      "outputs": [],
      "source": [
        "print(train_pandas.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYsspjOVnkvd"
      },
      "outputs": [],
      "source": [
        "#FIX ME TODAY\n",
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) with Clean Column Names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: Extract Sentiment for Each Category\n",
        "\n",
        "def extract_aspect_and_sentiment(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [(code.rsplit('.', 1)[0], int(code.rsplit('.', 1)[-1])) for code in codes]\n",
        "    return []\n",
        "\n",
        "train_pandas['aspect_sentiment_pairs'] = train_pandas['label_codes'].apply(extract_aspect_and_sentiment)\n",
        "\n",
        "# For each category, create a sentiment column (-1, 0, or 1)\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    train_pandas[f\"sentiment_{clean_aspect}\"] = train_pandas['aspect_sentiment_pairs'].apply(\n",
        "        lambda x: next((sent for asp, sent in x if asp == aspect), 0)\n",
        "    )\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery_life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbOTD5kNXdfx"
      },
      "source": [
        "Tokenize the user comments from the text column and remove stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw5lYX0_LauT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "# Select relevant columns for the ACD task\n",
        "feature_cols = [f\"has_{category.replace('-', '_').replace('.', '_')}\" for category in all_categories]\n",
        "acd_df = df_with_categories.select(\"id\", \"text\", *feature_cols)\n",
        "\n",
        "# Convert text to features using TF-IDF\n",
        "# First, tokenize the text\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(acd_df)\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered_data = remover.transform(wordsData)\n",
        "\n",
        "# Convert words to term frequency features\n",
        "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"tf\", minDF=2.0)\n",
        "cv_model = cv.fit(filtered_data)\n",
        "tf_data = cv_model.transform(filtered_data)\n",
        "\n",
        "# Convert term frequency features to TF-IDF\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
        "idf_model = idf.fit(tf_data)\n",
        "tfidf_data = idf_model.transform(tf_data)\n",
        "\n",
        "# Final dataset ready for modeling\n",
        "final_acd_df = tfidf_data.select(\"id\", \"features\", *feature_cols)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = final_acd_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Cache the datasets for faster processing\n",
        "train_data.cache()\n",
        "test_data.cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIddta9BZVDZ"
      },
      "outputs": [],
      "source": [
        "#final_acd_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCPtQoKDXbiF"
      },
      "outputs": [],
      "source": [
        "##stopword removal\n",
        "\n",
        "# Load the training and validate datasets into DataFrames\n",
        "#train_df = spark.read.csv(\"path/to/train/dataset\", header=True, inferSchema=True)\n",
        "#val_df = spark.read.csv(\"path/to/val/dataset\", header=True, inferSchema=True)\n",
        "\n",
        "# Define a function to extract features from each column in the dataset\n",
        "def feature_extraction(df):\n",
        "    # Tokenization and stopword removal\n",
        "    df = df.select(\n",
        "        explode(col(\"text\").cast(\"string\")).alias(\"tokens\"),\n",
        "        explode(col(\"sentiment\").cast(\"string\")).alias(\"sentiment\")\n",
        "    )\n",
        "\n",
        "    # Stemming or Lemmatization\n",
        "    df = df.withColumn(\n",
        "        \"stemmed_tokens\",\n",
        "        df.tokens.map(lambda x: x.lower() if x.isnumeric() else x)  # Remove numbers and convert to lowercase\n",
        "    )\n",
        "\n",
        "    # Part-of-speech tagging (optional)\n",
        "    #df = df.withColumn(\"pos_tags\", explode(col(\"word\").cast(\"string\")).map(lambda x: {\"POS\": \"NNP\"} if x.isnumeric() else {\"POS\": \"NOUN\"})  # Example\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeoRym6BzNyR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-idjLDxZGVL4"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#all_words = filtered_data.rdd.flatMap(lambda x: x.filtered_words).collect()\n",
        "all_words = filtered_data.select(\"filtered_words\").rdd.flatMap(lambda x: x.filtered_words).collect()\n",
        "word_counts = Counter(all_words)\n",
        "top_words = dict(word_counts.most_common(20))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(top_words.keys(), top_words.values())\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Top 20 Word Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#2. TF-IDF Visualization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAgDXRpaLYpC"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Extract only the binary category columns and sentiment\n",
        "category_columns = [col for col in train_pandas.columns if col.startswith('has_')]\n",
        "df_for_heatmap = train_pandas[category_columns + ['sentiment']]\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df_for_heatmap.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Aspect-Sentiment Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5ntO_-ZITfq"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Chart: Heatmap or bar chart.\n",
        "#Shows TF-IDF scores for selected words across documents.\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "#\n",
        "selected_docs = tfidf_data.take(5) #take the first 5 documents.\n",
        "selected_words = cv_model.vocabulary[:10] # take the first 10 vocabulary words.\n",
        "\n",
        "tfidf_matrix = np.zeros((len(selected_docs), len(selected_words)))\n",
        "\n",
        "for i, row in enumerate(selected_docs):\n",
        "    dense_features = row.features.toArray()\n",
        "    for j, word in enumerate(selected_words):\n",
        "        if word in cv_model.vocabulary:\n",
        "            word_index = cv_model.vocabulary.index(word)\n",
        "            tfidf_matrix[i,j] = dense_features[word_index]\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(tfidf_matrix, annot=True, xticklabels=selected_words)\n",
        "plt.xlabel(\"Selected Words\")\n",
        "plt.ylabel(\"Documents\")\n",
        "plt.title(\"TF-IDF Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "#3. Model Evaluation Metrics:\n",
        "#\n",
        "#Charts: Confusion matrix, ROC curve, precision-recall curve.\n",
        "\n",
        "\n",
        "doc_lengths = wordsData.rdd.map(lambda x: len(x.words)).collect()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(doc_lengths, bins=50)\n",
        "plt.xlabel(\"Document Length (Number of Words)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Document Length Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YdoY8B4lQQ6"
      },
      "outputs": [],
      "source": [
        "train_data.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9isoSjT1zSh"
      },
      "source": [
        "Models were running too slow. \\Caching the DataFrame during SVM training to avoid redundant data processing. Reducing the number of SVM iterations and limiting the dataset size for deep learning. Simplifying the LSTM and CNN architectures by reducing the number of units and filters. Lowering the number of epochs and silencing verbose outputs for faster training.\n",
        "\n",
        "Added visualizations to pipeline:\n",
        "\n",
        "LSTM Loss Over Epochs to show how the model improves during training.\n",
        "CNN Accuracy Over Epochs to track performance across epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhwcmnLmuzZF"
      },
      "outputs": [],
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) with Clean Column Names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: ABSA (SVM Classifier for each Aspect) with Caching for Speed\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{clean_aspect}') == 1).cache()\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=5)  # Reduced iterations for speed\n",
        "    model = svm.fit(tfidf_data)\n",
        "\n",
        "    aspect_df.unpersist()\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery_life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Z3_Jc1u1RR"
      },
      "source": [
        "THE END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-4eMTi81uzT"
      },
      "outputs": [],
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models)\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD)\n",
        "for aspect in all_categories:\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: ABSA (SVM Classifier for each Aspect) with Caching for Speed\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{aspect}') == 1).cache()\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=5)  # Reduced iterations for speed\n",
        "    model = svm.fit(tfidf_data)\n",
        "\n",
        "    aspect_df.unpersist()\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery-life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ltf8M5Oc2S-7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqRJCneG1a21"
      },
      "source": [
        "Models were running too slow.\n",
        "\\Caching the DataFrame during SVM training to avoid redundant data processing.\n",
        "Reducing the number of SVM iterations and limiting the dataset size for deep learning.\n",
        "Simplifying the LSTM and CNN architectures by reducing the number of units and filters.\n",
        "Lowering the number of epochs and silencing verbose outputs for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNmIWqJYF47I"
      },
      "outputs": [],
      "source": [
        "# Convert Spark DataFrame to Pandas for deep learning\n",
        "# Collect the data (be careful with large datasets)\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "# We need to convert the sparse vector features to numpy arrays\n",
        "from pyspark.ml.linalg import SparseVector\n",
        "\n",
        "def sparse_to_array(sparse_vector):\n",
        "    return sparse_vector.toArray() if isinstance(sparse_vector, SparseVector) else sparse_vector\n",
        "\n",
        "# Convert features to numpy arrays\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Prepare X (features) and y (labels) for training\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "\n",
        "# Get all label columns\n",
        "label_cols = [col for col in train_pandas.columns if col.startswith('has_')]\n",
        "y_train = train_pandas[label_cols].values\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "# Now build a deep learning model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define model architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "results = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {results[0]}\")\n",
        "print(f\"Test Accuracy: {results[1]}\")\n",
        "print(f\"Test Precision: {results[2]}\")\n",
        "print(f\"Test Recall: {results[3]}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate F1 score manually\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "precision = precision_score(y_test, y_pred_binary, average='micro')\n",
        "recall = recall_score(y_test, y_pred_binary, average='micro')\n",
        "f1 = f1_score(y_test, y_pred_binary, average='micro')\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onEAObLs1M6Y"
      },
      "source": [
        "END WORKING *AREA*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I0GaU5Umj0E"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "# For simplicity, let's implement a binary classification model for each category\n",
        "# In practice, you might want to use a multi-label approach\n",
        "\n",
        "# Function to train and evaluate an SVM model for one category\n",
        "def train_svm_for_category(category, train_data, test_data):\n",
        "    # Prepare label column name\n",
        "    label_col = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "\n",
        "    # Create SVM model\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=label_col, maxIter=10)\n",
        "\n",
        "    # Train the model\n",
        "    model = svm.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # Evaluate model\n",
        "   # evaluator = MulticlassClassificationEvaluator(\n",
        "   #     labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
        "   # f1 = evaluator.evaluate(predictions)\n",
        "    #\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col,\n",
        "                                              predictionCol=\"prediction\",\n",
        "                                              metricName=\"weightedPrecision\")\n",
        "precision = evaluator.evaluate(predictions)\n",
        "print(\"Weighted Precision:\", precision)\n",
        "    evaluator.setMetricName(\"precision\")\n",
        "    precision = evaluator.evaluate(predictions)\n",
        "\n",
        "    evaluator.setMetricName(\"recall\")\n",
        "    recall = evaluator.evaluate(predictions)\n",
        "\n",
        "    return {\"category\": category, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Train models for all categories\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "    result = train_svm_for_category(category, train_data, test_data)\n",
        "    svm_results.append(result)\n",
        "\n",
        "# Print results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "svm_results_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjwA9QxfmIkK"
      },
      "outputs": [],
      "source": [
        "print(\"Training SVM models for all categories...\")\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "    try:\n",
        "        result = train_svm_for_category(category, train_data, test_data)\n",
        "        svm_results.append(result)\n",
        "        print(f\"Completed category: {category}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing category {category}: {str(e)}\")\n",
        "\n",
        "# Create a dataframe with results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "print(\"SVM Results:\")\n",
        "svm_results_df.show()\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_precision = np.mean([r[\"precision\"] for r in svm_results])\n",
        "avg_recall = np.mean([r[\"recall\"] for r in svm_results])\n",
        "avg_f1 = np.mean([r[\"f1\"] for r in svm_results])\n",
        "\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average F1: {avg_f1:.4f}\")\n",
        "\n",
        "# Convert to pandas DataFrame for deep learning model\n",
        "# We'll process a subset of records to handle memory constraints\n",
        "# For a real model, you might want to use a data loader or mini-batches\n",
        "print(\"Preparing data for deep learning model...\")\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit to 1000 samples for demonstration\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Function to convert sparse vector to array\n",
        "def sparse_to_array(v):\n",
        "    if hasattr(v, 'toArray'):\n",
        "        return v.toArray()\n",
        "    return v\n",
        "\n",
        "# Apply conversion function\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Stack features into numpy arrays\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "\n",
        "# Extract labels\n",
        "y_train = train_pandas[feature_cols].values\n",
        "y_test = test_pandas[feature_cols].values\n",
        "\n",
        "print(\"Data preparation for deep learning complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZ0jSpGKD9d"
      },
      "outputs": [],
      "source": [
        "# Train models for all categories\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "   # result = train_svm_for_category(category, train_data, test_data)\n",
        "    result = train_svm_for_category(category, train_data, test_data)\n",
        "    svm_results.append(result)\n",
        "\n",
        "# Print results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "svm_results_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLr-hMhUwTP9"
      },
      "source": [
        "# **Speeded up version Linear SVC vs LSTM vs CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQuL0YIJwPjO"
      },
      "outputs": [],
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - corrected, previous version too slow\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) - Clean column names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "## Step 1: Aspect Category Detection (ACD)\n",
        "#for aspect in all_categories:\n",
        "#    df_with_categories = df_with_categories.withColumn(\n",
        "#        f\"has_{aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "#    )\n",
        "#\n",
        "#\n",
        "## Step 2: ABSA (SVM Classifier for each Aspect) with Caching for Speed\n",
        "#def train_absa_svm(aspect, df_with_categories):\n",
        "#    aspect_df = df_with_categories.filter(col(f'has_{aspect}') == 1).cache()\n",
        "#\n",
        "\n",
        "# Step 2: Adjust the SVM Function to Use the Clean Column Name\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{clean_aspect}') == 1).cache()\n",
        "\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=5)  # Reduced iterations for speed\n",
        "    model = svm.fit(tfidf_data)\n",
        "\n",
        "    aspect_df.unpersist()\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery-life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuP_tJHBwQaw"
      },
      "source": [
        "# **FINAL - NOT YET WORKING COMPARISION ON ROBERTA, CNN, and Logistic Reg. **\n",
        "Same bug. I have fixed a lot of bugs this month, but I have run out of time to fix this 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias' issue, unfortunately. Thanks for your time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Deep Learning Imports\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Scikit-learn Imports for Logistic Regression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# RoBERTa Imports\n",
        "import transformers\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "\n",
        "# Convert to pandas for easier processing\n",
        "train_pandas = dfs_train_with_sentiment.toPandas()\n",
        "test_pandas = dfs_test_with_sentiment.toPandas()\n",
        "validation_pandas = dfs_validation_with_sentiment.toPandas()\n",
        "\n",
        "# Tokenize text for CNN\n",
        "keras_tokenizer = KerasTokenizer(num_words=10000)\n",
        "keras_tokenizer.fit_on_texts(train_pandas['text'])\n",
        "\n",
        "# Convert text to sequences for CNN\n",
        "train_sequences = keras_tokenizer.texts_to_sequences(train_pandas['text'])\n",
        "test_sequences = keras_tokenizer.texts_to_sequences(test_pandas['text'])\n",
        "validation_sequences = keras_tokenizer.texts_to_sequences(validation_pandas['text'])\n",
        "\n",
        "# Pad sequences for CNN\n",
        "max_seq_length = 100\n",
        "train_data_cnn = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data_cnn = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "validation_data_cnn = pad_sequences(validation_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# RoBERTa Tokenizer and Model\n",
        "roberta_model_name = 'roberta-base'\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model_name)\n",
        "roberta_model = TFRobertaModel.from_pretrained(roberta_model_name)\n",
        "\n",
        "# RoBERTa Tokenization Function\n",
        "def roberta_tokenize(texts, max_length=128):\n",
        "    return roberta_tokenizer(\n",
        "        list(texts),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "# Tokenize for RoBERTa\n",
        "train_data_roberta = roberta_tokenize(train_pandas['text'])\n",
        "test_data_roberta = roberta_tokenize(test_pandas['text'])\n",
        "validation_data_roberta = roberta_tokenize(validation_pandas['text'])\n",
        "\n",
        "# Detailed performance tracking\n",
        "detailed_performance = {}\n",
        "\n",
        "# Sentiment label creation function\n",
        "def create_sentiment_labels(df, category_col):\n",
        "    neg_col = f\"{category_col}_negative\"\n",
        "    neu_col = f\"{category_col}_neutral\"\n",
        "    pos_col = f\"{category_col}_positive\"\n",
        "\n",
        "    y = np.zeros(len(df))\n",
        "    y[df[neg_col] == 1] = 0\n",
        "    y[df[neu_col] == 1] = 1\n",
        "    y[df[pos_col] == 1] = 2\n",
        "    # Default to neutral if no sentiment is available\n",
        "    y[(df[neg_col] == 0) & (df[neu_col] == 0) & (df[pos_col] == 0)] = 1\n",
        "\n",
        "    return y\n",
        "\n",
        "# Prepare to collect performance metrics for all categories\n",
        "all_category_metrics = {\n",
        "    'Category': [],\n",
        "    'Model': [],\n",
        "    'Accuracy': [],\n",
        "    'Precision_Negative': [],\n",
        "    'Precision_Neutral': [],\n",
        "    'Precision_Positive': [],\n",
        "    'Recall_Negative': [],\n",
        "    'Recall_Neutral': [],\n",
        "    'Recall_Positive': [],\n",
        "    'F1_Negative': [],\n",
        "    'F1_Neutral': [],\n",
        "    'F1_Positive': []\n",
        "}\n",
        "\n",
        "# Model training and evaluation loop\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Prepare labels\n",
        "    y_train = create_sentiment_labels(train_pandas, clean_cat)\n",
        "    y_test = create_sentiment_labels(test_pandas, clean_cat)\n",
        "    y_val = create_sentiment_labels(validation_pandas, clean_cat)\n",
        "\n",
        "    # One-hot encoding for deep learning models\n",
        "    y_train_one_hot = tf.keras.utils.to_categorical(y_train, 3)\n",
        "    y_test_one_hot = tf.keras.utils.to_categorical(y_test, 3)\n",
        "    y_val_one_hot = tf.keras.utils.to_categorical(y_val, 3)\n",
        "\n",
        "    # 1. CNN Model\n",
        "    vocab_size = min(10000, len(keras_tokenizer.word_index) + 1)\n",
        "    embedding_dim = 100\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_length))\n",
        "    cnn_model.add(SpatialDropout1D(0.2))\n",
        "    cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnn_model.add(MaxPooling1D(5))\n",
        "    cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
        "    cnn_model.add(GlobalMaxPooling1D())\n",
        "    cnn_model.add(Dense(128, activation='relu'))\n",
        "    cnn_model.add(Dropout(0.2))\n",
        "    cnn_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # 2. Logistic Regression Pipeline\n",
        "    lr_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
        "        ('scaler', StandardScaler(with_mean=False)),\n",
        "        ('classifier', LogisticRegression(\n",
        "            multi_class='multinomial',\n",
        "            solver='lbfgs',\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced'\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # 3. RoBERTa Model\n",
        "    roberta_inputs = {\n",
        "        'input_ids': train_data_roberta['input_ids'],\n",
        "        'attention_mask': train_data_roberta['attention_mask']\n",
        "    }\n",
        "\n",
        "    roberta_base_model = roberta_model(roberta_inputs)[0]\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(roberta_base_model)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    roberta_output = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "    roberta_model_custom = tf.keras.Model(\n",
        "        inputs=[\n",
        "            roberta_inputs['input_ids'],\n",
        "            roberta_inputs['attention_mask']\n",
        "        ],\n",
        "        outputs=roberta_output\n",
        "    )\n",
        "\n",
        "    roberta_model_custom.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Early Stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "    # Training\n",
        "    # CNN Training\n",
        "    cnn_history = cnn_model.fit(\n",
        "        train_data_cnn, y_train_one_hot,\n",
        "        validation_data=(validation_data_cnn, y_val_one_hot),\n",
        "        epochs=5,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Logistic Regression Training\n",
        "    lr_pipeline.fit(train_pandas['text'], y_train)\n",
        "\n",
        "    # RoBERTa Training\n",
        "    roberta_history = roberta_model_custom.fit(\n",
        "        x=[\n",
        "            train_data_roberta['input_ids'],\n",
        "            train_data_roberta['attention_mask']\n",
        "        ],\n",
        "        y=y_train_one_hot,\n",
        "        validation_data=(\n",
        "            [validation_data_roberta['input_ids'], validation_data_roberta['attention_mask']],\n",
        "            y_val_one_hot\n",
        "        ),\n",
        "        epochs=5,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Predictions and Metrics\n",
        "    # CNN Predictions\n",
        "    y_pred_probs_cnn = cnn_model.predict(test_data_cnn)\n",
        "    y_pred_cnn = np.argmax(y_pred_probs_cnn, axis=1)\n",
        "    cnn_precision, cnn_recall, cnn_f1, _ = precision_recall_fscore_support(y_test, y_pred_cnn, average=None)\n",
        "    cnn_accuracy = accuracy_score(y_test, y_pred_cnn)\n",
        "\n",
        "    # Logistic Regression Predictions\n",
        "    y_pred_lr = lr_pipeline.predict(test_pandas['text'])\n",
        "    lr_precision, lr_recall, lr_f1, _ = precision_recall_fscore_support(y_test, y_pred_lr, average=None)\n",
        "    lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "\n",
        "    # RoBERTa Predictions\n",
        "    y_pred_probs_roberta = roberta_model_custom.predict([\n",
        "        test_data_roberta['input_ids'],\n",
        "        test_data_roberta['attention_mask']\n",
        "    ])\n",
        "    y_pred_roberta = np.argmax(y_pred_probs_roberta, axis=1)\n",
        "    roberta_precision, roberta_recall, roberta_f1, _ = precision_recall_fscore_support(y_test, y_pred_roberta, average=None)\n",
        "    roberta_accuracy = accuracy_score(y_test, y_pred_roberta)\n",
        "\n",
        "    # Collect metrics for plotting\n",
        "    models = ['CNN', 'Logistic Regression', 'RoBERTa']\n",
        "    accuracies = [cnn_accuracy, lr_accuracy, roberta_accuracy]\n",
        "\n",
        "    # Collect performance for the current category\n",
        "    for model, accuracy, precision, recall, f1 in zip(\n",
        "        models,\n",
        "        accuracies,\n",
        "        [cnn_precision, lr_precision, roberta_precision],\n",
        "        [cnn_recall, lr_recall, roberta_recall],\n",
        "        [cnn_f1, lr_f1, roberta_f1]\n",
        "    ):\n",
        "        all_category_metrics['Category'].append(category)\n",
        "        all_category_metrics['Model'].append(model)\n",
        "        all_category_metrics['Accuracy'].append(accuracy)\n",
        "\n",
        "        # Precision\n",
        "        all_category_metrics['Precision_Negative'].append(precision[0])\n",
        "        all_category_metrics['Precision_Neutral'].append(precision[1])\n",
        "        all_category_metrics['Precision_Positive'].append(precision[2])\n",
        "\n",
        "        # Recall\n",
        "        all_category_metrics['Recall_Negative'].append(recall[0])\n",
        "        all_category_metrics['Recall_Neutral'].append(recall[1])\n",
        "        all_category_metrics['Recall_Positive'].append(recall[2])\n",
        "\n",
        "        # F1\n",
        "        all_category_metrics['F1_Negative'].append(f1[0])\n",
        "        all_category_metrics['F1_Neutral'].append(f1[1])\n",
        "        all_category_metrics['F1_Positive'].append(f1[2])\n",
        "\n",
        "# Convert metrics to DataFrame\n",
        "metrics_df = pd.DataFrame(all_category_metrics)\n",
        "\n",
        "# Visualization of Performance Metrics\n",
        "def plot_performance_comparison(metrics_df):\n",
        "    # Set up the plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "    fig.suptitle('Model Performance Comparison Across Categories', fontsize=16)\n",
        "\n",
        "    # Accuracy Comparison\n",
        "    accuracy_plot = sns.barplot(x='Category', y='Accuracy', hue='Model', data=metrics_df, ax=axes[0, 0])\n",
        "    accuracy_plot.set_xticklabels(accuracy_plot.get_xticklabels(), rotation=45, ha='right')\n",
        "    axes[0, 0].set_title('Accuracy Comparison')\n",
        "\n",
        "    # Precision Comparison\n",
        "    precision_data = metrics_df.melt(\n",
        "        id_vars=['Category', 'Model'],\n",
        "        value_vars=['Precision_Negative', 'Precision_Neutral', 'Precision_Positive'],\n",
        "        var_name='Sentiment', value_name='Precision'\n",
        "    )\n",
        "    precision_plot = sns.barplot(\n",
        "        x='Category', y='Precision', hue='Model',\n",
        "        data=precision_data, ax=axes[0, 1]\n",
        "    )\n",
        "    precision_plot.set_xticklabels(precision_plot.get_xticklabels(), rotation=45, ha='right')\n",
        "    axes[0, 1].set_title('Precision Comparison')\n",
        "\n",
        "    # Recall Comparison\n",
        "    recall_data = metrics_df.melt(\n",
        "        id_vars=['Category', 'Model'],\n",
        "        value_vars=['Recall_Negative', 'Recall_Neutral', 'Recall_Positive'],\n",
        "        var_name='Sentiment', value_name='Recall'\n",
        "    )\n",
        "    recall_plot = sns.barplot(\n",
        "        x='Category', y='Recall', hue='Model',\n",
        "        data=recall_data, ax=axes[1, 0]\n",
        "    )\n",
        "    recall_plot.set_xticklabels(recall_plot.get_xticklabels(), rotation=45, ha='right')\n",
        "    axes[1, 0].set_title('Recall Comparison')\n",
        "\n",
        "    # F1 Score Comparison\n",
        "    f1_data = metrics_df.melt(\n",
        "        id_vars=['Category', 'Model'],\n",
        "        value_vars=['F1_Negative', 'F1_Neutral', 'F1_Positive'],\n",
        "        var_name='Sentiment', value_name='F1 Score'\n",
        "    )\n",
        "    f1_plot = sns.barplot(\n",
        "        x='Category', y='F1 Score', hue='Model',\n",
        "        data=f1_data, ax=axes[1, 1]\n",
        "    )\n",
        "    f1_plot.set_xticklabels(f1_plot.get_xticklabels(), rotation=45, ha='right')\n",
        "    axes[1, 1].set_title('F1 Score Comparison')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Generate performance comparison plot\n",
        "plot_performance_comparison(metrics_df)\n",
        "\n",
        "# Save metrics to CSV for further analysis\n",
        "metrics_df.to_csv('model_performance_metrics.csv', index=False)\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nOverall Performance Summary:\")\n",
        "print(metrics_df.groupby('Model')[['Accuracy', 'Precision_Negative', 'Precision_Neutral', 'Precision_Positive',\n",
        "                                    'Recall_Negative', 'Recall_Neutral', 'Recall_Positive',\n",
        "                                    'F1_Negative', 'F1_Neutral', 'F1_Positive']].mean())"
      ],
      "metadata": {
        "id": "cWHozCe4uin2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Wv6VCnIy0ZOLqkXQtwpfqnWCeolQisVd",
      "authorship_tag": "ABX9TyM95FVNYjIsU/qgYcUsBw1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}