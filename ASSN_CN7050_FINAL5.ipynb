{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmetorior/CN7030-/blob/main/ASSN_CN7050_FINAL5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydLyxVtJ6Kvu",
        "outputId": "dedc4871-45e9-4f4e-b030-c2a4160b0d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, pandas\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3 pyarrow-19.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark\n",
        "!pip install --upgrade pyspark pyarrow pandas\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6KnA174v0fU"
      },
      "outputs": [],
      "source": [
        "#d=[]\n",
        "#while(1):\n",
        "#  d.append('1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXoMU4N61zyF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "import pyarrow as pa\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array_contains\n",
        "from pyspark.sql.functions import col, explode, split, array, lit\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "import ast\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r44Ofrp_4Z6S"
      },
      "source": [
        "Check Files Exist and Are Readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8njwXTOmAR0C",
        "outputId": "2d802df5-ace0-4987-8a0d-740e37623f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "parquet_validation_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/validation-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_validation_file))\n",
        "\n",
        "parquet_test_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/test-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_test_file))\n",
        "\n",
        "parquet_train_file = \"/content/drive/MyDrive/Colab Notebooks/CN7050/train-00000-of-00001.parquet\"\n",
        "print(os.path.exists(parquet_train_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcjt7dWfOWWL"
      },
      "source": [
        "Load files into Spark. We have a train, test and validation file. Each one is loaded in from the parquet downloaded from huggingface.co"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaV1SHPcMhKj"
      },
      "source": [
        "SET UP A SPARK SESSION AND SET OPTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w14Dd0wJMc7U"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.Builder().master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
        "spark.conf.set(\"spark.sql.parquet.binaryAsString\", \"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTvMuJLvQLDG"
      },
      "outputs": [],
      "source": [
        "dfs_train = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_train_file)\n",
        "dfs_test = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_test_file)\n",
        "dfs_validation = spark.read.format(\"parquet\").option(\"mergeSchema\", \"false\").load(parquet_validation_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8VcUfUHOgry"
      },
      "source": [
        "Check the file contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u26PJdQibtkR",
        "outputId": "a2eca435-ea39-47dc-c3d3-f3b6057b202a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|        industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "|301972057|      600| Trustpilot|Price Comparison|My experience is ...|[[Staff support: ...|['staff-support.a...|\n",
            "|301982453|      514|Google Play|         Banking|I love it so hand...|[[Company brand: ...|['company-brand.g...|\n",
            "|301980653|      369|Google Play|    Ride Hailing|  Sometimes it takes|[[Company brand: ...|['company-brand.g...|\n",
            "|301979991|      727|Apple Store|         Fashion|This is the worst...|[[Logistics rides...|['logistics-rides...|\n",
            "|301984330|      549|Google Play|  Travel Booking|So easy & loads o...|[[Company brand: ...|['company-brand.g...|\n",
            "+---------+---------+-----------+----------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|    industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "|301982094|      514|Google Play|     Banking|Very useful and e...|[[Purchase bookin...|['purchase-bookin...|\n",
            "|301981085|      369|Google Play|Ride Hailing|easy to use.gud r...|[[Staff support: ...|['staff-support.a...|\n",
            "|301986508|      685|Google Play|     Trading|            money 😁|[[Company brand: ...|['company-brand.g...|\n",
            "|301981875|      514|Google Play|     Banking|      Great facility|[[Company brand: ...|['company-brand.g...|\n",
            "|301977341|      411|Apple Store|   Groceries|Love doing my ORG...|[[Purchase bookin...|['purchase-bookin...|\n",
            "+---------+---------+-----------+------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+---------+-----------+--------------+--------------------+--------------------+--------------------+\n",
            "|       id|org_index|data_source|      industry|                text|              labels|         label_codes|\n",
            "+---------+---------+-----------+--------------+--------------------+--------------------+--------------------+\n",
            "|610309432|     5827|Google Play|    Consulting|How do I retrieve...|[[Account managem...|['account-managem...|\n",
            "|301974039|      616| Trustpilot|       Fashion|Super fast delive...|[[Logistics rides...|['logistics-rides...|\n",
            "|301983653|      549|Google Play|Travel Booking|Great tool for ch...|[[Online experien...|['online-experien...|\n",
            "|301985771|      616|Google Play|       Fashion|Fast shipping but...|[[Logistics rides...|['logistics-rides...|\n",
            "|301980111|      727|Apple Store|       Fashion|Great app. Easy t...|[[Staff support: ...|['staff-support.a...|\n",
            "+---------+---------+-----------+--------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfs_train.show(5)\n",
        "dfs_test.show(5)\n",
        "dfs_validation.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5cc0tMjaYRk"
      },
      "source": [
        "File was not loading. Implemented a Workaround by converting the pandas version\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "of the parquet file converted to CSV and read into spark to create a spark dataframe. Maybe delete this segment later as problem was resolved.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w_7RGLkjact"
      },
      "source": [
        "*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_nMCgTX5Ivi",
        "outputId": "dacd7a04-4252-43a1-8a57-2c6cb03e4966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file row count: 7930\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n",
            "Testing file row count: 1587\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n",
            "Validation file row count: 1057\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- org_index: long (nullable = true)\n",
            " |-- data_source: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- labels: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            " |-- label_codes: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training file row count:\", dfs_train.count())\n",
        "dfs_train.printSchema()\n",
        "print(\"Testing file row count:\", dfs_test.count())\n",
        "dfs_test.printSchema()\n",
        "print(\"Validation file row count:\", dfs_validation.count())\n",
        "dfs_validation.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUmFm9EZ51ga"
      },
      "outputs": [],
      "source": [
        "#findspark.init()\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql.functions import col, sum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFVBX1FfdmFS",
        "outputId": "230e0f5f-ad19-4635-a210-7ceb2984125a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "| id|org_index|data_source|industry|text|labels|label_codes|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "|  0|        0|          0|       0|   0|     0|          0|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "| id|org_index|data_source|industry|text|labels|label_codes|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "|  0|        0|          0|       0|   0|     0|          0|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "| id|org_index|data_source|industry|text|labels|label_codes|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "|  0|        0|          0|       0|   0|     0|          0|\n",
            "+---+---------+-----------+--------+----+------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "null_counts_train = dfs_train.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_train.columns])\n",
        "null_counts_train.show()\n",
        "null_counts_test = dfs_test.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_test.columns])\n",
        "null_counts_test.show()\n",
        "null_counts_validation = dfs_validation.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in dfs_validation.columns])\n",
        "null_counts_validation.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80PqdfPiNZo2"
      },
      "outputs": [],
      "source": [
        "# ACD\n",
        "from pyspark.sql.functions import col, array_contains, lit, explode, udf\n",
        "from pyspark.ml.classification import LogisticRegression, LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, HashingTF, IDF, Tokenizer\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4nUni9wp_6"
      },
      "source": [
        "INSERT FULL ABSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYY4WCz2S_Nn"
      },
      "source": [
        "Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBb1JXzSwspM"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from numpy import array\n",
        "\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_sentiment_values(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        # Extract the part after the dot which represents the sentiment value\n",
        "        return [code.rsplit('.', 1)[1] for code in codes]\n",
        "    return []\n",
        "\n",
        "# We'll also need to extract the category names to know which sentiment belongs to which category\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories_and_sentiments(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        # Return tuples of (category, sentiment)\n",
        "        return [(code.rsplit('.', 1)[0], code.rsplit('.', 1)[1]) for code in codes]\n",
        "    return []\n",
        "\n",
        "# Create a UDF to check if this category has a specific sentiment\n",
        "@udf(returnType=\"integer\")\n",
        "def has_sentiment(category_sentiments, target_category, target_sentiment):\n",
        "    if category_sentiments is None:\n",
        "        return 0\n",
        "    for cat, sent in category_sentiments:\n",
        "        if cat == target_category and sent == target_sentiment:\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPzmz8qETBAg"
      },
      "source": [
        "Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4m5edSOqIyv"
      },
      "source": [
        "Set Up the Data extracting the categories and the sentiments from the Label_codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpJ8abVVqHDs"
      },
      "outputs": [],
      "source": [
        "# Step 1: Extract categories and their sentiment values\n",
        "dfs_train_with_sentiment = dfs_train.withColumn(\"category_sentiments\", extract_categories_and_sentiments(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_with_sentiment = dfs_test.withColumn(\"category_sentiments\", extract_categories_and_sentiments(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_with_sentiment = dfs_validation.withColumn(\"category_sentiments\", extract_categories_and_sentiments(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Step 2: Get all unique categories\n",
        "all_categories = dfs_train_with_sentiment.select(explode(\"category_sentiments\")).distinct().rdd.flatMap(lambda x: x[0]).collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WjRflNvbTCwd",
        "outputId": "87e1ac75-c5be-46bd-d889-071714cc82a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o53913.withColumn.\n: java.lang.StackOverflowError\n\tat java.base/java.util.Collections$UnmodifiableMap.get(Collections.java:1454)\n\tat java.base/java.lang.ProcessEnvironment.getenv(ProcessEnvironment.java:85)\n\tat java.base/java.lang.System.getenv(System.java:1016)\n\tat org.apache.spark.util.Utils$.isTesting(Utils.scala:1878)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.assertNotAnalysisRule(AnalysisHelper.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.assertNotAnalysisRule$(AnalysisHelper.scala:249)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.assertNotAnalysisRule(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:266)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1608)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dd1c6acdf784>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create columns for negative (-1), neutral (0), and positive (1) sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     dfs_train_with_sentiment = dfs_train_with_sentiment.withColumn(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;34mf\"{clean_cat}_negative\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mhas_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"category_sentiments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5174\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5175\u001b[0m             )\n\u001b[0;32m-> 5176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o53913.withColumn.\n: java.lang.StackOverflowError\n\tat java.base/java.util.Collections$UnmodifiableMap.get(Collections.java:1454)\n\tat java.base/java.lang.ProcessEnvironment.getenv(ProcessEnvironment.java:85)\n\tat java.base/java.lang.System.getenv(System.java:1016)\n\tat org.apache.spark.util.Utils$.isTesting(Utils.scala:1878)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.assertNotAnalysisRule(AnalysisHelper.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.assertNotAnalysisRule$(AnalysisHelper.scala:249)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.assertNotAnalysisRule(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:266)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1608)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:71)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 3: For each category, create columns for negative, neutral, and positive sentiment\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create columns for negative (-1), neutral (0), and positive (1) sentiment\n",
        "    dfs_train_with_sentiment = dfs_train_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_negative\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"-1\"))\n",
        "    )\n",
        "    dfs_train_with_sentiment = dfs_train_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_neutral\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"0\"))\n",
        "    )\n",
        "    dfs_train_with_sentiment = dfs_train_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_positive\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"1\"))\n",
        "    )\n",
        "\n",
        "    # Do the same for test and validation sets\n",
        "    dfs_test_with_sentiment = dfs_test_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_negative\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"-1\"))\n",
        "    )\n",
        "    dfs_test_with_sentiment = dfs_test_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_neutral\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"0\"))\n",
        "    )\n",
        "    dfs_test_with_sentiment = dfs_test_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_positive\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"1\"))\n",
        "    )\n",
        "\n",
        "    dfs_validation_with_sentiment = dfs_validation_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_negative\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"-1\"))\n",
        "    )\n",
        "    dfs_validation_with_sentiment = dfs_validation_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_neutral\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"0\"))\n",
        "    )\n",
        "    dfs_validation_with_sentiment = dfs_validation_with_sentiment.withColumn(\n",
        "        f\"{clean_cat}_positive\",\n",
        "        has_sentiment(col(\"category_sentiments\"), lit(category), lit(\"1\"))\n",
        "    )\n",
        "\n",
        "# Cache dataframes\n",
        "dfs_train_with_sentiment.cache()\n",
        "dfs_test_with_sentiment.cache()\n",
        "dfs_validation_with_sentiment.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9D36XN2TDk7"
      },
      "source": [
        " Part 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch4aQSeQrVgi"
      },
      "outputs": [],
      "source": [
        "# Step 4: Text vectorization with TF-IDF (same as before)\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\", minDocFreq=2)\n",
        "\n",
        "# Apply transformations\n",
        "train_tokenized = tokenizer.transform(dfs_train_with_sentiment)\n",
        "test_tokenized = tokenizer.transform(dfs_test_with_sentiment)\n",
        "\n",
        "train_hashed = hashingTF.transform(train_tokenized)\n",
        "test_hashed = hashingTF.transform(test_tokenized)\n",
        "idf_model = idf.fit(train_hashed)\n",
        "\n",
        "train_tfidf = idf_model.transform(train_hashed)\n",
        "test_tfidf = idf_model.transform(test_hashed)\n",
        "\n",
        "# Step 5: Train sentiment classifiers for each category\n",
        "from pyspark.ml.classification import LinearSVC, LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "sentiment_results = {}\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create a combined label with 3 classes: -1, 0, 1\n",
        "    # We'll use a UDF to convert the 3 binary columns into a single multiclass label\n",
        "    @udf(returnType=\"double\")\n",
        "    def combine_sentiment(negative, neutral, positive):\n",
        "        if negative == 1:\n",
        "            return 0.0  # Class 0 = negative\n",
        "        elif neutral == 1:\n",
        "            return 1.0  # Class 1 = neutral\n",
        "        elif positive == 1:\n",
        "            return 2.0  # Class 2 = positive\n",
        "        else:\n",
        "            return 1.0  # Default to neutral if no sentiment is available\n",
        "\n",
        "    # Create multiclass label column\n",
        "    train_data = train_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    test_data = test_tfidf.withColumn(\n",
        "        \"sentiment_label\",\n",
        "        combine_sentiment(\n",
        "            col(f\"{clean_cat}_negative\"),\n",
        "            col(f\"{clean_cat}_neutral\"),\n",
        "            col(f\"{clean_cat}_positive\")\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Use TF-IDF features for prediction\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data).select(\"features\", \"sentiment_label\")\n",
        "    test_assembled = assembler.transform(test_data).select(\"features\", \"sentiment_label\")\n",
        "\n",
        "    # Train multiclass classifier - LogisticRegression for 3 classes\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_label\", maxIter=20)\n",
        "    lr_model = lr.fit(train_assembled)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = lr_model.transform(test_assembled)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1 = evaluator.evaluate(predictions)\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "\n",
        "    sentiment_results[category] = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"accuracy\": accuracy\n",
        "    }\n",
        "\n",
        "    print(f\"Category: {category}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6sdgS8osCPo"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm4s4n2ir_Js"
      },
      "source": [
        "Deep learning model for sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvbWDYKYr-Iv"
      },
      "outputs": [],
      "source": [
        "# Step 6: Prepare data for deep learning models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Convert to pandas for easier processing\n",
        "train_pandas = dfs_train_with_sentiment.toPandas()\n",
        "test_pandas = dfs_test_with_sentiment.toPandas()\n",
        "validation_pandas = dfs_validation_with_sentiment.toPandas()\n",
        "\n",
        "# Tokenize text\n",
        "keras_tokenizer = KerasTokenizer(num_words=10000)\n",
        "keras_tokenizer.fit_on_texts(train_pandas['text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sequences = keras_tokenizer.texts_to_sequences(train_pandas['text'])\n",
        "test_sequences = keras_tokenizer.texts_to_sequences(test_pandas['text'])\n",
        "validation_sequences = keras_tokenizer.texts_to_sequences(validation_pandas['text'])\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_length = 100\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "validation_data = pad_sequences(validation_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# Deep learning model results\n",
        "dl_sentiment_results = {}\n",
        "\n",
        "# For each category, train a deep learning model\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "\n",
        "    # Create y labels\n",
        "    # 0: negative, 1: neutral, 2: positive\n",
        "    def create_sentiment_labels(df, category_col):\n",
        "        neg_col = f\"{category_col}_negative\"\n",
        "        neu_col = f\"{category_col}_neutral\"\n",
        "        pos_col = f\"{category_col}_positive\"\n",
        "\n",
        "        y = np.zeros(len(df))\n",
        "        y[df[neg_col] == 1] = 0\n",
        "        y[df[neu_col] == 1] = 1\n",
        "        y[df[pos_col] == 1] = 2\n",
        "        # Default to neutral if no sentiment is available\n",
        "        y[(df[neg_col] == 0) & (df[neu_col] == 0) & (df[pos_col] == 0)] = 1\n",
        "\n",
        "        return y\n",
        "\n",
        "    y_train = create_sentiment_labels(train_pandas, clean_cat)\n",
        "    y_test = create_sentiment_labels(test_pandas, clean_cat)\n",
        "    y_val = create_sentiment_labels(validation_pandas, clean_cat)\n",
        "\n",
        "    # Convert to one-hot encoding\n",
        "    y_train_one_hot = tf.keras.utils.to_categorical(y_train, 3)\n",
        "    y_test_one_hot = tf.keras.utils.to_categorical(y_test, 3)\n",
        "    y_val_one_hot = tf.keras.utils.to_categorical(y_val, 3)\n",
        "\n",
        "    # Build CNN model\n",
        "    vocab_size = min(10000, len(keras_tokenizer.word_index) + 1)\n",
        "    embedding_dim = 100\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_seq_length))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(5))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(3, activation='softmax'))  # 3 classes: negative, neutral, positive\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train with early stopping\n",
        "    from tensorflow.keras.callbacks import EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data, y_train_one_hot,\n",
        "        validation_data=(validation_data, y_val_one_hot),\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_probs = model.predict(test_data)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    dl_sentiment_results[category] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_neg\": report['0']['f1-score'] if '0' in report else 0,\n",
        "        \"f1_neu\": report['1']['f1-score'] if '1' in report else 0,\n",
        "        \"f1_pos\": report['2']['f1-score'] if '2' in report else 0,\n",
        "        \"f1_weighted\": report['weighted avg']['f1-score']\n",
        "    }\n",
        "\n",
        "    print(f\"Category: {category}, Accuracy: {accuracy:.4f}, F1 (weighted): {report['weighted avg']['f1-score']:.4f}\")\n",
        "    print(f\"F1 Scores - Negative: {report['0']['f1-score'] if '0' in report else 0:.4f}, \" +\n",
        "          f\"Neutral: {report['1']['f1-score'] if '1' in report else 0:.4f}, \" +\n",
        "          f\"Positive: {report['2']['f1-score'] if '2' in report else 0:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "    plt.title(f'Confusion Matrix - {category}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_{clean_cat}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title(f'Model Accuracy - {category}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5Hd0lBhtgMn"
      },
      "outputs": [],
      "source": [
        "Further charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9RtqvzDtjB_"
      },
      "outputs": [],
      "source": [
        "# Step 8: Perform cross-category sentiment analysis\n",
        "def analyze_cross_category_sentiment_correlation(df, all_categories):\n",
        "    \"\"\"Analyze correlation between sentiment predictions across different categories\"\"\"\n",
        "    # Create a dataframe with sentiment scores for each category\n",
        "    sentiment_scores = {}\n",
        "\n",
        "    for category in all_categories:\n",
        "        clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "        neg_col = f\"{clean_cat}_negative\"\n",
        "        neu_col = f\"{clean_cat}_neutral\"\n",
        "        pos_col = f\"{clean_cat}_positive\"\n",
        "\n",
        "        # If the columns exist, calculate a simple sentiment score\n",
        "        if neg_col in df.columns and neu_col in df.columns and pos_col in df.columns:\n",
        "            # Convert to numeric sentiment score: -1 for negative, 0 for neutral, 1 for positive\n",
        "            df[f\"{clean_cat}_score\"] = df[pos_col].astype(int) - df[neg_col].astype(int)\n",
        "            sentiment_scores[category] = f\"{clean_cat}_score\"\n",
        "\n",
        "    # Create correlation matrix\n",
        "    score_columns = list(sentiment_scores.values())\n",
        "    if len(score_columns) > 1:\n",
        "        correlation_matrix = df[score_columns].corr()\n",
        "\n",
        "        # Plot heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
        "                    mask=mask, vmin=-1, vmax=1, center=0,\n",
        "                    xticklabels=sentiment_scores.keys(),\n",
        "                    yticklabels=sentiment_scores.keys())\n",
        "        plt.title('Cross-Category Sentiment Correlation', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('sentiment_correlation_heatmap.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        return correlation_matrix\n",
        "    return None\n",
        "\n",
        "# Analyze cross-category correlations\n",
        "correlation_matrix = analyze_cross_category_sentiment_correlation(train_pandas, all_categories)\n",
        "\n",
        "# Step 9: Error analysis\n",
        "def perform_error_analysis(model, tokenizer, texts, true_labels, category, label_names=['Negative', 'Neutral', 'Positive']):\n",
        "    \"\"\"Perform error analysis on incorrect predictions\"\"\"\n",
        "    # Make predictions\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    predictions = model.predict(padded_sequences)\n",
        "    pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Find incorrect predictions\n",
        "    incorrect_indices = np.where(pred_labels != true_labels)[0]\n",
        "\n",
        "    if len(incorrect_indices) == 0:\n",
        "        print(f\"No incorrect predictions found for {category}\")\n",
        "        return\n",
        "\n",
        "    # Sample up to 10 incorrect predictions\n",
        "    sample_size = min(10, len(incorrect_indices))\n",
        "    sample_indices = np.random.choice(incorrect_indices, size=sample_size, replace=False)\n",
        "\n",
        "    # Create a dataframe for analysis\n",
        "    error_df = pd.DataFrame({\n",
        "        'Text': [texts[i] for i in sample_indices],\n",
        "        'True Label': [label_names[true_labels[i]] for i in sample_indices],\n",
        "        'Predicted Label': [label_names[pred_labels[i]] for i in sample_indices],\n",
        "        'Confidence': [np.max(predictions[i]) for i in sample_indices]\n",
        "    })\n",
        "\n",
        "    # Create and plot confusion matrix\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_names, yticklabels=label_names)\n",
        "    plt.title(f'Confusion Matrix for {category}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confusion_matrix_{category}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Print error analysis\n",
        "    print(f\"\\n--- Error Analysis for {category} ---\")\n",
        "    print(f\"Total samples: {len(texts)}\")\n",
        "    print(f\"Incorrect predictions: {len(incorrect_indices)} ({len(incorrect_indices)/len(texts)*100:.2f}%)\")\n",
        "    print(\"\\nSample of incorrect predictions:\")\n",
        "    print(error_df)\n",
        "\n",
        "    return error_df\n",
        "\n",
        "# Perform error analysis for a few selected categories\n",
        "selected_categories = all_categories[:3]  # Choose first 3 categories for analysis\n",
        "for category in selected_categories:\n",
        "    # Get test data for this category\n",
        "    # Note: You would need to extract test data for each category\n",
        "    # This is a placeholder - adapt to your data structure\n",
        "    test_texts = test_texts_by_category.get(category, [])\n",
        "    test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "    if len(test_texts) > 0:\n",
        "        perform_error_analysis(model, keras_tokenizer, test_texts, test_labels, category)\n",
        "\n",
        "# Step 10: Analyze prediction confidence\n",
        "def analyze_prediction_confidence(model, tokenizer, texts, true_labels, category):\n",
        "    \"\"\"Analyze prediction confidence and its relationship with accuracy\"\"\"\n",
        "    # Make predictions\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    predictions = model.predict(padded_sequences)\n",
        "    pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Get confidence of predictions (max probability)\n",
        "    confidences = np.max(predictions, axis=1)\n",
        "\n",
        "    # Create a dataframe\n",
        "    confidence_df = pd.DataFrame({\n",
        "        'Confidence': confidences,\n",
        "        'Correct': pred_labels == true_labels\n",
        "    })\n",
        "\n",
        "    # Plot confidence distribution\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Overall confidence distribution\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(confidences, bins=20, kde=True)\n",
        "    plt.title(f'Prediction Confidence Distribution - {category}')\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Confidence by correctness\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.boxplot(x='Correct', y='Confidence', data=confidence_df)\n",
        "    plt.title('Confidence by Prediction Correctness')\n",
        "    plt.xlabel('Prediction Correct')\n",
        "    plt.ylabel('Confidence')\n",
        "\n",
        "    # Accuracy by confidence bucket\n",
        "    plt.subplot(2, 2, 3)\n",
        "    confidence_df['ConfidenceBucket'] = pd.cut(confidence_df['Confidence'], bins=10)\n",
        "    accuracy_by_bucket = confidence_df.groupby('ConfidenceBucket')['Correct'].mean()\n",
        "\n",
        "    sns.barplot(x=accuracy_by_bucket.index.astype(str), y=accuracy_by_bucket.values)\n",
        "    plt.title('Accuracy by Confidence Range')\n",
        "    plt.xlabel('Confidence Range')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Count by confidence bucket\n",
        "    plt.subplot(2, 2, 4)\n",
        "    count_by_bucket = confidence_df.groupby('ConfidenceBucket').size()\n",
        "    sns.barplot(x=count_by_bucket.index.astype(str), y=count_by_bucket.values)\n",
        "    plt.title('Sample Count by Confidence Range')\n",
        "    plt.xlabel('Confidence Range')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'confidence_analysis_{category}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    return confidence_df\n",
        "\n",
        "# Analyze prediction confidence for selected categories\n",
        "for category in selected_categories:\n",
        "    test_texts = test_texts_by_category.get(category, [])\n",
        "    test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "    if len(test_texts) > 0:\n",
        "        analyze_prediction_confidence(model, keras_tokenizer, test_texts, test_labels, category)\n",
        "\n",
        "# Step 11: Text length vs accuracy analysis\n",
        "def analyze_text_length_vs_accuracy(model, tokenizer, texts, true_labels, category):\n",
        "    \"\"\"Analyze the relationship between text length and prediction accuracy\"\"\"\n",
        "    # Make predictions\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    predictions = model.predict(padded_sequences)\n",
        "    pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate text lengths\n",
        "    text_lengths = [len(text.split()) for text in texts]\n",
        "\n",
        "    # Create dataframe\n",
        "    length_df = pd.DataFrame({\n",
        "        'TextLength': text_lengths,\n",
        "        'Correct': pred_labels == true_labels\n",
        "    })\n",
        "\n",
        "    # Create length buckets\n",
        "    length_df['LengthBucket'] = pd.cut(length_df['TextLength'], bins=10)\n",
        "\n",
        "    # Calculate accuracy by length bucket\n",
        "    accuracy_by_length = length_df.groupby('LengthBucket')['Correct'].mean()\n",
        "    count_by_length = length_df.groupby('LengthBucket').size()\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Text length distribution\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(text_lengths, bins=20, kde=True)\n",
        "    plt.title(f'Text Length Distribution - {category}')\n",
        "    plt.xlabel('Text Length (words)')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Text length vs accuracy (scatter)\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.scatterplot(x='TextLength', y='Correct', data=length_df, alpha=0.3)\n",
        "    plt.title('Text Length vs Prediction Correctness')\n",
        "    plt.xlabel('Text Length (words)')\n",
        "    plt.ylabel('Prediction Correct')\n",
        "\n",
        "    # Accuracy by length bucket\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.barplot(x=accuracy_by_length.index.astype(str), y=accuracy_by_length.values)\n",
        "    plt.title('Accuracy by Text Length Range')\n",
        "    plt.xlabel('Text Length Range')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Count by length bucket\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.barplot(x=count_by_length.index.astype(str), y=count_by_length.values)\n",
        "    plt.title('Sample Count by Text Length Range')\n",
        "    plt.xlabel('Text Length Range')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'length_analysis_{category}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    return length_df\n",
        "\n",
        "# Analyze text length vs accuracy for selected categories\n",
        "for category in selected_categories:\n",
        "    test_texts = test_texts_by_category.get(category, [])\n",
        "    test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "    if len(test_texts) > 0:\n",
        "        analyze_text_length_vs_accuracy(model, keras_tokenizer, test_texts, test_labels, category)\n",
        "\n",
        "# Step 12: Create interactive dashboard (optional - if using in a Jupyter notebook)\n",
        "# Note: This requires ipywidgets and is best used in a Jupyter environment\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "\n",
        "    # Create category selector\n",
        "    category_dropdown = widgets.Dropdown(\n",
        "        options=all_categories,\n",
        "        description='Category:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create visualization type selector\n",
        "    viz_dropdown = widgets.Dropdown(\n",
        "        options=['Performance Metrics', 'Confusion Matrix', 'Important Words', 'Error Examples'],\n",
        "        description='Visualization:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Function to update visualization based on selections\n",
        "    def update_visualization(category, viz_type):\n",
        "        if viz_type == 'Performance Metrics':\n",
        "            # Display performance metrics for selected category\n",
        "            if category in dl_sentiment_results:\n",
        "                metrics = dl_sentiment_results[category]\n",
        "                print(f\"Performance metrics for {category}:\")\n",
        "                for metric, value in metrics.items():\n",
        "                    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "        elif viz_type == 'Confusion Matrix':\n",
        "            # Display confusion matrix for selected category\n",
        "            test_texts = test_texts_by_category.get(category, [])\n",
        "            test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "            if len(test_texts) > 0:\n",
        "                # Make predictions\n",
        "                sequences = keras_tokenizer.texts_to_sequences(test_texts)\n",
        "                padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "                predictions = model.predict(padded_sequences)\n",
        "                pred_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "                # Create and plot confusion matrix\n",
        "                cm = confusion_matrix(test_labels, pred_labels)\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "                            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "                plt.title(f'Confusion Matrix for {category}')\n",
        "                plt.xlabel('Predicted Label')\n",
        "                plt.ylabel('True Label')\n",
        "                plt.show()\n",
        "\n",
        "        elif viz_type == 'Important Words':\n",
        "            # Display important words for each sentiment\n",
        "            for sentiment_class in range(3):\n",
        "                visualize_important_words_for_sentiment(model, keras_tokenizer, category, sentiment_class)\n",
        "\n",
        "        elif viz_type == 'Error Examples':\n",
        "            # Display error examples\n",
        "            test_texts = test_texts_by_category.get(category, [])\n",
        "            test_labels = test_labels_by_category.get(category, [])\n",
        "\n",
        "            if len(test_texts) > 0:\n",
        "                perform_error_analysis(model, keras_tokenizer, test_texts, test_labels, category)\n",
        "\n",
        "    # Create interactive output\n",
        "    interactive_output = widgets.interactive_output(\n",
        "        update_visualization,\n",
        "        {'category': category_dropdown, 'viz_type': viz_dropdown}\n",
        "    )\n",
        "\n",
        "    # Display widgets\n",
        "    display(widgets.HBox([category_dropdown, viz_dropdown]))\n",
        "    display(interactive_output)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Interactive visualizations require ipywidgets. Install with: pip install ipywidgets\")\n",
        "\n",
        "# Step 13: Save model results summary to file\n",
        "def save_results_summary(ml_results, dl_results, filename='sentiment_analysis_results.csv'):\n",
        "    \"\"\"Save a summary of results to CSV file\"\"\"\n",
        "    results_data = []\n",
        "\n",
        "    # Combine results from both models\n",
        "    for category in all_categories:\n",
        "        row = {'Category': category}\n",
        "\n",
        "        # Add ML results if available\n",
        "        if category in ml_results:\n",
        "            for metric, value in ml_results[category].items():\n",
        "                row[f'ML_{metric}'] = value\n",
        "\n",
        "        # Add DL results if available\n",
        "        if category in dl_results:\n",
        "            for metric, value in dl_results[category].items():\n",
        "                row[f'DL_{metric}'] = value\n",
        "\n",
        "        results_data.append(row)\n",
        "\n",
        "    # Convert to DataFrame and save\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "    results_df.to_csv(filename, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mBD9mxhtmM9"
      },
      "source": [
        "END OF ACSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA73hKfbtqwN"
      },
      "source": [
        "ACD ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGh4QjI9tqAH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrrO2uTQdxko"
      },
      "source": [
        "INSERT 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l0FCDzLdw1Z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, array_contains, explode, udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.ml.feature import VectorAssembler, HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Extract and clean categories (same as before)\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [code.rsplit('.', 1)[0] for code in codes]\n",
        "    return []\n",
        "\n",
        "# Apply the UDF to extract categories\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Step 2: Get all unique categories with consistent naming\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Step 3: Create binary indicator columns (same as before)\n",
        "for category in all_categories:\n",
        "    # Use the original category name in the column name to avoid confusion\n",
        "    col_name = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames (same as before)\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "\n",
        "# Step 4: TF-IDF Features for traditional ML model (SVM)\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\", minDocFreq=2)\n",
        "\n",
        "# Apply transformations\n",
        "train_tokenized = tokenizer.transform(dfs_train_clean)\n",
        "test_tokenized = tokenizer.transform(dfs_test_clean)\n",
        "train_hashed = hashingTF.transform(train_tokenized)\n",
        "test_hashed = hashingTF.transform(test_tokenized)\n",
        "idf_model = idf.fit(train_hashed)\n",
        "train_tfidf = idf_model.transform(train_hashed)\n",
        "test_tfidf = idf_model.transform(test_hashed)\n",
        "\n",
        "# Step 5: Prepare data for deep learning models (CNN and LSTM)\n",
        "# Convert to pandas for easier processing\n",
        "train_pandas = dfs_train_clean.toPandas()\n",
        "test_pandas = dfs_test_clean.toPandas()\n",
        "validation_pandas = dfs_validation_clean.toPandas()\n",
        "\n",
        "# Prepare text data for deep learning models\n",
        "keras_tokenizer = KerasTokenizer(num_words=10000)\n",
        "keras_tokenizer.fit_on_texts(train_pandas['text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sequences = keras_tokenizer.texts_to_sequences(train_pandas['text'])\n",
        "test_sequences = keras_tokenizer.texts_to_sequences(test_pandas['text'])\n",
        "validation_sequences = keras_tokenizer.texts_to_sequences(validation_pandas['text'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_seq_length = 100\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_seq_length)\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_seq_length)\n",
        "validation_data = pad_sequences(validation_sequences, maxlen=max_seq_length)\n",
        "\n",
        "# Prepare multi-label targets for deep learning\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "train_labels = train_pandas[feature_cols].values\n",
        "test_labels = test_pandas[feature_cols].values\n",
        "validation_labels = validation_pandas[feature_cols].values\n",
        "\n",
        "# Step 6: Build and train different models\n",
        "\n",
        "# 6.1: SVM Model for each category (similar to previous code)\n",
        "svm_results = {}\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    col_name = f\"has_{clean_cat}\"\n",
        "\n",
        "    # Create label column for this specific category\n",
        "    train_data_svm = train_tfidf.withColumn(\"label\", col(col_name))\n",
        "    test_data_svm = test_tfidf.withColumn(\"label\", col(col_name))\n",
        "\n",
        "    # Use only TF-IDF features for prediction\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data_svm).select(\"features\", \"label\")\n",
        "    test_assembled = assembler.transform(test_data_svm).select(\"features\", \"label\")\n",
        "\n",
        "    # Train binary classifier\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=20, regParam=0.1)\n",
        "    svm_model = svm.fit(train_assembled)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = svm_model.transform(test_assembled)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1 = evaluator.evaluate(predictions)\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "    svm_results[category] = {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "    print(f\"SVM - Category: {category}, F1: {f1}, Precision: {precision}, Recall: {recall}\")\n",
        "\n",
        "# 6.2: CNN Model for multi-label classification\n",
        "def build_cnn_model(vocab_size, embedding_dim, max_length, num_categories):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(5))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_categories, activation='sigmoid'))  # Sigmoid for multi-label\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and train CNN model\n",
        "vocab_size = min(10000, len(keras_tokenizer.word_index) + 1)\n",
        "embedding_dim = 100\n",
        "num_categories = len(all_categories)\n",
        "\n",
        "cnn_model = build_cnn_model(vocab_size, embedding_dim, max_seq_length, num_categories)\n",
        "print(cnn_model.summary())\n",
        "\n",
        "# Train with early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history_cnn = cnn_model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(validation_data, validation_labels),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate CNN model\n",
        "cnn_predictions = cnn_model.predict(test_data)\n",
        "cnn_predictions_binary = (cnn_predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics for CNN\n",
        "cnn_results = {}\n",
        "for i, category in enumerate(all_categories):\n",
        "    f1 = f1_score(test_labels[:, i], cnn_predictions_binary[:, i])\n",
        "    report = classification_report(test_labels[:, i], cnn_predictions_binary[:, i], output_dict=True)\n",
        "    cnn_results[category] = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": report['1']['precision'] if 1 in report else 0,\n",
        "        \"recall\": report['1']['recall'] if 1 in report else 0\n",
        "    }\n",
        "    print(f\"CNN - Category: {category}, F1: {f1}\")\n",
        "\n",
        "# 6.3: LSTM Model for multi-label classification\n",
        "def build_lstm_model(vocab_size, embedding_dim, max_length, num_categories):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_categories, activation='sigmoid'))  # Sigmoid for multi-label\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and train LSTM model\n",
        "lstm_model = build_lstm_model(vocab_size, embedding_dim, max_seq_length, num_categories)\n",
        "print(lstm_model.summary())\n",
        "\n",
        "# Train with early stopping\n",
        "history_lstm = lstm_model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(validation_data, validation_labels),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate LSTM model\n",
        "lstm_predictions = lstm_model.predict(test_data)\n",
        "lstm_predictions_binary = (lstm_predictions > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics for LSTM\n",
        "lstm_results = {}\n",
        "for i, category in enumerate(all_categories):\n",
        "    f1 = f1_score(test_labels[:, i], lstm_predictions_binary[:, i])\n",
        "    report = classification_report(test_labels[:, i], lstm_predictions_binary[:, i], output_dict=True)\n",
        "    lstm_results[category] = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": report['1']['precision'] if 1 in report else 0,\n",
        "        \"recall\": report['1']['recall'] if 1 in report else 0\n",
        "    }\n",
        "    print(f\"LSTM - Category: {category}, F1: {f1}\")\n",
        "\n",
        "# Step 7: Visualize and compare model performance\n",
        "def plot_model_comparison(svm_results, cnn_results, lstm_results, all_categories):\n",
        "    # Prepare data\n",
        "    categories = [cat.replace('-', '_').replace('.', '_') for cat in all_categories]\n",
        "    svm_f1 = [svm_results[cat]['f1'] for cat in all_categories]\n",
        "    cnn_f1 = [cnn_results[cat]['f1'] for cat in all_categories]\n",
        "    lstm_f1 = [lstm_results[cat]['f1'] for cat in all_categories]\n",
        "\n",
        "    # Sort by average F1 score\n",
        "    avg_f1 = [(svm_f1[i] + cnn_f1[i] + lstm_f1[i])/3 for i in range(len(all_categories))]\n",
        "    indices = np.argsort(avg_f1)[::-1]\n",
        "\n",
        "    categories = [categories[i] for i in indices]\n",
        "    svm_f1 = [svm_f1[i] for i in indices]\n",
        "    cnn_f1 = [cnn_f1[i] for i in indices]\n",
        "    lstm_f1 = [lstm_f1[i] for i in indices]\n",
        "\n",
        "    # Plot\n",
        "    x = np.arange(len(categories))\n",
        "    width = 0.25\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    ax.bar(x - width, svm_f1, width, label='SVM', color='#4e79a7')\n",
        "    ax.bar(x, cnn_f1, width, label='CNN', color='#f28e2b')\n",
        "    ax.bar(x + width, lstm_f1, width, label='LSTM', color='#59a14f')\n",
        "\n",
        "    ax.set_xlabel('Categories', fontsize=12)\n",
        "    ax.set_ylabel('F1 Score', fontsize=12)\n",
        "    ax.set_title('Model Performance Comparison by Category', fontsize=16)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(categories, rotation=60, ha='right')\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Plot training history for deep learning models\n",
        "def plot_training_history(history_cnn, history_lstm):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # CNN training history\n",
        "    ax1.plot(history_cnn.history['accuracy'], label='Train Accuracy')\n",
        "    ax1.plot(history_cnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_title('CNN Model Accuracy')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend()\n",
        "    ax1.grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "    # LSTM training history\n",
        "    ax2.plot(history_lstm.history['accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(history_lstm.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax2.set_title('LSTM Model Accuracy')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend()\n",
        "    ax2.grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Execute visualization functions\n",
        "plot_model_comparison(svm_results, cnn_results, lstm_results, all_categories)\n",
        "plot_training_history(history_cnn, history_lstm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ex5spSTw7NO"
      },
      "source": [
        "INSERT ABSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEj4vthxvUox"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH7S22OYw6N0"
      },
      "outputs": [],
      "source": [
        "# Step 1: Extract and clean categories - use a consistent naming convention\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [code.rsplit('.', 1)[0] for code in codes]\n",
        "    return []\n",
        "\n",
        "# Apply the UDF to extract categories\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Step 2: Get all unique categories with consistent naming\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Step 3: Create binary indicator columns with consistent naming\n",
        "for category in all_categories:\n",
        "    # Use the original category name in the column name to avoid confusion\n",
        "    col_name = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "dfs_train_clean.show(10)\n",
        "\n",
        "# Step 4: Text vectorization with TF-IDF\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\", minDocFreq=2)\n",
        "\n",
        "# Apply transformations\n",
        "train_tokenized = tokenizer.transform(dfs_train_clean)\n",
        "test_tokenized = tokenizer.transform(dfs_test_clean)\n",
        "train_hashed = hashingTF.transform(train_tokenized)\n",
        "test_hashed = hashingTF.transform(test_tokenized)\n",
        "idf_model = idf.fit(train_hashed)\n",
        "train_tfidf = idf_model.transform(train_hashed)\n",
        "test_tfidf = idf_model.transform(test_hashed)\n",
        "\n",
        "# Step 5: Select the correct feature columns based on the actual column names\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Step 6: Build a multi-label classifier for each category\n",
        "# For each category, train a separate binary classifier\n",
        "results = {}\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    col_name = f\"has_{clean_cat}\"\n",
        "\n",
        "    # Create label column for this specific category\n",
        "    train_data = train_tfidf.withColumn(\"label\", col(col_name))\n",
        "    test_data = test_tfidf.withColumn(\"label\", col(col_name))\n",
        "\n",
        "    # Use only TF-IDF features for prediction\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data).select(\"features\", \"label\")\n",
        "    test_assembled = assembler.transform(test_data).select(\"features\", \"label\")\n",
        "\n",
        "    # Train binary classifier\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=20, regParam=0.1)\n",
        "    svm_model = svm.fit(train_assembled)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = svm_model.transform(test_assembled)\n",
        "\n",
        "    # Evaluate\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1 = evaluator.evaluate(predictions)\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "    results[category] = {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "    print(f\"Category: {category}, F1: {f1}, Precision: {precision}, Recall: {recall}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u814LsIoy0U9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'results' is the dictionary containing performance metrics\n",
        "categories = list(results.keys())\n",
        "f1_scores = [results[cat]['f1'] for cat in categories]\n",
        "precision_scores = [results[cat]['precision'] for cat in categories]\n",
        "recall_scores = [results[cat]['recall'] for cat in categories]\n",
        "\n",
        "# Create a DataFrame for easy plotting\n",
        "df = pd.DataFrame({\n",
        "    'Category': categories,\n",
        "    'F1 Score': f1_scores,\n",
        "    'Precision': precision_scores,\n",
        "    'Recall': recall_scores\n",
        "})\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "x = np.arange(len(categories))\n",
        "width = 0.25\n",
        "\n",
        "plt.bar(x - width, f1_scores, width, label='F1 Score')\n",
        "plt.bar(x, precision_scores, width, label='Precision')\n",
        "plt.bar(x + width, recall_scores, width, label='Recall')\n",
        "\n",
        "plt.xlabel('Categories')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Performance Metrics by Category')\n",
        "plt.xticks(x, categories, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('category_performance.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbORQisu09w9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Assuming 'results' is your dictionary of performance metrics\n",
        "# and 'all_categories' contains your category names\n",
        "\n",
        "# 1. Performance Metrics by Category\n",
        "def plot_category_performance(results, all_categories):\n",
        "    # Prepare data\n",
        "    categories = [cat.replace('-', '_').replace('.', '_') for cat in all_categories]\n",
        "    metrics = pd.DataFrame({\n",
        "        'Category': categories,\n",
        "        'F1 Score': [results[cat]['f1'] for cat in all_categories],\n",
        "        'Precision': [results[cat]['precision'] for cat in all_categories],\n",
        "        'Recall': [results[cat]['recall'] for cat in all_categories]\n",
        "    })\n",
        "\n",
        "    # Sort by F1 score\n",
        "    metrics = metrics.sort_values('F1 Score', ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.25\n",
        "\n",
        "    plt.bar(x - width, metrics['F1 Score'], width, label='F1 Score', color='#4e79a7')\n",
        "    plt.bar(x, metrics['Precision'], width, label='Precision', color='#f28e2b')\n",
        "    plt.bar(x + width, metrics['Recall'], width, label='Recall', color='#59a14f')\n",
        "\n",
        "    plt.xlabel('Categories', fontsize=12)\n",
        "    plt.ylabel('Score', fontsize=12)\n",
        "    plt.title('Performance Metrics by Category', fontsize=16)\n",
        "    plt.xticks(x, metrics['Category'], rotation=60, ha='right')\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('category_performance.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 2. Category Distribution Heatmap\n",
        "def plot_category_distribution(df_train):\n",
        "    # Get category counts\n",
        "    category_counts = {}\n",
        "    for category in all_categories:\n",
        "        col_name = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "        category_counts[category] = df_train.filter(col(col_name) == 1).count()\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    counts_df = pd.DataFrame(list(category_counts.items()), columns=['Category', 'Count'])\n",
        "    counts_df = counts_df.sort_values('Count', ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Count', y='Category', data=counts_df, palette='viridis')\n",
        "    plt.title('Distribution of Categories in Training Data', fontsize=16)\n",
        "    plt.xlabel('Number of Instances', fontsize=12)\n",
        "    plt.ylabel('Category', fontsize=12)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('category_distribution.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 3. Co-occurrence Matrix Heatmap\n",
        "def plot_category_cooccurrence(df_train):\n",
        "    # Convert to pandas for easier processing\n",
        "    feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "    df_pandas = df_train.select(feature_cols).toPandas()\n",
        "\n",
        "    # Calculate co-occurrence\n",
        "    cooccurrence = df_pandas.T.dot(df_pandas)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    mask = np.triu(np.ones_like(cooccurrence, dtype=bool))\n",
        "    with sns.axes_style(\"white\"):\n",
        "        sns.heatmap(cooccurrence, mask=mask, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n",
        "                    xticklabels=all_categories, yticklabels=all_categories)\n",
        "    plt.title('Category Co-occurrence Matrix', fontsize=16)\n",
        "    plt.xticks(rotation=60, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('category_cooccurrence.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 4. Word Importance Visualization\n",
        "def plot_word_importance(vectorizer, model, category, top_n=20):\n",
        "    \"\"\"Visualize the most important words for a specific category.\"\"\"\n",
        "    # Get feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Get coefficients\n",
        "    coef = model.coef_[0]\n",
        "\n",
        "    # Create DataFrame with words and their importance\n",
        "    word_importance = pd.DataFrame(\n",
        "        {'Word': feature_names, 'Importance': coef}\n",
        "    ).sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Plot top positive and negative words\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Top positive words (indicating presence of category)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    top_positive = word_importance.head(top_n)\n",
        "    sns.barplot(x='Importance', y='Word', data=top_positive, palette='Blues_d')\n",
        "    plt.title(f'Top {top_n} Words Indicating \"{category}\"', fontsize=14)\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Top negative words (indicating absence of category)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    top_negative = word_importance.tail(top_n).sort_values('Importance')\n",
        "    sns.barplot(x='Importance', y='Word', data=top_negative, palette='Reds_d')\n",
        "    plt.title(f'Top {top_n} Words Against \"{category}\"', fontsize=14)\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'word_importance_{category}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 5. Confusion Matrix for Each Category\n",
        "def plot_confusion_matrices(y_true, y_pred, all_categories):\n",
        "    \"\"\"Plot confusion matrices for all categories.\"\"\"\n",
        "    # Create a grid of confusion matrices\n",
        "    n_categories = len(all_categories)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_categories + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, category in enumerate(all_categories):\n",
        "        if i < len(axes):\n",
        "            # Get true and predicted values for this category\n",
        "            true = y_true[:, i]\n",
        "            pred = y_pred[:, i]\n",
        "\n",
        "            # Calculate confusion matrix\n",
        "            cm = confusion_matrix(true, pred)\n",
        "\n",
        "            # Plot\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])\n",
        "            axes[i].set_title(f'Confusion Matrix: {category}')\n",
        "            axes[i].set_xlabel('Predicted')\n",
        "            axes[i].set_ylabel('True')\n",
        "            axes[i].set_xticklabels(['No', 'Yes'])\n",
        "            axes[i].set_yticklabels(['No', 'Yes'])\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for i in range(n_categories, len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 6. ROC Curves\n",
        "def plot_roc_curves(models, X_test, y_test, all_categories):\n",
        "    \"\"\"Plot ROC curves for all category classifiers.\"\"\"\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    for i, category in enumerate(all_categories):\n",
        "        # Get true values\n",
        "        y_true = y_test[:, i]\n",
        "\n",
        "        # Get predictions\n",
        "        y_score = models[category].decision_function(X_test)\n",
        "\n",
        "        # Calculate ROC curve\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plot\n",
        "        plt.plot(fpr, tpr, lw=2,\n",
        "                 label=f'{category} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for All Categories')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('roc_curves.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# 7. Learning Curve\n",
        "def plot_learning_curve(model, X, y, category, cv=5):\n",
        "    \"\"\"Plot learning curve to show model performance with increasing data.\"\"\"\n",
        "    from sklearn.model_selection import learning_curve\n",
        "\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        model, X, y, cv=cv, n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10), scoring='f1')\n",
        "\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='r', label='Training score')\n",
        "    plt.plot(train_sizes, test_mean, 'o-', color='g', label='Cross-validation score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')\n",
        "    plt.xlabel('Training set size')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title(f'Learning Curve for {category}')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'learning_curve_{category}.png', dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I2e9Cbncf8f"
      },
      "source": [
        "INSERT 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txj523U7aHk3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aANKu0tA1bNA"
      },
      "outputs": [],
      "source": [
        "dfs_train_clean.show(5)\n",
        "dfs_test_clean.show(5)\n",
        "plot_category_distribution(dfs_test_clean)\n",
        "plot_category_cooccurrence(dfs_test_clean)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOrPiwbaOY9K"
      },
      "source": [
        "# Step 1: Extract Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTg0cCjYNd_P"
      },
      "outputs": [],
      "source": [
        "#Cleaning and Extracting functions\n",
        "def clean_categories(categories):\n",
        "    return categories.replace('-', '_').replace('.', '_')\n",
        "\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "        return [clean_categories(cat) for cat in categories]\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEDS52iDOj18"
      },
      "outputs": [],
      "source": [
        "#dfs_train_clean = dfs_train_clean.limit(5000)\n",
        "#\n",
        "# Extract categories for each DataFrame\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "dfs_train_clean = dfs_train_clean.limit(80)\n",
        "\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "dfs_test_clean = dfs_test_clean.limit(80)\n",
        "\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "dfs_validation_clean = dfs_validation_clean.limit(80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3TLXIdil6uR"
      },
      "outputs": [],
      "source": [
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames to avoid recomputation\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYcDfHqTmI-J"
      },
      "outputs": [],
      "source": [
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z43w955emZB5"
      },
      "outputs": [],
      "source": [
        "dfs_train_clean.show(5)\n",
        "dfs_test_clean.show(5)\n",
        "\n",
        "# Cache DataFrames to avoid recomputation\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "\n",
        "dfs_train_clean.select(\"data_source\", \"categories\").show(20, truncate=False)\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "print(f\"Feature cols=\",feature_cols)\n",
        "\n",
        "\n",
        "dfs_train_clean.select(\"data_source\", \"categories\").show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYBtBgDNuquY"
      },
      "outputs": [],
      "source": [
        "# SKIP\n",
        "# 3. Train Classifiers\n",
        "#models = {}\n",
        "#for category in all_categories:\n",
        "#    lr = LogisticRegression(featuresCol=\"features\", labelCol=category)\n",
        "#    pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, lr])\n",
        "#    models[category] = pipeline.fit(df)#\n",
        "#\n",
        "## 4. Test on New Data (new_text_df)\n",
        "#for category, model in models.items():\n",
        "#    predictions = model.transform(new_text_df)\n",
        "#    predictions.select(\"text\", category, \"prediction\").show() #show the text, the real category, and the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tonS2_OvtJYV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, array_contains, lit\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBQbxdYgoHAO"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Train SVM Model\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train_data = assembler.transform(dfs_train_clean).select(\"features\", \"text\")\n",
        "train_data.show(5)\n",
        "\n",
        "#Fit the data\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"text\", maxIter=20, regParam=0.1)\n",
        "svm_model = svm.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmvS2HLNs_q2"
      },
      "outputs": [],
      "source": [
        "# Train SVM Model\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train_data = assembler.transform(dfs_train_clean).select(\"features\", \"id\")\n",
        "\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"id\", maxIter=20, regParam=0.1)\n",
        "svm_model = svm.fit(train_data)\n",
        "\n",
        "# Test on test set\n",
        "test_data = assembler.transform(dfs_test_clean).select(\"features\", \"id\")\n",
        "svm_predictions = svm_model.transform(test_data)\n",
        "\n",
        "# Evaluate SVM\n",
        "svm_evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1_svm = svm_evaluator.evaluate(svm_predictions)\n",
        "precision_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"SVM F1 Score: {f1_svm}\")\n",
        "print(f\"SVM Precision: {precision_svm}\")\n",
        "print(f\"SVM Recall: {recall_svm}\")\n",
        "\n",
        "# Convert to Pandas for LSTM and CNN\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "X_train = train_pandas[feature_cols].values\n",
        "X_test = test_pandas[feature_cols].values\n",
        "\n",
        "y_train = train_pandas['id'].values\n",
        "y_test = test_pandas['id'].values\n",
        "\n",
        "# Reshape for LSTM and CNN\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1)),\n",
        "    Dropout(0.2),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"LSTM Accuracy: {lstm_acc}\")\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "cnn_loss, cnn_acc = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"CNN Accuracy: {cnn_acc}\")\n",
        "\n",
        "# Clear cache to free memory\n",
        "dfs_train_clean.unpersist()\n",
        "dfs_test_clean.unpersist()\n",
        "dfs_validation_clean.unpersist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK-BA9GQve4r"
      },
      "outputs": [],
      "source": [
        "\n",
        "#lr = LogisticRegression(featuresCol=\"features\", labelCol=\"id\", maxIter=20, regParam=0.1)\n",
        "#model = lr.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJtWWzN-o_VR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0UXCRfrPuzH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test on test set\n",
        "test_data = assembler.transform(dfs_test_clean).select(\"features\", \"id\")\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1 = evaluator.evaluate(predictions)\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Validate on validation set\n",
        "validation_data = assembler.transform(dfs_validation_clean).select(\"features\", \"id\")\n",
        "validation_predictions = model.transform(validation_data)\n",
        "\n",
        "f1_val = evaluator.evaluate(validation_predictions)\n",
        "precision_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"Validation F1 Score: {f1_val}\")\n",
        "print(f\"Validation Precision: {precision_val}\")\n",
        "print(f\"Validation Recall: {recall_val}\")\n",
        "\n",
        "# Clear cache to free memory\n",
        "dfs_train_clean.unpersist()\n",
        "dfs_test_clean.unpersist()\n",
        "dfs_validation_clean.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEP3xhGmKyX-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Train Logistic Regression Model\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train_data = assembler.transform(dfs_train_clean).select(\"features\", \"id\")\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"id\")\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# Test on test set\n",
        "test_data = assembler.transform(dfs_test_clean).select(\"features\", \"id\")\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1 = evaluator.evaluate(predictions)\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Validate on validation set\n",
        "validation_data = assembler.transform(dfs_validation_clean).select(\"features\", \"id\")\n",
        "validation_predictions = model.transform(validation_data)\n",
        "\n",
        "f1_val = evaluator.evaluate(validation_predictions)\n",
        "precision_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_val = evaluator.evaluate(validation_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"Validation F1 Score: {f1_val}\")\n",
        "print(f\"Validation Precision: {precision_val}\")\n",
        "print(f\"Validation Recall: {recall_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq7X4GbkTyjW"
      },
      "outputs": [],
      "source": [
        "# Add me back later, if needed. Initialize Spark session. Display the schema to understand the data structure\n",
        "#dfp_train.printSchema()  Let's extract the categories from label_codes. First, we need to convert the string representation of label codes to actual lists. Then We'll use a UDF (User Defined Function) for this\n",
        "def clean_categories(categories):\n",
        "  return categories.replace('-', '_').replace('.', '_')\n",
        "\n",
        "#Extract just the category part from each label code\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "\n",
        "    # Convert string representation to list\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)#\n",
        "\n",
        "        # Extract category part (remove the sentiment indicator at the end)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "\n",
        "        categories = clean_categories(categories)\n",
        "        return categories\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a23wEzZimqsp"
      },
      "outputs": [],
      "source": [
        "#This one contains text vectorization embeddings\n",
        "# ACD-Only Classifier Pipeline\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains, lit\n",
        "from pyspark.ml.feature import VectorAssembler, HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# Step 1: Extract Categories\n",
        "\n",
        "def clean_categories(categories):\n",
        "    return categories.replace('-', '_').replace('.', '_')\n",
        "\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "        return [clean_categories(cat) for cat in categories]\n",
        "    return []\n",
        "\n",
        "# Extract categories for each DataFrame\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames to avoid recomputation\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "\n",
        "# Text vectorization using TF-IDF\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
        "\n",
        "# Apply transformations\n",
        "train_tokenized = tokenizer.transform(dfs_train_clean)\n",
        "test_tokenized = tokenizer.transform(dfs_test_clean)\n",
        "\n",
        "train_hashed = hashingTF.transform(train_tokenized)\n",
        "test_hashed = hashingTF.transform(test_tokenized)\n",
        "\n",
        "idf_model = idf.fit(train_hashed)\n",
        "train_tfidf = idf_model.transform(train_hashed)\n",
        "test_tfidf = idf_model.transform(test_hashed)\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Combine binary and text-based features\n",
        "assembler = VectorAssembler(inputCols=feature_cols + [\"tfidf_features\"], outputCol=\"features\")\n",
        "train_data = assembler.transform(train_tfidf).select(\"features\", \"id\")\n",
        "test_data = assembler.transform(test_tfidf).select(\"features\", \"id\")\n",
        "\n",
        "# Train SVM Model\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"id\", maxIter=20, regParam=0.1)\n",
        "svm_model = svm.fit(train_data)\n",
        "\n",
        "# Test on test set\n",
        "svm_predictions = svm_model.transform(test_data)\n",
        "\n",
        "# Evaluate SVM\n",
        "svm_evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1_svm = svm_evaluator.evaluate(svm_predictions)\n",
        "precision_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"SVM F1 Score: {f1_svm}\")\n",
        "print(f\"SVM Precision: {precision_svm}\")\n",
        "print(f\"SVM Recall: {recall_svm}\")\n",
        "\n",
        "# Convert to Pandas for LSTM and CNN\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "X_train = train_pandas[feature_cols].values\n",
        "X_test = test_pandas[feature_cols].values\n",
        "\n",
        "y_train = train_pandas['id'].values\n",
        "y_test = test_pandas['id'].values\n",
        "\n",
        "# Reshape for LSTM and CNN\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1)),\n",
        "    Dropout(0.2),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"LSTM Accuracy: {lstm_acc}\")\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "cnn_loss, cnn_acc = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"CNN Accuracy: {cnn_acc}\")\n",
        "\n",
        "# Clear cache to free memory\n",
        "dfs_train_clean.unpersist()\n",
        "dfs_test_clean.unpersist()\n",
        "dfs_validation_clean.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd7M82pWT78Y"
      },
      "outputs": [],
      "source": [
        "# ACD-Only Classifier Pipeline\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains, lit\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# Step 1: Extract Categories\n",
        "\n",
        "def clean_categories(categories):\n",
        "    return categories.replace('-', '_').replace('.', '_')\n",
        "\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "        return [clean_categories(cat) for cat in categories]\n",
        "    return []\n",
        "\n",
        "# Extract categories for each DataFrame\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Generate binary indicator columns for each category\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(f\"has_{clean_cat}\", array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames to avoid recomputation\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Train SVM Model\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "train_data = assembler.transform(dfs_train_clean).select(\"features\", \"text\")\n",
        "\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"text\", maxIter=20, regParam=0.1)\n",
        "svm_model = svm.fit(train_data)\n",
        "\n",
        "# Test on test set\n",
        "test_data = assembler.transform(dfs_test_clean).select(\"features\", \"text\")\n",
        "svm_predictions = svm_model.transform(test_data)\n",
        "\n",
        "# Evaluate SVM\n",
        "svm_evaluator = MulticlassClassificationEvaluator(labelCol=\"id\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "f1_svm = svm_evaluator.evaluate(svm_predictions)\n",
        "precision_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall_svm = svm_evaluator.evaluate(svm_predictions, {svm_evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(f\"SVM F1 Score: {f1_svm}\")\n",
        "print(f\"SVM Precision: {precision_svm}\")\n",
        "print(f\"SVM Recall: {recall_svm}\")\n",
        "\n",
        "# Convert to Pandas for LSTM and CNN\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "X_train = train_pandas[feature_cols].values\n",
        "X_test = test_pandas[feature_cols].values\n",
        "\n",
        "y_train = train_pandas['id'].values\n",
        "y_test = test_pandas['id'].values\n",
        "\n",
        "# Reshape for LSTM and CNN\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1)),\n",
        "    Dropout(0.2),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"LSTM Accuracy: {lstm_acc}\")\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "cnn_loss, cnn_acc = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"CNN Accuracy: {cnn_acc}\")\n",
        "\n",
        "# Clear cache to free memory\n",
        "dfs_train_clean.unpersist()\n",
        "dfs_test_clean.unpersist()\n",
        "dfs_validation_clean.unpersist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky-gnLs4JDV1"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "# Convert 'label_codes' from string to list\n",
        "dfs_train['label_codes'] = dfs_train['label_codes'].apply(\n",
        "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "# Extract categories and clean them\n",
        "def extract_categories(label_codes):\n",
        "    return [\n",
        "        code.rsplit('.', 1)[0].replace('-', '_').replace('.', '_')\n",
        "        for code in label_codes\n",
        "    ]\n",
        "\n",
        "# Apply the function to create a 'categories' column\n",
        "dfp_train['categories'] = dfp_train['label_codes'].apply(extract_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH6mayVWnipM"
      },
      "outputs": [],
      "source": [
        "\n",
        "#dfs_train = spark.createDataFrame(dfp_train)     #pandas df has no ... withColumn so we need to convert it - Oops it's not a pandas df, it's a pyarrow Table that needs to be first converted to a pandas df\n",
        "#dfp_train = dfp_train.to_pandas()\n",
        "#dfs_train = spark.createDataFrame(dfp_train)\n",
        "##It seems that the to_pandas method is not available for Spark DataFrames -  use the collect method to convert the Spark DataFrame to a Pandas DataFrame:\n",
        "\n",
        "# Convert Spark DataFrame to Pandas\n",
        "dfp_train = spark.read.parquet(parquet_train_file).toPandas()\n",
        "print ( dfp_train.columns)\n",
        "# For each category, create a binary indicator column (1 if present, 0 if not)\n",
        "print(dfp_train.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6JHSyWfJi6K"
      },
      "outputs": [],
      "source": [
        "## Add me back later, if needed. Initialize Spark session. Display the schema to understand the data structure\n",
        "##dfp_train.printSchema()  Let's extract the categories from label_codes. First, we need to convert the string representation of label codes to actual lists. Then We'll use a UDF (User Defined Function) for this\n",
        "#def clean_categories(categories):\n",
        "#  return categories.replace('-', '_').replace('.', '_')\n",
        "#\n",
        "##Extract just the category part from each label code\n",
        "#@udf(returnType=ArrayType(StringType()))\n",
        "#def extract_categories(label_codes):\n",
        "#\n",
        "#    # Convert string representation to list\n",
        "#    if isinstance(label_codes, str):\n",
        "#        codes = ast.literal_eval(label_codes)#\n",
        "#\n",
        "#        # Extract category part (remove the sentiment indicator at the end)\n",
        "#        categories = [code.rsplit('.', 1)[0] for code in codes]\n",
        "#\n",
        "#        categories = clean_categories(categories)\n",
        "#        return categories\n",
        "#    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSDhKNxTpY3R"
      },
      "outputs": [],
      "source": [
        "#df_with_categories = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\")))\n",
        "# Create a list of all unique categories from the dataset\n",
        "df_with_categories.printSchema()\n",
        "#all_categories = df_with_categories.select(explode(\"categories\")).distinct().collect()\n",
        "#all_categories = [row[0] for row in all_categories]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7h538G6hGd_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cye52wMXoiFh"
      },
      "outputs": [],
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) with Clean Column Names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: Extract Sentiment for Each Category\n",
        "\n",
        "# Extract 'label_codes' from the original Spark DataFrame\n",
        "#label_codes_df = df_with_categories.select('text', 'label_codes').toPandas()\n",
        "label_codes_df = df_with_categories.select('text', 'label_codes', 'categories').toPandas()\n",
        "print(label_codes_df)\n",
        "\n",
        "# Merge with train_pandas using the 'id' column\n",
        "train_pandas = train_pandas.merge(label_codes_df, on='id', how='left')\n",
        "\n",
        "def extract_aspect_and_sentiment(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [(code.rsplit('.', 1)[0], int(code.rsplit('.', 1)[-1])) for code in codes]\n",
        "    return []\n",
        "\n",
        "train_pandas['aspect_sentiment_pairs'] = train_pandas['label_codes'].apply(extract_aspect_and_sentiment)\n",
        "\n",
        "# For each category, create a sentiment column (-1, 0, or 1)\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    train_pandas[f\"sentiment_{clean_aspect}\"] = train_pandas['aspect_sentiment_pairs'].apply(\n",
        "        lambda x: next((sent for asp, sent in x if asp == aspect), 0)\n",
        "    )\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "#train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "#test_pandas = test_data.limit(200).toPandas()\n",
        "train_pandas = df_with_categories.limit(1000).toPandas()\n",
        "test_pandas = df_with_categories.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery_life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfeFLTrL-i_U"
      },
      "source": [
        "PRINT ALL CATEGORIES FOUND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd0-2x4Lu2ys"
      },
      "outputs": [],
      "source": [
        "for cat in all_categories:\n",
        "  print(cat + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suGL6w1BwrI8"
      },
      "outputs": [],
      "source": [
        "# For each category, create a binary indicator column (1 if present, 0 if not)\n",
        "for cat in all_categories:\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{cat.replace('-', '_').replace('.', '_')}\",\n",
        "        array_contains(col(\"categories\"), cat).cast(\"integer\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0u6elBY-uWj"
      },
      "outputs": [],
      "source": [
        "for cat in all_categories:\n",
        "  print(cat + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM7ZkSlToFeK"
      },
      "outputs": [],
      "source": [
        "print(train_pandas.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYsspjOVnkvd"
      },
      "outputs": [],
      "source": [
        "#\n",
        "FIX ME TODAY\n",
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) with Clean Column Names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: Extract Sentiment for Each Category\n",
        "\n",
        "def extract_aspect_and_sentiment(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [(code.rsplit('.', 1)[0], int(code.rsplit('.', 1)[-1])) for code in codes]\n",
        "    return []\n",
        "\n",
        "train_pandas['aspect_sentiment_pairs'] = train_pandas['label_codes'].apply(extract_aspect_and_sentiment)\n",
        "\n",
        "# For each category, create a sentiment column (-1, 0, or 1)\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    train_pandas[f\"sentiment_{clean_aspect}\"] = train_pandas['aspect_sentiment_pairs'].apply(\n",
        "        lambda x: next((sent for asp, sent in x if asp == aspect), 0)\n",
        "    )\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery_life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbOTD5kNXdfx"
      },
      "source": [
        "Tokenize the user comments from the text column and remove stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw5lYX0_LauT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "# Select relevant columns for the ACD task\n",
        "feature_cols = [f\"has_{category.replace('-', '_').replace('.', '_')}\" for category in all_categories]\n",
        "acd_df = df_with_categories.select(\"id\", \"text\", *feature_cols)\n",
        "\n",
        "# Convert text to features using TF-IDF\n",
        "# First, tokenize the text\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(acd_df)\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "filtered_data = remover.transform(wordsData)\n",
        "\n",
        "# Convert words to term frequency features\n",
        "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"tf\", minDF=2.0)\n",
        "cv_model = cv.fit(filtered_data)\n",
        "tf_data = cv_model.transform(filtered_data)\n",
        "\n",
        "# Convert term frequency features to TF-IDF\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"features\")\n",
        "idf_model = idf.fit(tf_data)\n",
        "tfidf_data = idf_model.transform(tf_data)\n",
        "\n",
        "# Final dataset ready for modeling\n",
        "final_acd_df = tfidf_data.select(\"id\", \"features\", *feature_cols)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = final_acd_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Cache the datasets for faster processing\n",
        "train_data.cache()\n",
        "test_data.cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIddta9BZVDZ"
      },
      "outputs": [],
      "source": [
        "#final_acd_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCPtQoKDXbiF"
      },
      "outputs": [],
      "source": [
        "#Since you're working with text data, you'll need to extract relevant features from each document. Some common techniques include:\n",
        "#Tokenization (splitting text into individual words or tokens)\n",
        "#Stopword removal (removing common words like \"the\", \"and\", etc.)\n",
        "#Stemming or Lemmatization (reducing words to their base form)\n",
        "#Part-of-speech tagging (identifying grammatical categories of each word)\n",
        "# Create a SparkSession with your test/train/validate data sets\n",
        "#spark = SparkSession.builder.appName(\"Aspect-Based Sentiment Analysis\").getOrCreate()\n",
        "\n",
        "# Load the training and validate datasets into DataFrames\n",
        "#train_df = spark.read.csv(\"path/to/train/dataset\", header=True, inferSchema=True)\n",
        "#val_df = spark.read.csv(\"path/to/val/dataset\", header=True, inferSchema=True)\n",
        "\n",
        "# Define a function to extract features from each column in the dataset\n",
        "def feature_extraction(df):\n",
        "    # Tokenization and stopword removal\n",
        "    df = df.select(\n",
        "        explode(col(\"text\").cast(\"string\")).alias(\"tokens\"),\n",
        "        explode(col(\"sentiment\").cast(\"string\")).alias(\"sentiment\")\n",
        "    )\n",
        "\n",
        "    # Stemming or Lemmatization\n",
        "    df = df.withColumn(\n",
        "        \"stemmed_tokens\",\n",
        "        df.tokens.map(lambda x: x.lower() if x.isnumeric() else x)  # Remove numbers and convert to lowercase\n",
        "    )\n",
        "\n",
        "    # Part-of-speech tagging (optional)\n",
        "    #df = df.withColumn(\"pos_tags\", explode(col(\"word\").cast(\"string\")).map(lambda x: {\"POS\": \"NNP\"} if x.isnumeric() else {\"POS\": \"NOUN\"})  # Example\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeoRym6BzNyR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-idjLDxZGVL4"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#all_words = filtered_data.rdd.flatMap(lambda x: x.filtered_words).collect()\n",
        "all_words = filtered_data.select(\"filtered_words\").rdd.flatMap(lambda x: x.filtered_words).collect()\n",
        "word_counts = Counter(all_words)\n",
        "top_words = dict(word_counts.most_common(20))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(top_words.keys(), top_words.values())\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Top 20 Word Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#2. TF-IDF Visualization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAgDXRpaLYpC"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Extract only the binary category columns and sentiment\n",
        "category_columns = [col for col in train_pandas.columns if col.startswith('has_')]\n",
        "df_for_heatmap = train_pandas[category_columns + ['sentiment']]\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df_for_heatmap.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Aspect-Sentiment Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5ntO_-ZITfq"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Chart: Heatmap or bar chart.\n",
        "#Purpose: Shows the TF-IDF scores for selected words across documents.\n",
        "#Implementation: After TF-IDF, convert the sparse vectors to dense arrays and plot the values.\n",
        "#Python\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have a few documents and words you want to visualize\n",
        "selected_docs = tfidf_data.take(5) #take the first 5 documents.\n",
        "selected_words = cv_model.vocabulary[:10] # take the first 10 vocabulary words.\n",
        "\n",
        "tfidf_matrix = np.zeros((len(selected_docs), len(selected_words)))\n",
        "\n",
        "for i, row in enumerate(selected_docs):\n",
        "    dense_features = row.features.toArray()\n",
        "    for j, word in enumerate(selected_words):\n",
        "        if word in cv_model.vocabulary:\n",
        "            word_index = cv_model.vocabulary.index(word)\n",
        "            tfidf_matrix[i,j] = dense_features[word_index]\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(tfidf_matrix, annot=True, xticklabels=selected_words)\n",
        "plt.xlabel(\"Selected Words\")\n",
        "plt.ylabel(\"Documents\")\n",
        "plt.title(\"TF-IDF Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "#3. Model Evaluation Metrics:\n",
        "#\n",
        "#Charts: Confusion matrix, ROC curve, precision-recall curve.\n",
        "#Purpose: Shows the performance of the trained classification model.\n",
        "#Implementation: After training and predicting on the test set, use metrics like accuracy, precision, recall, F1-score, and create plots to visualize them.\n",
        "#Confusion Matrix: shows the number of correct and incorrect predictions for each class.\n",
        "#ROC Curve: shows the trade-off between true positive rate and false positive rate.\n",
        "#Precision-Recall Curve: shows the trade-off between precision and recall.\n",
        "#4. Document Length Distribution:#\n",
        "#\n",
        "#Chart: Histogram.\n",
        "#Purpose: Shows the distribution of the number of words in each document.\n",
        "#Implementation: Count the number of words in each document after tokenization.\n",
        "#Python#\n",
        "#\n",
        "doc_lengths = wordsData.rdd.map(lambda x: len(x.words)).collect()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(doc_lengths, bins=50)\n",
        "plt.xlabel(\"Document Length (Number of Words)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Document Length Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YdoY8B4lQQ6"
      },
      "outputs": [],
      "source": [
        "train_data.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "024iF0ZjFCon"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Function to train and evaluate an SVM model for one category\n",
        "def train_svm_for_category(category, train_data, test_data):\n",
        "    # Prepare label column name\n",
        "    label_col = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "\n",
        "    # Create SVM model\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=label_col, maxIter=10)\n",
        "\n",
        "    # Train the model\n",
        "    model = svm.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # For binary classification metrics (precision, recall)\n",
        "    binary_evaluator = BinaryClassificationEvaluator(\n",
        "        labelCol=label_col, rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "    # For F1 score (use MulticlassClassificationEvaluator)\n",
        "    multi_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "    # Calculate AUC (Area Under ROC)\n",
        "    auc = binary_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate F1\n",
        "    f1 = multi_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Calculate precision and recall manually\n",
        "    # First, get true positives, false positives, true negatives, false negatives\n",
        "    tp = predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 1)).count()\n",
        "    fp = predictions.filter((col(label_col) == 0) & (col(\"prediction\") == 1)).count()\n",
        "    fn = predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 0)).count()\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"category\": category,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"auc\": auc\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9isoSjT1zSh"
      },
      "source": [
        "Models were running too slow. \\Caching the DataFrame during SVM training to avoid redundant data processing. Reducing the number of SVM iterations and limiting the dataset size for deep learning. Simplifying the LSTM and CNN architectures by reducing the number of units and filters. Lowering the number of epochs and silencing verbose outputs for faster training.\n",
        "\n",
        "Added visualizations to pipeline:\n",
        "\n",
        "LSTM Loss Over Epochs to show how the model improves during training.\n",
        "CNN Accuracy Over Epochs to track performance across epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhwcmnLmuzZF"
      },
      "outputs": [],
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) with Clean Column Names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: ABSA (SVM Classifier for each Aspect) with Caching for Speed\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{clean_aspect}') == 1).cache()\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=5)  # Reduced iterations for speed\n",
        "    model = svm.fit(tfidf_data)\n",
        "\n",
        "    aspect_df.unpersist()\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery_life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Z3_Jc1u1RR"
      },
      "source": [
        "THE END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-4eMTi81uzT"
      },
      "outputs": [],
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD)\n",
        "for aspect in all_categories:\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: ABSA (SVM Classifier for each Aspect) with Caching for Speed\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{aspect}') == 1).cache()\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=5)  # Reduced iterations for speed\n",
        "    model = svm.fit(tfidf_data)\n",
        "\n",
        "    aspect_df.unpersist()\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_cnn = cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery-life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))\n",
        "\n",
        "# Step 7: Visualizations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot LSTM Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['loss'], label='LSTM Loss')\n",
        "plt.title('LSTM Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot CNN Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['accuracy'], label='CNN Accuracy')\n",
        "plt.title('CNN Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDyy3yc2TtN"
      },
      "source": [
        "Additional charts, such as confusion matrices or precision-recall curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ltf8M5Oc2S-7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqRJCneG1a21"
      },
      "source": [
        "Models were running too slow.\n",
        "\\Caching the DataFrame during SVM training to avoid redundant data processing.\n",
        "Reducing the number of SVM iterations and limiting the dataset size for deep learning.\n",
        "Simplifying the LSTM and CNN architectures by reducing the number of units and filters.\n",
        "Lowering the number of epochs and silencing verbose outputs for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNmIWqJYF47I"
      },
      "outputs": [],
      "source": [
        "# Convert Spark DataFrame to Pandas for deep learning\n",
        "# Collect the data (be careful with large datasets)\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "# We need to convert the sparse vector features to numpy arrays\n",
        "from pyspark.ml.linalg import SparseVector\n",
        "\n",
        "def sparse_to_array(sparse_vector):\n",
        "    return sparse_vector.toArray() if isinstance(sparse_vector, SparseVector) else sparse_vector\n",
        "\n",
        "# Convert features to numpy arrays\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Prepare X (features) and y (labels) for training\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "\n",
        "# Get all label columns\n",
        "label_cols = [col for col in train_pandas.columns if col.startswith('has_')]\n",
        "y_train = train_pandas[label_cols].values\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "# Now build a deep learning model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define model architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "results = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {results[0]}\")\n",
        "print(f\"Test Accuracy: {results[1]}\")\n",
        "print(f\"Test Precision: {results[2]}\")\n",
        "print(f\"Test Recall: {results[3]}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate F1 score manually\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "precision = precision_score(y_test, y_pred_binary, average='micro')\n",
        "recall = recall_score(y_test, y_pred_binary, average='micro')\n",
        "f1 = f1_score(y_test, y_pred_binary, average='micro')\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onEAObLs1M6Y"
      },
      "source": [
        "END WORKING *AREA*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I0GaU5Umj0E"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "# For simplicity, let's implement a binary classification model for each category\n",
        "# In practice, you might want to use a multi-label approach\n",
        "\n",
        "# Function to train and evaluate an SVM model for one category\n",
        "def train_svm_for_category(category, train_data, test_data):\n",
        "    # Prepare label column name\n",
        "    label_col = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "\n",
        "    # Create SVM model\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=label_col, maxIter=10)\n",
        "\n",
        "    # Train the model\n",
        "    model = svm.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # Evaluate model\n",
        "   # evaluator = MulticlassClassificationEvaluator(\n",
        "   #     labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
        "   # f1 = evaluator.evaluate(predictions)\n",
        "    #\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col,\n",
        "                                              predictionCol=\"prediction\",\n",
        "                                              metricName=\"weightedPrecision\")\n",
        "precision = evaluator.evaluate(predictions)\n",
        "print(\"Weighted Precision:\", precision)\n",
        "    evaluator.setMetricName(\"precision\")\n",
        "    precision = evaluator.evaluate(predictions)\n",
        "\n",
        "    evaluator.setMetricName(\"recall\")\n",
        "    recall = evaluator.evaluate(predictions)\n",
        "\n",
        "    return {\"category\": category, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Train models for all categories\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "    result = train_svm_for_category(category, train_data, test_data)\n",
        "    svm_results.append(result)\n",
        "\n",
        "# Print results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "svm_results_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjwA9QxfmIkK"
      },
      "outputs": [],
      "source": [
        "print(\"Training SVM models for all categories...\")\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "    try:\n",
        "        result = train_svm_for_category(category, train_data, test_data)\n",
        "        svm_results.append(result)\n",
        "        print(f\"Completed category: {category}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing category {category}: {str(e)}\")\n",
        "\n",
        "# Create a dataframe with results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "print(\"SVM Results:\")\n",
        "svm_results_df.show()\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_precision = np.mean([r[\"precision\"] for r in svm_results])\n",
        "avg_recall = np.mean([r[\"recall\"] for r in svm_results])\n",
        "avg_f1 = np.mean([r[\"f1\"] for r in svm_results])\n",
        "\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average F1: {avg_f1:.4f}\")\n",
        "\n",
        "# Convert to pandas DataFrame for deep learning model\n",
        "# We'll process a subset of records to handle memory constraints\n",
        "# For a real model, you might want to use a data loader or mini-batches\n",
        "print(\"Preparing data for deep learning model...\")\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit to 1000 samples for demonstration\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Function to convert sparse vector to array\n",
        "def sparse_to_array(v):\n",
        "    if hasattr(v, 'toArray'):\n",
        "        return v.toArray()\n",
        "    return v\n",
        "\n",
        "# Apply conversion function\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Stack features into numpy arrays\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "\n",
        "# Extract labels\n",
        "y_train = train_pandas[feature_cols].values\n",
        "y_test = test_pandas[feature_cols].values\n",
        "\n",
        "print(\"Data preparation for deep learning complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQZ0jSpGKD9d"
      },
      "outputs": [],
      "source": [
        "# Train models for all categories\n",
        "svm_results = []\n",
        "for category in all_categories:\n",
        "   # result = train_svm_for_category(category, train_data, test_data)\n",
        "    result = train_svm_for_category(category, train_data, test_data)\n",
        "    svm_results.append(result)\n",
        "\n",
        "# Print results\n",
        "svm_results_df = spark.createDataFrame(svm_results)\n",
        "svm_results_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLr-hMhUwTP9"
      },
      "source": [
        "Speeded up version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQuL0YIJwPjO"
      },
      "outputs": [],
      "source": [
        "# ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models) - Optimized for Speed\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD) - Clean column names\n",
        "for aspect in all_categories:\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{clean_aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "## Step 1: Aspect Category Detection (ACD)\n",
        "#for aspect in all_categories:\n",
        "#    df_with_categories = df_with_categories.withColumn(\n",
        "#        f\"has_{aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "#    )\n",
        "#\n",
        "#\n",
        "## Step 2: ABSA (SVM Classifier for each Aspect) with Caching for Speed\n",
        "#def train_absa_svm(aspect, df_with_categories):\n",
        "#    aspect_df = df_with_categories.filter(col(f'has_{aspect}') == 1).cache()\n",
        "#\n",
        "\n",
        "# Step 2: Adjust the SVM Function to Use the Clean Column Name\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    clean_aspect = aspect.replace('-', '_').replace('.', '_')\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{clean_aspect}') == 1).cache()\n",
        "\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=5)  # Reduced iterations for speed\n",
        "    model = svm.fit(tfidf_data)\n",
        "\n",
        "    aspect_df.unpersist()\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.limit(1000).toPandas()  # Limit data for speed\n",
        "test_pandas = test_data.limit(200).toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA with Reduced Complexity\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train.shape[1], 1), return_sequences=False),  # Reduced LSTM units\n",
        "    Dropout(0.2),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)  # Fewer epochs\n",
        "\n",
        "# Step 5: CNN Model for ABSA with Faster Conv1D\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),  # Reduced filters\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Step 6: Compare Models\n",
        "svm_model = train_absa_svm(\"battery-life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test, verbose=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuP_tJHBwQaw"
      },
      "source": [
        "Very slow version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOVK6q_eKHHt"
      },
      "outputs": [],
      "source": [
        " #ACD + ABSA Full Pipeline (SVM, LSTM, and CNN Models)\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Aspect Category Detection (ACD)\n",
        "# Binary indicator columns for each aspect\n",
        "for aspect in all_categories:\n",
        "    df_with_categories = df_with_categories.withColumn(\n",
        "        f\"has_{aspect}\", array_contains(col(\"categories\"), aspect).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "# Step 2: ABSA (SVM Classifier for each Aspect)\n",
        "def train_absa_svm(aspect, df_with_categories):\n",
        "    aspect_df = df_with_categories.filter(col(f'has_{aspect}') == 1)\n",
        "\n",
        "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "    wordsData = tokenizer.transform(aspect_df)\n",
        "\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "    featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "    tfidf_data = idf.fit(featurizedData).transform(featurizedData)\n",
        "\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"sentiment\", maxIter=10)\n",
        "    model = svm.fit(tfidf_data)\n",
        "    return model\n",
        "\n",
        "# Step 3: Convert to Pandas for Deep Learning\n",
        "train_pandas = train_data.toPandas()\n",
        "test_pandas = test_data.toPandas()\n",
        "\n",
        "# Convert sparse vector to numpy array\n",
        "def sparse_to_array(v):\n",
        "    return v.toArray() if hasattr(v, \"toArray\") else v\n",
        "\n",
        "train_pandas['features_array'] = train_pandas['features'].apply(sparse_to_array)\n",
        "test_pandas['features_array'] = test_pandas['features'].apply(sparse_to_array)\n",
        "\n",
        "# Step 4: LSTM Model for ABSA\n",
        "X_train = np.stack(train_pandas['features_array'].values)\n",
        "y_train = train_pandas[label_cols].values\n",
        "\n",
        "X_test = np.stack(test_pandas['features_array'].values)\n",
        "y_test = test_pandas[label_cols].values\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Step 5: CNN Model for ABSA\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(label_cols), activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Step 6: Compare SVM, LSTM, and CNN Models\n",
        "svm_model = train_absa_svm(\"battery-life\", df_with_categories)\n",
        "\n",
        "print(\"SVM Model Trained\")\n",
        "print(\"LSTM Accuracy:\", lstm_model.evaluate(X_test, y_test))\n",
        "print(\"CNN Accuracy:\", cnn_model.evaluate(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z6G2T8CNNuW"
      },
      "outputs": [],
      "source": [
        "#Old version of file loading into pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBCZU7BKbVOH"
      },
      "source": [
        " see what's happening here. You're facing an issue with your approach to handling categories in your PySpark code. Let me help you fix this.\n",
        "The main problems I see are:\n",
        "\n",
        "You're mixing column naming conventions (using both hyphenated and underscore-separated category names)\n",
        "You're trying to use binary indicators for categories but hitting reference errors\n",
        "Your feature engineering approach is trying to use category indicators as features, which is causing confusion\n",
        "\n",
        "Here's how I would restructure your approach:\n",
        "python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh4Hk5YWbex-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SG2rDcubfYY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA-CSUaEbXIX"
      },
      "source": [
        "from pyspark.sql.functions import col, array_contains, explode, udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.ml.feature import VectorAssembler, HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import ast\n",
        "\n",
        "# Step 1: Extract and clean categories - use a consistent naming convention\n",
        "@udf(returnType=ArrayType(StringType()))\n",
        "def extract_categories(label_codes):\n",
        "    if isinstance(label_codes, str):\n",
        "        codes = ast.literal_eval(label_codes)\n",
        "        return [code.rsplit('.', 1)[0] for code in codes]\n",
        "    return []\n",
        "\n",
        "# Apply the UDF to extract categories\n",
        "dfs_train_clean = dfs_train.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_test_clean = dfs_test.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "dfs_validation_clean = dfs_validation.withColumn(\"categories\", extract_categories(col(\"label_codes\"))).limit(1000)\n",
        "\n",
        "# Step 2: Get all unique categories with consistent naming\n",
        "all_categories = dfs_train_clean.select(explode(\"categories\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Step 3: Create binary indicator columns with consistent naming\n",
        "for category in all_categories:\n",
        "    # Use the original category name in the column name to avoid confusion\n",
        "    col_name = f\"has_{category.replace('-', '_').replace('.', '_')}\"\n",
        "    dfs_train_clean = dfs_train_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_test_clean = dfs_test_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "    dfs_validation_clean = dfs_validation_clean.withColumn(col_name, array_contains(col(\"categories\"), category).cast(\"integer\"))\n",
        "\n",
        "# Cache DataFrames\n",
        "dfs_train_clean.cache()\n",
        "dfs_test_clean.cache()\n",
        "dfs_validation_clean.cache()\n",
        "\n",
        "# Step 4: Text vectorization with TF-IDF\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\", minDocFreq=2)\n",
        "\n",
        "# Apply transformations\n",
        "train_tokenized = tokenizer.transform(dfs_train_clean)\n",
        "test_tokenized = tokenizer.transform(dfs_test_clean)\n",
        "train_hashed = hashingTF.transform(train_tokenized)\n",
        "test_hashed = hashingTF.transform(test_tokenized)\n",
        "idf_model = idf.fit(train_hashed)\n",
        "train_tfidf = idf_model.transform(train_hashed)\n",
        "test_tfidf = idf_model.transform(test_hashed)\n",
        "\n",
        "# Step 5: Select the correct feature columns based on the actual column names\n",
        "feature_cols = [f\"has_{cat.replace('-', '_').replace('.', '_')}\" for cat in all_categories]\n",
        "\n",
        "# Step 6: Build a multi-label classifier for each category\n",
        "# For each category, train a separate binary classifier\n",
        "results = {}\n",
        "\n",
        "for category in all_categories:\n",
        "    clean_cat = category.replace('-', '_').replace('.', '_')\n",
        "    col_name = f\"has_{clean_cat}\"\n",
        "    \n",
        "    # Create label column for this specific category\n",
        "    train_data = train_tfidf.withColumn(\"label\", col(col_name))\n",
        "    test_data = test_tfidf.withColumn(\"label\", col(col_name))\n",
        "    \n",
        "    # Use only TF-IDF features for prediction\n",
        "    assembler = VectorAssembler(inputCols=[\"tfidf_features\"], outputCol=\"features\")\n",
        "    train_assembled = assembler.transform(train_data).select(\"features\", \"label\")\n",
        "    test_assembled = assembler.transform(test_data).select(\"features\", \"label\")\n",
        "    \n",
        "    # Train binary classifier\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=20, regParam=0.1)\n",
        "    svm_model = svm.fit(train_assembled)\n",
        "    \n",
        "    # Make predictions\n",
        "    predictions = svm_model.transform(test_assembled)\n",
        "    \n",
        "    # Evaluate\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    f1 = evaluator.evaluate(predictions)\n",
        "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "    \n",
        "    results[category] = {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "    print(f\"Category: {category}, F1: {f1}, Precision: {precision}, Recall: {recall}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8zGv69MPmIV"
      },
      "outputs": [],
      "source": [
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Try with pyarrow first to validate the file\n",
        "try:\n",
        "    dfp_train = pq.read_table(parquet_train_file)\n",
        "    dfp_test = pq.read_table(parquet_train_file)\n",
        "    dfp_validation = pq.read_table(parquet_validation_file)\n",
        "    print(\"File can be read with pyarrow\")\n",
        "except Exception as e:\n",
        "    print(f\"Pyarrow error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJXEpoaBbn7w"
      },
      "source": [
        "This approach fixes several issues:\n",
        "\n",
        "It uses a consistent naming convention for category columns\n",
        "It handles the text and category features separately\n",
        "It treats this as a multi-label classification problem by training separate binary classifiers for each category\n",
        "It uses only the text features (TF-IDF) to predict the categories, rather than trying to use category indicators as features\n",
        "\n",
        "The error you were seeing (A column or function parameter with name 'has_battery-life' cannot be resolved) was because you were trying to reference a column with a hyphen in the name, which is not allowed in Spark SQL column references."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QSQip5a0XM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjsENdIkaz1t"
      },
      "source": [
        "\n",
        "Category Classification Performance: This graph shows the F1 score, precision, and recall for each category. The sample data shows that \"price_value\" has the highest F1 score (0.87) and precision (0.92), while \"discounts\" has the lowest metrics across the board.\n",
        "Category Distribution: This chart shows how many instances of each category appear in your dataset, which helps identify imbalances. For example, \"app_website\" appears most frequently (1430 instances), while \"discounts\" is the least common (537 instances).\n",
        "Confusion Matrix Metrics: This provides an overview of the model's overall performance across all categories, with true positives (2456), false positives (365), false negatives (412), and true negatives (1867).\n",
        "\n",
        "In Python, you could generate similar charts using matplotlib or seaborn. Here's how you might create the category performance chart:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8ooSKVda1Nz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrPxkZceay_E"
      },
      "source": [
        "These charts provide valuable insights for your multi-category classification task:\n",
        "\n",
        "Performance Metrics Chart: Shows F1, precision, and recall for each category, sorted by F1 score to quickly identify your strongest and weakest categories.\n",
        "Category Distribution: Visualizes how many instances of each category appear in your dataset, helping identify class imbalance issues.\n",
        "Co-occurrence Matrix: Shows which categories frequently appear together in the same document, revealing relationships between categories.\n",
        "Word Importance Visualization: For each category, displays the most predictive words (both positive and negative), helping you understand what text features drive classification decisions.\n",
        "Confusion Matrices: Shows true positive, false positive, true negative, and false negative rates for each category classifier.\n",
        "ROC Curves: Plots the receiver operating characteristic curves for all categories, showing the tradeoff between true positive rate and false positive rate.\n",
        "Learning Curve: Shows how model performance changes with increasing training data size, helping you determine if more data would improve results.\n",
        "\n",
        "To use these functions, you'll need to adapt them slightly to your specific data structures and column names. For best results, run them after your model training and evaluation is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMbo81cMaxsL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJsf9oSeag68"
      },
      "outputs": [],
      "source": [
        "import React from 'react';\n",
        "import { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from 'recharts';\n",
        "\n",
        "const CategoryPerformanceCharts = () => {\n",
        "  // Sample data representing category classification performance\n",
        "  const categoryPerformance = [\n",
        "    { category: 'price_value', f1: 0.87, precision: 0.92, recall: 0.83 },\n",
        "    { category: 'staff_attitude', f1: 0.78, precision: 0.85, recall: 0.72 },\n",
        "    { category: 'app_website', f1: 0.82, precision: 0.88, recall: 0.77 },\n",
        "    { category: 'ease_of_use', f1: 0.75, precision: 0.81, recall: 0.70 },\n",
        "    { category: 'brand_reviews', f1: 0.71, precision: 0.77, recall: 0.66 },\n",
        "    { category: 'discounts', f1: 0.65, precision: 0.72, recall: 0.60 },\n",
        "    { category: 'rides_speed', f1: 0.79, precision: 0.83, recall: 0.75 },\n",
        "  ];\n",
        "\n",
        "  // Sample data for category distribution\n",
        "  const categoryDistribution = [\n",
        "    { name: 'price_value', count: 1254 },\n",
        "    { name: 'staff_attitude', count: 986 },\n",
        "    { name: 'app_website', count: 1430 },\n",
        "    { name: 'ease_of_use', count: 845 },\n",
        "    { name: 'brand_reviews', count: 762 },\n",
        "    { name: 'discounts', count: 537 },\n",
        "    { name: 'rides_speed', count: 689 },\n",
        "  ];\n",
        "\n",
        "  // Sample data for confusion matrix\n",
        "  const confusionMatrix = [\n",
        "    { name: 'True Positive', count: 2456 },\n",
        "    { name: 'False Positive', count: 365 },\n",
        "    { name: 'False Negative', count: 412 },\n",
        "    { name: 'True Negative', count: 1867 },\n",
        "  ];\n",
        "\n",
        "  return (\n",
        "    <div className=\"flex flex-col gap-8 p-4\">\n",
        "      <div className=\"bg-white p-6 rounded-lg shadow\">\n",
        "        <h2 className=\"text-xl font-bold mb-4\">Category Classification Performance</h2>\n",
        "        <div className=\"h-80\">\n",
        "          <ResponsiveContainer width=\"100%\" height=\"100%\">\n",
        "            <BarChart\n",
        "              data={categoryPerformance}\n",
        "              margin={{ top: 20, right: 30, left: 20, bottom: 5 }}\n",
        "            >\n",
        "              <CartesianGrid strokeDasharray=\"3 3\" />\n",
        "              <XAxis dataKey=\"category\" />\n",
        "              <YAxis />\n",
        "              <Tooltip />\n",
        "              <Legend />\n",
        "              <Bar dataKey=\"f1\" fill=\"#8884d8\" name=\"F1 Score\" />\n",
        "              <Bar dataKey=\"precision\" fill=\"#82ca9d\" name=\"Precision\" />\n",
        "              <Bar dataKey=\"recall\" fill=\"#ffc658\" name=\"Recall\" />\n",
        "            </BarChart>\n",
        "          </ResponsiveContainer>\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <div className=\"bg-white p-6 rounded-lg shadow\">\n",
        "        <h2 className=\"text-xl font-bold mb-4\">Category Distribution in Dataset</h2>\n",
        "        <div className=\"h-64\">\n",
        "          <ResponsiveContainer width=\"100%\" height=\"100%\">\n",
        "            <BarChart\n",
        "              data={categoryDistribution}\n",
        "              margin={{ top: 20, right: 30, left: 20, bottom: 5 }}\n",
        "            >\n",
        "              <CartesianGrid strokeDasharray=\"3 3\" />\n",
        "              <XAxis dataKey=\"name\" />\n",
        "              <YAxis />\n",
        "              <Tooltip />\n",
        "              <Legend />\n",
        "              <Bar dataKey=\"count\" fill=\"#8884d8\" name=\"Count\" />\n",
        "            </BarChart>\n",
        "          </ResponsiveContainer>\n",
        "        </div>\n",
        "      </div>\n",
        "\n",
        "      <div className=\"bg-white p-6 rounded-lg shadow\">\n",
        "        <h2 className=\"text-xl font-bold mb-4\">Confusion Matrix Metrics</h2>\n",
        "        <div className=\"h-64\">\n",
        "          <ResponsiveContainer width=\"100%\" height=\"100%\">\n",
        "            <BarChart\n",
        "              data={confusionMatrix}\n",
        "              margin={{ top: 20, right: 30, left: 20, bottom: 5 }}\n",
        "            >\n",
        "              <CartesianGrid strokeDasharray=\"3 3\" />\n",
        "              <XAxis dataKey=\"name\" />\n",
        "              <YAxis />\n",
        "              <Tooltip />\n",
        "              <Legend />\n",
        "              <Bar dataKey=\"count\" fill=\"#82ca9d\" name=\"Count\" />\n",
        "            </BarChart>\n",
        "          </ResponsiveContainer>\n",
        "        </div>\n",
        "      </div>\n",
        "    </div>\n",
        "  );\n",
        "};\n",
        "\n",
        "export default CategoryPerformanceCharts;"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Wv6VCnIy0ZOLqkXQtwpfqnWCeolQisVd",
      "authorship_tag": "ABX9TyO45wVx4/J2cjK3LPyjylHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}